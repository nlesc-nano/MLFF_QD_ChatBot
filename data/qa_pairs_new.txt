OFFICIAL MLFF_QD Q&A DATASET (Technical + Scientific)
=====================================================

Q: What is the purpose of the  --only-generate  and  --train-after-generate  flags in the MLFF_QD platform?
A: The  --only-generate  flag is used to only generate the engine-specific YAML and/or convert data, without starting training. The  --train-after-generate  flag is used to generate data/config, then immediately start training using the generated engine YAML.


Q: How do I run the training code in the MLFF_QD platform?
A: To run the training code, use the following command:  python -m mlff_qd.training . By default, the code looks for a config file named  input.yaml , but you can specify a different config file using the  --config  flag.

Q: How do I extract scalar training metrics from TensorBoard event files in the MLFF_QD platform?
A: To extract scalar training metrics from TensorBoard event files, use the  analysis/extract_metrics.py  script and provide the path to the TensorBoard event file using the  --path  flag. You can also specify the output file path using the  --output_file  flag.

Q: How do I install the MLFF_QD platform and required packages?
A: To install the MLFF_QD platform and required packages, create a conda environment using Python 3.12 by following these steps: cloning the repository, setting up the Conda environment, activating the environment, installing the  mace-torch  package, and installing the  mlff_qd  package in editable mode.

Q: How do I extract metrics from a TensorBoard event file using the  app.py  script?
A: You can extract metrics by uploading a TensorBoard event file, choosing the output CSV filename, and optionally previewing the extracted metrics before downloading them as a CSV file.

Q: What is the purpose of the  platform  CLI argument in  app.py ?
A: The  platform  CLI argument is used to select the platform, which can be either SchNet or NequIP.

Q: How do I customize plot settings for Matplotlib plots in  app.py ?
A: You can customize plot settings by selecting options such as title, colors, log scale, and grid for Matplotlib plots.

Q: What export formats are supported for Matplotlib plots in  app.py ?
A: Matplotlib plots can be exported in PNG, JPG, PDF, and SVG formats.

Q: How do I choose the plot type (Static Matplotlib or Interactive Plotly) in  app.py ?
A: You can choose the plot type by selecting the  plot_type  option, which can be either Static Matplotlib or Interactive Plotly.

Q: How do I execute the  extract_metrics.py  script from the command line?
A: You can execute the script from the command line using the following syntax:  python extract_metrics.py -p <event_file_path> [-o <output_csv_file_path>] 

Q: What is the default output CSV file path if I don't specify it using the  -o  or  --output_file  flag?
A: The default output CSV file path is  metrics_output.csv .

Q: What error is raised if the provided event file does not exist?
A: A  FileNotFoundError  is raised if the provided event file does not exist.

Q: How does the script handle errors during event file processing or CSV file saving?
A: The script raises a  RuntimeError  if an error occurs during event file processing or CSV file saving, and it also prints error messages for specific exceptions, such as  FileNotFoundError  or  Exception  during CSV file saving.

Q: Which library is used by the script to handle data manipulation?
A: The  pandas  library is used by the script to handle data manipulation.

Q: What flag do I use in the  SOAP  section to indicate whether the system is periodic?
A: The flag to indicate whether the system is periodic is  periodic .

Q: How do I save the modified  preprocess_config.yaml  file after making changes to the desired settings for data preprocessing?
A: You should save the file as  preprocess_config.yaml  in the  MLFF_QD/config_files/preprocessing  directory.

Q: What parameter do I use in the  dataset  section to specify the fraction of outliers removed by Isolation Forest?
A: The parameter to specify the fraction of outliers removed by Isolation Forest is  contamination .

Q: How do I specify the tasks to perform in the Allegro model training process?
A: You specify the tasks to perform by setting the "run" parameter in the  allegro.yaml  file.

Q: What is the maximum distance for neighbor list calculations in the Allegro model?
A: The maximum distance for neighbor list calculations is specified by the "cutoff_radius" parameter.

Q: What happens if there is a conflict between keys in the  common  and  overrides  sections of the  input.yaml  file?
A: In case of conflict, keys from  common  always take priority over  overrides .

Q: How do I specify the machine learning engine to use in the  input.yaml  file?
A: You specify the machine learning engine to use in the  platform  parameter.

Q: How do I override common settings with engine-specific settings in the  input.yaml  file?
A: You use dot notation for override keys, such as  model.n_rbf , in the  overrides  section under your desired engine section:
   
   override:
	  schnet:
		model.n_rbf
   

Q: What happens if a key is not present in the engine template in the  input.yaml  file?
A: Keys not present in the engine template will be ignored (with a warning).

Q: Where would I adjust the output directory settings in the  input.yaml  file?
A: You would adjust the output directory settings in the  output  section.

Q: What command is used to submit the job to the SLURM cluster after updating the script?
A: The command used to submit the job is  sbatch run_training.sh .

Q: What is the required configuration file to run the  training_inference.sh  script, and how do I provide it?
A: The required configuration file is  input.yaml , which must be provided as an argument when running the script.

Q: If I want to generate the engine YAML file but not run training, which flag should I use?
A: You should use the  --only-generate  flag to generate the engine YAML file but not run training.

Q: How can I override the default engine to use the Allegro engine?
A: You can override the default engine by using the  --engine  argument with the value  allegro .


Q: Show me the full content of preprocess_config.yaml.
A: Here is the content of  preprocess_config.yaml :
   yaml
# ───────────────────────────────────────────────
#                 Dataset Input
#  Choose ONE of the two options below for your data.
# ───────────────────────────────────────────────
dataset:
  # --- Option A: Use a single, pre-combined file (Recommended) ---
  # Uncomment 'input_file' if you have one file with positions, forces, and energy.
  # input_file: "dataset_pos_frc_ev.xyz"

  # --- Option B: Auto combine separate position and force files ---
  # Use these lines if your positions and forces are in two different files.
   pos_file: "HgTe-pos-1.xyz"
   frc_file: "HgTe-frc-1.xyz"
  
   output_prefix: "consolidated_dataset" 
   
   sizes: [200,500]
  # Subset counts (number of structures from each method)
   subset_counts:
     MD: 4036  # Structures obtained from Molecular Dynamics (MD) simulation 
   contamination: 0.05 # Fraction of outliers removed by Isolation Forest 
   
# ───────────────────────────────────────────────
# Optional SOAP descriptor settings
# (You can safely remove this section to let MLFF-QD
#  auto-detect species and use default SOAP parameters)
# ───────────────────────────────────────────────
SOAP: 
   species: ["Hg", "Cl", "Te"] 
   r_cut: 12.0 
   n_max: 7 
   l_max: 3 
   sigma: 0.1 
   periodic: False
   sparse: False
   

Q: How do I set the  pos_file  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  pos_file  parameter is located under  dataset.pos_file . Current default:  HgTe-pos-1.xyz .

Q: What is the default value for  dataset.pos_file  in preprocess_config.yaml?
A: The default value is  HgTe-pos-1.xyz .

Q: How do I set the  frc_file  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  frc_file  parameter is located under  dataset.frc_file . Current default:  HgTe-frc-1.xyz .

Q: What is the default value for  dataset.frc_file  in preprocess_config.yaml?
A: The default value is  HgTe-frc-1.xyz .

Q: How do I set the  output_prefix  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  output_prefix  parameter is located under  dataset.output_prefix . Current default:  consolidated_dataset .

Q: What is the default value for  dataset.output_prefix  in preprocess_config.yaml?
A: The default value is  consolidated_dataset .

Q: How do I set the  sizes  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  sizes  parameter is located under  dataset.sizes . Current default:  [200, 500] .

Q: What is the default value for  dataset.sizes  in preprocess_config.yaml?
A: The default value is  [200, 500] .

Q: How do I set the  MD  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  MD  parameter is located under  dataset.subset_counts.MD . Current default:  4036 .

Q: What is the default value for  dataset.subset_counts.MD  in preprocess_config.yaml?
A: The default value is  4036 .

Q: How do I set the  contamination  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  contamination  parameter is located under  dataset.contamination . Current default:  0.05 .

Q: What is the default value for  dataset.contamination  in preprocess_config.yaml?
A: The default value is  0.05 .

Q: How do I set the  species  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  species  parameter is located under  SOAP.species . Current default:  ['Hg', 'Cl', 'Te'] .

Q: What is the default value for  SOAP.species  in preprocess_config.yaml?
A: The default value is  ['Hg', 'Cl', 'Te'] .

Q: How do I set the  r_cut  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  r_cut  parameter is located under  SOAP.r_cut . Current default:  12.0 .

Q: What is the default value for  SOAP.r_cut  in preprocess_config.yaml?
A: The default value is  12.0 .

Q: How do I set the  n_max  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  n_max  parameter is located under  SOAP.n_max . Current default:  7 .

Q: What is the default value for  SOAP.n_max  in preprocess_config.yaml?
A: The default value is  7 .

Q: How do I set the  l_max  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  l_max  parameter is located under  SOAP.l_max . Current default:  3 .

Q: What is the default value for  SOAP.l_max  in preprocess_config.yaml?
A: The default value is  3 .

Q: How do I set the  sigma  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  sigma  parameter is located under  SOAP.sigma . Current default:  0.1 .

Q: What is the default value for  SOAP.sigma  in preprocess_config.yaml?
A: The default value is  0.1 .

Q: How do I set the  periodic  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  periodic  parameter is located under  SOAP.periodic . Current default:  False .

Q: What is the default value for  SOAP.periodic  in preprocess_config.yaml?
A: The default value is  False .

Q: How do I set the  sparse  in  preprocess_config.yaml ?
A: In  preprocess_config.yaml , the  sparse  parameter is located under  SOAP.sparse . Current default:  False .

Q: What is the default value for  SOAP.sparse  in preprocess_config.yaml?
A: The default value is  False .

Q: Show me the full content of allegro.yaml.
A: Here is the content of  allegro.yaml :
   yaml
run: [train, val, test]


cutoff_radius: 12.0
chemical_symbols: [Cd, Cl, Se] 
model_type_names: ${chemical_symbols}

data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 456             
  split_dataset:
    file_path:  #./consolidated_dataset_1000_CdSe_new.xyz
    train: 0.8
    val: 0.1
    test: 0.1
  transforms:
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}
  
  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
  test_dataloader: ${data.val_dataloader}
  stats_manager:
    _target_: nequip.data.CommonDataStatisticsManager
    type_names: ${model_type_names}

trainer:
  _target_: lightning.Trainer
  accelerator: auto
  devices: 1
  max_epochs: 3
  check_val_every_n_epoch: 1
  log_every_n_steps: 5
  
  callbacks:
      
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ./results 
      filename: best
      save_last: true
    
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      min_delta: 1e-3                         # how much to be considered a "change"
      patience: 20                            # how many instances of "no change" before stopping
      
  logger:
    - _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ./logs
      name: tutorial_log
      version: null
      default_hp_metric: false


# NOTE:
# interpolation parameters for Allegro model
num_scalar_features: 64


training_module:
  _target_: nequip.train.EMALightningModule
  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 0.05
      forces: 0.95
  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      per_atom_energy_mae: 0.05
      forces_mae: 0.95
  test_metrics: ${training_module.val_metrics}
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
  # ^ IMPORTANT: Allegro models do better with learning rates around 1e-3

  # to use the Allegro model in the NequIP framework, the following  model  block has to be changed to be that of Allegro's
  model:
    _target_: allegro.model.AllegroModel

    # === basic model params ===
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    # === two-body scalar embedding ===
    radial_chemical_embed:
      # the defaults for the Bessel embedding module are usually appropriate
      _target_: allegro.nn.TwoBodyBesselScalarEmbed
      num_bessels: 8
      bessel_trainable: false
      polynomial_cutoff_p: 6

    # output dimension of the radial-chemical embedding
    radial_chemical_embed_dim: ${num_scalar_features}

    # scalar embedding MLP
    scalar_embed_mlp_hidden_layers_depth: 1
    scalar_embed_mlp_hidden_layers_width: ${num_scalar_features}
    scalar_embed_mlp_nonlinearity: silu

    # === core hyperparameters ===
    # The following hyperparameters are the main ones that one should focus on tuning.

    # maximum order l to use in spherical harmonics embedding, 1 is baseline (fast), 2 is more accurate, but slower, 3 highly accurate but slow
    l_max: 1

    # number of tensor product layers, 1-3 usually best, more is more accurate but slower
    num_layers: 2

    # number of scalar features, more is more accurate but slower
    # 16, 32, 64, 128, 256 are good options to try depending on the dataset
    num_scalar_features: ${num_scalar_features}

    # number of tensor features, more is more accurate but slower
    # 8, 16, 32, 64 are good options to try depending on the dataset
    num_tensor_features: 32

    # == allegro MLPs ==
    # neural network parameters in the Allegro layers
    allegro_mlp_hidden_layers_depth: 1
    allegro_mlp_hidden_layers_width: ${num_scalar_features}
    allegro_mlp_nonlinearity: silu
    # ^ setting  nonlinearity  to  null  means that the Allegro MLPs are effectively linear layers

    # === advanced hyperparameters ===
    # The following hyperparameters should remain in their default states until the above core hyperparameters have been set.

    # whether to include features with odd mirror parity
    # often turning parity off gives equally good results but faster networks, so do consider this
    parity: true

    # whether the tensor product weights couple the paths and channels or not (otherwise the weights are only applied per-path)
    # default is  true , which is expected to be more expressive than  false 
    tp_path_channel_coupling: true

    # == readout MLP ==
    # neural network parameters in the readout layer
    readout_mlp_hidden_layers_depth: 1
    readout_mlp_hidden_layers_width: ${num_scalar_features}
    readout_mlp_nonlinearity: silu
    # ^ setting  nonlinearity  to  null  means that output MLP is effectively a linear layer

    # === misc hyperparameters ===
    # average number of neighbors for edge sum normalization
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}

    # per-type per-atom scales and shifts
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    # ^ this should typically be the isolated atom energies for your dataset
    #   provided as a dict, e.g.
    # per_type_energy_shifts: 
    #   C: 1.234
    #   H: 2.345
    #   O: 3.456
    per_type_energy_scales: ${training_data_stats:forces_rms}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    # ZBL pair potential (optional, can be removed or included depending on aplication)
    # see NequIP docs for details:
    # https://nequip.readthedocs.io/en/latest/api/nn.html#nequip.nn.pair_potential.ZBL
    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: real     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types


global_options:
  allow_tf32: false
   

Q: How do I set the  run  in  allegro.yaml ?
A: In  allegro.yaml , the  run  parameter is located under  run . Current default:  ['train', 'val', 'test'] .

Q: What is the default value for  run  in allegro.yaml?
A: The default value is  ['train', 'val', 'test'] .

Q: How do I set the  cutoff_radius  in  allegro.yaml ?
A: In  allegro.yaml , the  cutoff_radius  parameter is located under  cutoff_radius . Current default:  12.0 .

Q: What is the default value for  cutoff_radius  in allegro.yaml?
A: The default value is  12.0 .

Q: How do I set the  chemical_symbols  in  allegro.yaml ?
A: In  allegro.yaml , the  chemical_symbols  parameter is located under  chemical_symbols . Current default:  ['Cd', 'Cl', 'Se'] .

Q: What is the default value for  chemical_symbols  in allegro.yaml?
A: The default value is  ['Cd', 'Cl', 'Se'] .

Q: How do I set the  model_type_names  in  allegro.yaml ?
A: In  allegro.yaml , the  model_type_names  parameter is located under  model_type_names . Current default:  ${chemical_symbols} .

Q: What is the default value for  model_type_names  in allegro.yaml?
A: The default value is  ${chemical_symbols} .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  data._target_ . Current default:  nequip.data.datamodule.ASEDataModule .

Q: What is the default value for  data._target_  in allegro.yaml?
A: The default value is  nequip.data.datamodule.ASEDataModule .

Q: How do I set the  seed  in  allegro.yaml ?
A: In  allegro.yaml , the  seed  parameter is located under  data.seed . Current default:  456 .

Q: What is the default value for  data.seed  in allegro.yaml?
A: The default value is  456 .

Q: How do I set the  file_path  in  allegro.yaml ?
A: In  allegro.yaml , the  file_path  parameter is located under  data.split_dataset.file_path . Current default:  None .

Q: What is the default value for  data.split_dataset.file_path  in allegro.yaml?
A: The default value is  None .

Q: How do I set the  train  in  allegro.yaml ?
A: In  allegro.yaml , the  train  parameter is located under  data.split_dataset.train . Current default:  0.8 .

Q: What is the default value for  data.split_dataset.train  in allegro.yaml?
A: The default value is  0.8 .

Q: How do I set the  val  in  allegro.yaml ?
A: In  allegro.yaml , the  val  parameter is located under  data.split_dataset.val . Current default:  0.1 .

Q: What is the default value for  data.split_dataset.val  in allegro.yaml?
A: The default value is  0.1 .

Q: How do I set the  test  in  allegro.yaml ?
A: In  allegro.yaml , the  test  parameter is located under  data.split_dataset.test . Current default:  0.1 .

Q: What is the default value for  data.split_dataset.test  in allegro.yaml?
A: The default value is  0.1 .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  data.train_dataloader._target_ . Current default:  torch.utils.data.DataLoader .

Q: What is the default value for  data.train_dataloader._target_  in allegro.yaml?
A: The default value is  torch.utils.data.DataLoader .

Q: How do I set the  batch_size  in  allegro.yaml ?
A: In  allegro.yaml , the  batch_size  parameter is located under  data.train_dataloader.batch_size . Current default:  16 .

Q: What is the default value for  data.train_dataloader.batch_size  in allegro.yaml?
A: The default value is  16 .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  data.val_dataloader._target_ . Current default:  torch.utils.data.DataLoader .

Q: What is the default value for  data.val_dataloader._target_  in allegro.yaml?
A: The default value is  torch.utils.data.DataLoader .

Q: How do I set the  batch_size  in  allegro.yaml ?
A: In  allegro.yaml , the  batch_size  parameter is located under  data.val_dataloader.batch_size . Current default:  16 .

Q: What is the default value for  data.val_dataloader.batch_size  in allegro.yaml?
A: The default value is  16 .

Q: How do I set the  test_dataloader  in  allegro.yaml ?
A: In  allegro.yaml , the  test_dataloader  parameter is located under  data.test_dataloader . Current default:  ${data.val_dataloader} .

Q: What is the default value for  data.test_dataloader  in allegro.yaml?
A: The default value is  ${data.val_dataloader} .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  data.stats_manager._target_ . Current default:  nequip.data.CommonDataStatisticsManager .

Q: What is the default value for  data.stats_manager._target_  in allegro.yaml?
A: The default value is  nequip.data.CommonDataStatisticsManager .

Q: How do I set the  type_names  in  allegro.yaml ?
A: In  allegro.yaml , the  type_names  parameter is located under  data.stats_manager.type_names . Current default:  ${model_type_names} .

Q: What is the default value for  data.stats_manager.type_names  in allegro.yaml?
A: The default value is  ${model_type_names} .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  trainer._target_ . Current default:  lightning.Trainer .

Q: What is the default value for  trainer._target_  in allegro.yaml?
A: The default value is  lightning.Trainer .

Q: How do I set the  accelerator  in  allegro.yaml ?
A: In  allegro.yaml , the  accelerator  parameter is located under  trainer.accelerator . Current default:  auto .

Q: What is the default value for  trainer.accelerator  in allegro.yaml?
A: The default value is  auto .

Q: How do I set the  devices  in  allegro.yaml ?
A: In  allegro.yaml , the  devices  parameter is located under  trainer.devices . Current default:  1 .

Q: What is the default value for  trainer.devices  in allegro.yaml?
A: The default value is  1 .

Q: How do I set the  max_epochs  in  allegro.yaml ?
A: In  allegro.yaml , the  max_epochs  parameter is located under  trainer.max_epochs . Current default:  3 .

Q: What is the default value for  trainer.max_epochs  in allegro.yaml?
A: The default value is  3 .

Q: How do I set the  check_val_every_n_epoch  in  allegro.yaml ?
A: In  allegro.yaml , the  check_val_every_n_epoch  parameter is located under  trainer.check_val_every_n_epoch . Current default:  1 .

Q: What is the default value for  trainer.check_val_every_n_epoch  in allegro.yaml?
A: The default value is  1 .

Q: How do I set the  log_every_n_steps  in  allegro.yaml ?
A: In  allegro.yaml , the  log_every_n_steps  parameter is located under  trainer.log_every_n_steps . Current default:  5 .

Q: What is the default value for  trainer.log_every_n_steps  in allegro.yaml?
A: The default value is  5 .

Q: How do I set the  num_scalar_features  in  allegro.yaml ?
A: In  allegro.yaml , the  num_scalar_features  parameter is located under  num_scalar_features . Current default:  64 .

Q: What is the default value for  num_scalar_features  in allegro.yaml?
A: The default value is  64 .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module._target_ . Current default:  nequip.train.EMALightningModule .

Q: What is the default value for  training_module._target_  in allegro.yaml?
A: The default value is  nequip.train.EMALightningModule .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module.loss._target_ . Current default:  nequip.train.EnergyForceLoss .

Q: What is the default value for  training_module.loss._target_  in allegro.yaml?
A: The default value is  nequip.train.EnergyForceLoss .

Q: How do I set the  per_atom_energy  in  allegro.yaml ?
A: In  allegro.yaml , the  per_atom_energy  parameter is located under  training_module.loss.per_atom_energy . Current default:  True .

Q: What is the default value for  training_module.loss.per_atom_energy  in allegro.yaml?
A: The default value is  True .

Q: How do I set the  total_energy  in  allegro.yaml ?
A: In  allegro.yaml , the  total_energy  parameter is located under  training_module.loss.coeffs.total_energy . Current default:  0.05 .

Q: What is the default value for  training_module.loss.coeffs.total_energy  in allegro.yaml?
A: The default value is  0.05 .

Q: How do I set the  forces  in  allegro.yaml ?
A: In  allegro.yaml , the  forces  parameter is located under  training_module.loss.coeffs.forces . Current default:  0.95 .

Q: What is the default value for  training_module.loss.coeffs.forces  in allegro.yaml?
A: The default value is  0.95 .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module.val_metrics._target_ . Current default:  nequip.train.EnergyForceMetrics .

Q: What is the default value for  training_module.val_metrics._target_  in allegro.yaml?
A: The default value is  nequip.train.EnergyForceMetrics .

Q: How do I set the  per_atom_energy_mae  in  allegro.yaml ?
A: In  allegro.yaml , the  per_atom_energy_mae  parameter is located under  training_module.val_metrics.coeffs.per_atom_energy_mae . Current default:  0.05 .

Q: What is the default value for  training_module.val_metrics.coeffs.per_atom_energy_mae  in allegro.yaml?
A: The default value is  0.05 .

Q: How do I set the  forces_mae  in  allegro.yaml ?
A: In  allegro.yaml , the  forces_mae  parameter is located under  training_module.val_metrics.coeffs.forces_mae . Current default:  0.95 .

Q: What is the default value for  training_module.val_metrics.coeffs.forces_mae  in allegro.yaml?
A: The default value is  0.95 .

Q: How do I set the  test_metrics  in  allegro.yaml ?
A: In  allegro.yaml , the  test_metrics  parameter is located under  training_module.test_metrics . Current default:  ${training_module.val_metrics} .

Q: What is the default value for  training_module.test_metrics  in allegro.yaml?
A: The default value is  ${training_module.val_metrics} .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module.optimizer._target_ . Current default:  torch.optim.Adam .

Q: What is the default value for  training_module.optimizer._target_  in allegro.yaml?
A: The default value is  torch.optim.Adam .

Q: How do I set the  lr  in  allegro.yaml ?
A: In  allegro.yaml , the  lr  parameter is located under  training_module.optimizer.lr . Current default:  0.001 .

Q: What is the default value for  training_module.optimizer.lr  in allegro.yaml?
A: The default value is  0.001 .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module.model._target_ . Current default:  allegro.model.AllegroModel .

Q: What is the default value for  training_module.model._target_  in allegro.yaml?
A: The default value is  allegro.model.AllegroModel .

Q: How do I set the  seed  in  allegro.yaml ?
A: In  allegro.yaml , the  seed  parameter is located under  training_module.model.seed . Current default:  456 .

Q: What is the default value for  training_module.model.seed  in allegro.yaml?
A: The default value is  456 .

Q: How do I set the  model_dtype  in  allegro.yaml ?
A: In  allegro.yaml , the  model_dtype  parameter is located under  training_module.model.model_dtype . Current default:  float32 .

Q: What is the default value for  training_module.model.model_dtype  in allegro.yaml?
A: The default value is  float32 .

Q: How do I set the  type_names  in  allegro.yaml ?
A: In  allegro.yaml , the  type_names  parameter is located under  training_module.model.type_names . Current default:  ${model_type_names} .

Q: What is the default value for  training_module.model.type_names  in allegro.yaml?
A: The default value is  ${model_type_names} .

Q: How do I set the  r_max  in  allegro.yaml ?
A: In  allegro.yaml , the  r_max  parameter is located under  training_module.model.r_max . Current default:  ${cutoff_radius} .

Q: What is the default value for  training_module.model.r_max  in allegro.yaml?
A: The default value is  ${cutoff_radius} .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module.model.radial_chemical_embed._target_ . Current default:  allegro.nn.TwoBodyBesselScalarEmbed .

Q: What is the default value for  training_module.model.radial_chemical_embed._target_  in allegro.yaml?
A: The default value is  allegro.nn.TwoBodyBesselScalarEmbed .

Q: How do I set the  num_bessels  in  allegro.yaml ?
A: In  allegro.yaml , the  num_bessels  parameter is located under  training_module.model.radial_chemical_embed.num_bessels . Current default:  8 .

Q: What is the default value for  training_module.model.radial_chemical_embed.num_bessels  in allegro.yaml?
A: The default value is  8 .

Q: How do I set the  bessel_trainable  in  allegro.yaml ?
A: In  allegro.yaml , the  bessel_trainable  parameter is located under  training_module.model.radial_chemical_embed.bessel_trainable . Current default:  False .

Q: What is the default value for  training_module.model.radial_chemical_embed.bessel_trainable  in allegro.yaml?
A: The default value is  False .

Q: How do I set the  polynomial_cutoff_p  in  allegro.yaml ?
A: In  allegro.yaml , the  polynomial_cutoff_p  parameter is located under  training_module.model.radial_chemical_embed.polynomial_cutoff_p . Current default:  6 .

Q: What is the default value for  training_module.model.radial_chemical_embed.polynomial_cutoff_p  in allegro.yaml?
A: The default value is  6 .

Q: How do I set the  radial_chemical_embed_dim  in  allegro.yaml ?
A: In  allegro.yaml , the  radial_chemical_embed_dim  parameter is located under  training_module.model.radial_chemical_embed_dim . Current default:  ${num_scalar_features} .

Q: What is the default value for  training_module.model.radial_chemical_embed_dim  in allegro.yaml?
A: The default value is  ${num_scalar_features} .

Q: How do I set the  scalar_embed_mlp_hidden_layers_depth  in  allegro.yaml ?
A: In  allegro.yaml , the  scalar_embed_mlp_hidden_layers_depth  parameter is located under  training_module.model.scalar_embed_mlp_hidden_layers_depth . Current default:  1 .

Q: What is the default value for  training_module.model.scalar_embed_mlp_hidden_layers_depth  in allegro.yaml?
A: The default value is  1 .

Q: How do I set the  scalar_embed_mlp_hidden_layers_width  in  allegro.yaml ?
A: In  allegro.yaml , the  scalar_embed_mlp_hidden_layers_width  parameter is located under  training_module.model.scalar_embed_mlp_hidden_layers_width . Current default:  ${num_scalar_features} .

Q: What is the default value for  training_module.model.scalar_embed_mlp_hidden_layers_width  in allegro.yaml?
A: The default value is  ${num_scalar_features} .

Q: How do I set the  scalar_embed_mlp_nonlinearity  in  allegro.yaml ?
A: In  allegro.yaml , the  scalar_embed_mlp_nonlinearity  parameter is located under  training_module.model.scalar_embed_mlp_nonlinearity . Current default:  silu .

Q: What is the default value for  training_module.model.scalar_embed_mlp_nonlinearity  in allegro.yaml?
A: The default value is  silu .

Q: How do I set the  l_max  in  allegro.yaml ?
A: In  allegro.yaml , the  l_max  parameter is located under  training_module.model.l_max . Current default:  1 .

Q: What is the default value for  training_module.model.l_max  in allegro.yaml?
A: The default value is  1 .

Q: How do I set the  num_layers  in  allegro.yaml ?
A: In  allegro.yaml , the  num_layers  parameter is located under  training_module.model.num_layers . Current default:  2 .

Q: What is the default value for  training_module.model.num_layers  in allegro.yaml?
A: The default value is  2 .

Q: How do I set the  num_scalar_features  in  allegro.yaml ?
A: In  allegro.yaml , the  num_scalar_features  parameter is located under  training_module.model.num_scalar_features . Current default:  ${num_scalar_features} .

Q: What is the default value for  training_module.model.num_scalar_features  in allegro.yaml?
A: The default value is  ${num_scalar_features} .

Q: How do I set the  num_tensor_features  in  allegro.yaml ?
A: In  allegro.yaml , the  num_tensor_features  parameter is located under  training_module.model.num_tensor_features . Current default:  32 .

Q: What is the default value for  training_module.model.num_tensor_features  in allegro.yaml?
A: The default value is  32 .

Q: How do I set the  allegro_mlp_hidden_layers_depth  in  allegro.yaml ?
A: In  allegro.yaml , the  allegro_mlp_hidden_layers_depth  parameter is located under  training_module.model.allegro_mlp_hidden_layers_depth . Current default:  1 .

Q: What is the default value for  training_module.model.allegro_mlp_hidden_layers_depth  in allegro.yaml?
A: The default value is  1 .

Q: How do I set the  allegro_mlp_hidden_layers_width  in  allegro.yaml ?
A: In  allegro.yaml , the  allegro_mlp_hidden_layers_width  parameter is located under  training_module.model.allegro_mlp_hidden_layers_width . Current default:  ${num_scalar_features} .

Q: What is the default value for  training_module.model.allegro_mlp_hidden_layers_width  in allegro.yaml?
A: The default value is  ${num_scalar_features} .

Q: How do I set the  allegro_mlp_nonlinearity  in  allegro.yaml ?
A: In  allegro.yaml , the  allegro_mlp_nonlinearity  parameter is located under  training_module.model.allegro_mlp_nonlinearity . Current default:  silu .

Q: What is the default value for  training_module.model.allegro_mlp_nonlinearity  in allegro.yaml?
A: The default value is  silu .

Q: How do I set the  parity  in  allegro.yaml ?
A: In  allegro.yaml , the  parity  parameter is located under  training_module.model.parity . Current default:  True .

Q: What is the default value for  training_module.model.parity  in allegro.yaml?
A: The default value is  True .

Q: How do I set the  tp_path_channel_coupling  in  allegro.yaml ?
A: In  allegro.yaml , the  tp_path_channel_coupling  parameter is located under  training_module.model.tp_path_channel_coupling . Current default:  True .

Q: What is the default value for  training_module.model.tp_path_channel_coupling  in allegro.yaml?
A: The default value is  True .

Q: How do I set the  readout_mlp_hidden_layers_depth  in  allegro.yaml ?
A: In  allegro.yaml , the  readout_mlp_hidden_layers_depth  parameter is located under  training_module.model.readout_mlp_hidden_layers_depth . Current default:  1 .

Q: What is the default value for  training_module.model.readout_mlp_hidden_layers_depth  in allegro.yaml?
A: The default value is  1 .

Q: How do I set the  readout_mlp_hidden_layers_width  in  allegro.yaml ?
A: In  allegro.yaml , the  readout_mlp_hidden_layers_width  parameter is located under  training_module.model.readout_mlp_hidden_layers_width . Current default:  ${num_scalar_features} .

Q: What is the default value for  training_module.model.readout_mlp_hidden_layers_width  in allegro.yaml?
A: The default value is  ${num_scalar_features} .

Q: How do I set the  readout_mlp_nonlinearity  in  allegro.yaml ?
A: In  allegro.yaml , the  readout_mlp_nonlinearity  parameter is located under  training_module.model.readout_mlp_nonlinearity . Current default:  silu .

Q: What is the default value for  training_module.model.readout_mlp_nonlinearity  in allegro.yaml?
A: The default value is  silu .

Q: How do I set the  avg_num_neighbors  in  allegro.yaml ?
A: In  allegro.yaml , the  avg_num_neighbors  parameter is located under  training_module.model.avg_num_neighbors . Current default:  ${training_data_stats:num_neighbors_mean} .

Q: What is the default value for  training_module.model.avg_num_neighbors  in allegro.yaml?
A: The default value is  ${training_data_stats:num_neighbors_mean} .

Q: How do I set the  per_type_energy_shifts  in  allegro.yaml ?
A: In  allegro.yaml , the  per_type_energy_shifts  parameter is located under  training_module.model.per_type_energy_shifts . Current default:  ${training_data_stats:per_atom_energy_mean} .

Q: What is the default value for  training_module.model.per_type_energy_shifts  in allegro.yaml?
A: The default value is  ${training_data_stats:per_atom_energy_mean} .

Q: How do I set the  per_type_energy_scales  in  allegro.yaml ?
A: In  allegro.yaml , the  per_type_energy_scales  parameter is located under  training_module.model.per_type_energy_scales . Current default:  ${training_data_stats:forces_rms} .

Q: What is the default value for  training_module.model.per_type_energy_scales  in allegro.yaml?
A: The default value is  ${training_data_stats:forces_rms} .

Q: How do I set the  per_type_energy_scales_trainable  in  allegro.yaml ?
A: In  allegro.yaml , the  per_type_energy_scales_trainable  parameter is located under  training_module.model.per_type_energy_scales_trainable . Current default:  False .

Q: What is the default value for  training_module.model.per_type_energy_scales_trainable  in allegro.yaml?
A: The default value is  False .

Q: How do I set the  per_type_energy_shifts_trainable  in  allegro.yaml ?
A: In  allegro.yaml , the  per_type_energy_shifts_trainable  parameter is located under  training_module.model.per_type_energy_shifts_trainable . Current default:  False .

Q: What is the default value for  training_module.model.per_type_energy_shifts_trainable  in allegro.yaml?
A: The default value is  False .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module.model.pair_potential._target_ . Current default:  nequip.nn.pair_potential.ZBL .

Q: What is the default value for  training_module.model.pair_potential._target_  in allegro.yaml?
A: The default value is  nequip.nn.pair_potential.ZBL .

Q: How do I set the  units  in  allegro.yaml ?
A: In  allegro.yaml , the  units  parameter is located under  training_module.model.pair_potential.units . Current default:  real .

Q: What is the default value for  training_module.model.pair_potential.units  in allegro.yaml?
A: The default value is  real .

Q: How do I set the  chemical_species  in  allegro.yaml ?
A: In  allegro.yaml , the  chemical_species  parameter is located under  training_module.model.pair_potential.chemical_species . Current default:  ${chemical_symbols} .

Q: What is the default value for  training_module.model.pair_potential.chemical_species  in allegro.yaml?
A: The default value is  ${chemical_symbols} .

Q: How do I set the  allow_tf32  in  allegro.yaml ?
A: In  allegro.yaml , the  allow_tf32  parameter is located under  global_options.allow_tf32 . Current default:  False .

Q: What is the default value for  global_options.allow_tf32  in allegro.yaml?
A: The default value is  False .

Q: Show me the full content of fusion.yaml.
A: Here is the content of  fusion.yaml :
   yaml
# General settings
general:
    seed: 42  # Random seed for reproducibility
    database_name: 'CdSe.db'  # Name of the database file

# Data handling settings
data:
    dataset_path: 
    use_last_n: 100  # current testing
# Model architecture settings
model:
    model_type: nequip_mace_interaction_fusion   # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.
    n_rbf: 40
    n_atom_basis: 192
    n_interactions: 2
    
    dropout_rate: null # Default is 0.1 but it will not work when you select only 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null # [256, 128, 64, 32] (if null then n_neurons == n_atom_basis)
    distance_unit: 'Ang'
    property_unit_dict:
      energy: 'eV'
      forces: 'eV/Ang'

# Output settings
outputs:
    energy:
      loss_weight: 0.05
      metrics: "MAE"
    forces:
      loss_weight: 0.95
      metrics: "MAE"

# Training settings
training:
    accelerator: 'gpu'
    devices: 1
    precision: 32
    batch_size: 16
    num_train: 800
    num_val: 200
    max_epochs: 3
    num_workers: 24
    pin_memory: true
    optimizer:
      type: 'AdamW'
      lr: 0.0001 #changed 0.0001 to 0.001
    scheduler:
      type: 'ReduceLROnPlateau'
      factor: 0.8
      patience: 30
      verbose: true

# Logging and checkpoint settings
logging:
    folder: './results'
    log_dir: "lightning_logs"
    checkpoint_dir: "best_inference_model"
    monitor: "val_loss"

# Testing settings
testing:
    trained_model_path: './results'  # Path to load the trained model
    csv_file_name: 'actual_vs_predicted_enrgforc.csv'  # Path to save the predictions CSV

# Resume training settings
resume_training:
    resume_checkpoint_dir: null # Path to model checkpoint file if required 

# Fine Tuning settings
fine_tuning:
    pretrained_checkpoint: # checkpoint file path
    freeze_embedding: true
    freeze_interactions_up_to: 2 # add no of layers
    freeze_all_representation: true
    lr: 0.00005
    early_stopping_patience: 10
    best_model_dir: fine_tuned_best_model  # Subdirectory only
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory only
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
   

Q: How do I set the  seed  in  fusion.yaml ?
A: In  fusion.yaml , the  seed  parameter is located under  general.seed . Current default:  42 .

Q: What is the default value for  general.seed  in fusion.yaml?
A: The default value is  42 .

Q: How do I set the  database_name  in  fusion.yaml ?
A: In  fusion.yaml , the  database_name  parameter is located under  general.database_name . Current default:  CdSe.db .

Q: What is the default value for  general.database_name  in fusion.yaml?
A: The default value is  CdSe.db .

Q: How do I set the  dataset_path  in  fusion.yaml ?
A: In  fusion.yaml , the  dataset_path  parameter is located under  data.dataset_path . Current default:  None .

Q: What is the default value for  data.dataset_path  in fusion.yaml?
A: The default value is  None .

Q: How do I set the  use_last_n  in  fusion.yaml ?
A: In  fusion.yaml , the  use_last_n  parameter is located under  data.use_last_n . Current default:  100 .

Q: What is the default value for  data.use_last_n  in fusion.yaml?
A: The default value is  100 .

Q: How do I set the  model_type  in  fusion.yaml ?
A: In  fusion.yaml , the  model_type  parameter is located under  model.model_type . Current default:  nequip_mace_interaction_fusion .

Q: What is the default value for  model.model_type  in fusion.yaml?
A: The default value is  nequip_mace_interaction_fusion .

Q: How do I set the  cutoff  in  fusion.yaml ?
A: In  fusion.yaml , the  cutoff  parameter is located under  model.cutoff . Current default:  12.0 .

Q: What is the default value for  model.cutoff  in fusion.yaml?
A: The default value is  12.0 .

Q: How do I set the  n_rbf  in  fusion.yaml ?
A: In  fusion.yaml , the  n_rbf  parameter is located under  model.n_rbf . Current default:  40 .

Q: What is the default value for  model.n_rbf  in fusion.yaml?
A: The default value is  40 .

Q: How do I set the  n_atom_basis  in  fusion.yaml ?
A: In  fusion.yaml , the  n_atom_basis  parameter is located under  model.n_atom_basis . Current default:  192 .

Q: What is the default value for  model.n_atom_basis  in fusion.yaml?
A: The default value is  192 .

Q: How do I set the  n_interactions  in  fusion.yaml ?
A: In  fusion.yaml , the  n_interactions  parameter is located under  model.n_interactions . Current default:  2 .

Q: What is the default value for  model.n_interactions  in fusion.yaml?
A: The default value is  2 .

Q: How do I set the  dropout_rate  in  fusion.yaml ?
A: In  fusion.yaml , the  dropout_rate  parameter is located under  model.dropout_rate . Current default:  None .

Q: What is the default value for  model.dropout_rate  in fusion.yaml?
A: The default value is  None .

Q: How do I set the  n_layers  in  fusion.yaml ?
A: In  fusion.yaml , the  n_layers  parameter is located under  model.n_layers . Current default:  1 .

Q: What is the default value for  model.n_layers  in fusion.yaml?
A: The default value is  1 .

Q: How do I set the  n_neurons  in  fusion.yaml ?
A: In  fusion.yaml , the  n_neurons  parameter is located under  model.n_neurons . Current default:  None .

Q: What is the default value for  model.n_neurons  in fusion.yaml?
A: The default value is  None .

Q: How do I set the  distance_unit  in  fusion.yaml ?
A: In  fusion.yaml , the  distance_unit  parameter is located under  model.distance_unit . Current default:  Ang .

Q: What is the default value for  model.distance_unit  in fusion.yaml?
A: The default value is  Ang .

Q: How do I set the  energy  in  fusion.yaml ?
A: In  fusion.yaml , the  energy  parameter is located under  model.property_unit_dict.energy . Current default:  eV .

Q: What is the default value for  model.property_unit_dict.energy  in fusion.yaml?
A: The default value is  eV .

Q: How do I set the  forces  in  fusion.yaml ?
A: In  fusion.yaml , the  forces  parameter is located under  model.property_unit_dict.forces . Current default:  eV/Ang .

Q: What is the default value for  model.property_unit_dict.forces  in fusion.yaml?
A: The default value is  eV/Ang .

Q: How do I set the  loss_weight  in  fusion.yaml ?
A: In  fusion.yaml , the  loss_weight  parameter is located under  outputs.energy.loss_weight . Current default:  0.05 .

Q: What is the default value for  outputs.energy.loss_weight  in fusion.yaml?
A: The default value is  0.05 .

Q: How do I set the  metrics  in  fusion.yaml ?
A: In  fusion.yaml , the  metrics  parameter is located under  outputs.energy.metrics . Current default:  MAE .

Q: What is the default value for  outputs.energy.metrics  in fusion.yaml?
A: The default value is  MAE .

Q: How do I set the  loss_weight  in  fusion.yaml ?
A: In  fusion.yaml , the  loss_weight  parameter is located under  outputs.forces.loss_weight . Current default:  0.95 .

Q: What is the default value for  outputs.forces.loss_weight  in fusion.yaml?
A: The default value is  0.95 .

Q: How do I set the  metrics  in  fusion.yaml ?
A: In  fusion.yaml , the  metrics  parameter is located under  outputs.forces.metrics . Current default:  MAE .

Q: What is the default value for  outputs.forces.metrics  in fusion.yaml?
A: The default value is  MAE .

Q: How do I set the  accelerator  in  fusion.yaml ?
A: In  fusion.yaml , the  accelerator  parameter is located under  training.accelerator . Current default:  gpu .

Q: What is the default value for  training.accelerator  in fusion.yaml?
A: The default value is  gpu .

Q: How do I set the  devices  in  fusion.yaml ?
A: In  fusion.yaml , the  devices  parameter is located under  training.devices . Current default:  1 .

Q: What is the default value for  training.devices  in fusion.yaml?
A: The default value is  1 .

Q: How do I set the  precision  in  fusion.yaml ?
A: In  fusion.yaml , the  precision  parameter is located under  training.precision . Current default:  32 .

Q: What is the default value for  training.precision  in fusion.yaml?
A: The default value is  32 .

Q: How do I set the  batch_size  in  fusion.yaml ?
A: In  fusion.yaml , the  batch_size  parameter is located under  training.batch_size . Current default:  16 .

Q: What is the default value for  training.batch_size  in fusion.yaml?
A: The default value is  16 .

Q: How do I set the  num_train  in  fusion.yaml ?
A: In  fusion.yaml , the  num_train  parameter is located under  training.num_train . Current default:  800 .

Q: What is the default value for  training.num_train  in fusion.yaml?
A: The default value is  800 .

Q: How do I set the  num_val  in  fusion.yaml ?
A: In  fusion.yaml , the  num_val  parameter is located under  training.num_val . Current default:  200 .

Q: What is the default value for  training.num_val  in fusion.yaml?
A: The default value is  200 .

Q: How do I set the  max_epochs  in  fusion.yaml ?
A: In  fusion.yaml , the  max_epochs  parameter is located under  training.max_epochs . Current default:  3 .

Q: What is the default value for  training.max_epochs  in fusion.yaml?
A: The default value is  3 .

Q: How do I set the  num_workers  in  fusion.yaml ?
A: In  fusion.yaml , the  num_workers  parameter is located under  training.num_workers . Current default:  24 .

Q: What is the default value for  training.num_workers  in fusion.yaml?
A: The default value is  24 .

Q: How do I set the  pin_memory  in  fusion.yaml ?
A: In  fusion.yaml , the  pin_memory  parameter is located under  training.pin_memory . Current default:  True .

Q: What is the default value for  training.pin_memory  in fusion.yaml?
A: The default value is  True .

Q: How do I set the  type  in  fusion.yaml ?
A: In  fusion.yaml , the  type  parameter is located under  training.optimizer.type . Current default:  AdamW .

Q: What is the default value for  training.optimizer.type  in fusion.yaml?
A: The default value is  AdamW .

Q: How do I set the  lr  in  fusion.yaml ?
A: In  fusion.yaml , the  lr  parameter is located under  training.optimizer.lr . Current default:  0.0001 .

Q: What is the default value for  training.optimizer.lr  in fusion.yaml?
A: The default value is  0.0001 .

Q: How do I set the  type  in  fusion.yaml ?
A: In  fusion.yaml , the  type  parameter is located under  training.scheduler.type . Current default:  ReduceLROnPlateau .

Q: What is the default value for  training.scheduler.type  in fusion.yaml?
A: The default value is  ReduceLROnPlateau .

Q: How do I set the  factor  in  fusion.yaml ?
A: In  fusion.yaml , the  factor  parameter is located under  training.scheduler.factor . Current default:  0.8 .

Q: What is the default value for  training.scheduler.factor  in fusion.yaml?
A: The default value is  0.8 .

Q: How do I set the  patience  in  fusion.yaml ?
A: In  fusion.yaml , the  patience  parameter is located under  training.scheduler.patience . Current default:  30 .

Q: What is the default value for  training.scheduler.patience  in fusion.yaml?
A: The default value is  30 .

Q: How do I set the  verbose  in  fusion.yaml ?
A: In  fusion.yaml , the  verbose  parameter is located under  training.scheduler.verbose . Current default:  True .

Q: What is the default value for  training.scheduler.verbose  in fusion.yaml?
A: The default value is  True .

Q: How do I set the  folder  in  fusion.yaml ?
A: In  fusion.yaml , the  folder  parameter is located under  logging.folder . Current default:  ./results .

Q: What is the default value for  logging.folder  in fusion.yaml?
A: The default value is  ./results .

Q: How do I set the  log_dir  in  fusion.yaml ?
A: In  fusion.yaml , the  log_dir  parameter is located under  logging.log_dir . Current default:  lightning_logs .

Q: What is the default value for  logging.log_dir  in fusion.yaml?
A: The default value is  lightning_logs .

Q: How do I set the  checkpoint_dir  in  fusion.yaml ?
A: In  fusion.yaml , the  checkpoint_dir  parameter is located under  logging.checkpoint_dir . Current default:  best_inference_model .

Q: What is the default value for  logging.checkpoint_dir  in fusion.yaml?
A: The default value is  best_inference_model .

Q: How do I set the  monitor  in  fusion.yaml ?
A: In  fusion.yaml , the  monitor  parameter is located under  logging.monitor . Current default:  val_loss .

Q: What is the default value for  logging.monitor  in fusion.yaml?
A: The default value is  val_loss .

Q: How do I set the  trained_model_path  in  fusion.yaml ?
A: In  fusion.yaml , the  trained_model_path  parameter is located under  testing.trained_model_path . Current default:  ./results .

Q: What is the default value for  testing.trained_model_path  in fusion.yaml?
A: The default value is  ./results .

Q: How do I set the  csv_file_name  in  fusion.yaml ?
A: In  fusion.yaml , the  csv_file_name  parameter is located under  testing.csv_file_name . Current default:  actual_vs_predicted_enrgforc.csv .

Q: What is the default value for  testing.csv_file_name  in fusion.yaml?
A: The default value is  actual_vs_predicted_enrgforc.csv .

Q: How do I set the  resume_checkpoint_dir  in  fusion.yaml ?
A: In  fusion.yaml , the  resume_checkpoint_dir  parameter is located under  resume_training.resume_checkpoint_dir . Current default:  None .

Q: What is the default value for  resume_training.resume_checkpoint_dir  in fusion.yaml?
A: The default value is  None .

Q: How do I set the  pretrained_checkpoint  in  fusion.yaml ?
A: In  fusion.yaml , the  pretrained_checkpoint  parameter is located under  fine_tuning.pretrained_checkpoint . Current default:  None .

Q: What is the default value for  fine_tuning.pretrained_checkpoint  in fusion.yaml?
A: The default value is  None .

Q: How do I set the  freeze_embedding  in  fusion.yaml ?
A: In  fusion.yaml , the  freeze_embedding  parameter is located under  fine_tuning.freeze_embedding . Current default:  True .

Q: What is the default value for  fine_tuning.freeze_embedding  in fusion.yaml?
A: The default value is  True .

Q: How do I set the  freeze_interactions_up_to  in  fusion.yaml ?
A: In  fusion.yaml , the  freeze_interactions_up_to  parameter is located under  fine_tuning.freeze_interactions_up_to . Current default:  2 .

Q: What is the default value for  fine_tuning.freeze_interactions_up_to  in fusion.yaml?
A: The default value is  2 .

Q: How do I set the  freeze_all_representation  in  fusion.yaml ?
A: In  fusion.yaml , the  freeze_all_representation  parameter is located under  fine_tuning.freeze_all_representation . Current default:  True .

Q: What is the default value for  fine_tuning.freeze_all_representation  in fusion.yaml?
A: The default value is  True .

Q: How do I set the  lr  in  fusion.yaml ?
A: In  fusion.yaml , the  lr  parameter is located under  fine_tuning.lr . Current default:  5e-05 .

Q: What is the default value for  fine_tuning.lr  in fusion.yaml?
A: The default value is  5e-05 .

Q: How do I set the  early_stopping_patience  in  fusion.yaml ?
A: In  fusion.yaml , the  early_stopping_patience  parameter is located under  fine_tuning.early_stopping_patience . Current default:  10 .

Q: What is the default value for  fine_tuning.early_stopping_patience  in fusion.yaml?
A: The default value is  10 .

Q: How do I set the  best_model_dir  in  fusion.yaml ?
A: In  fusion.yaml , the  best_model_dir  parameter is located under  fine_tuning.best_model_dir . Current default:  fine_tuned_best_model .

Q: What is the default value for  fine_tuning.best_model_dir  in fusion.yaml?
A: The default value is  fine_tuned_best_model .

Q: How do I set the  checkpoint_dir  in  fusion.yaml ?
A: In  fusion.yaml , the  checkpoint_dir  parameter is located under  fine_tuning.checkpoint_dir . Current default:  fine_tuned_checkpoints .

Q: What is the default value for  fine_tuning.checkpoint_dir  in fusion.yaml?
A: The default value is  fine_tuned_checkpoints .

Q: How do I set the  log_name  in  fusion.yaml ?
A: In  fusion.yaml , the  log_name  parameter is located under  fine_tuning.log_name . Current default:  fine_tune_logs .

Q: What is the default value for  fine_tuning.log_name  in fusion.yaml?
A: The default value is  fine_tune_logs .

Q: Show me the full content of input.yaml.
A: Here is the content of  input.yaml :
   yaml
# ==============================================================================
# Notes for users:
#
# - Use **dot notation** for override keys (see below for example).
# - In case of conflict, keys from  common  always take priority over  overrides .
# - Keys not present in the engine template will be ignored (with a warning).
# - n_rbf: For schnet, painn, fusion → RBF basis functions. For nequip/allegro → mapped to bessel basis.
# ==============================================================================

platform: fusion  # [schnet, painn, fusion, nequip, allegro, mace]

common:
  data:
    input_xyz_file: ./basic_consolidated_dataset_1000CdSe.xyz   # Path to your XYZ file

  model:
    mp_layers: 3       # Number of message-passing layers in the neural network.
    features: 32       # Dimension of atomic feature vectors (per-atom embedding size).
    cutoff: 12.0
    n_rbf: 8           # For schnet, painn, fusion: number of RBF (radial basis functions)
                       # For nequip, allegro: this value will be mapped to number of bessel basis functions
    l_max: 1
    parity: true
    model_dtype: float32
    chemical_symbols: [Cd, Se, Cl]
    # pair_potential: Option to enable ZBL for NequIP/Allegro models only.
    #   - Set to "ZBL" (as a string) to ENABLE ZBL pair potential
    #   - Set to null to DISABLE the pair potential block (recommended for most cases)
    #   - Any other value will raise an error
    pair_potential: null   # Use "ZBL" (string), or null to disable

  training:
    seed: 42
    train_size: 0.8
    val_size: 0.1
    test_size: 0.1
    batch_size: 16      # Global batch size for training. 
                        # If using multiple GPUs, this value is split evenly across devices 
                        # (e.g., 16 total → 8 per GPU when devices=2). 
                        # For a single GPU, the full batch size is used.
    epochs: 3
    learning_rate: 0.001
    num_workers: 24
    accelerator: cuda
    devices: 2              # Number of GPUs to use for distributed or data-parallel training. Set 1 for single-GPU.
    log_every_n_steps: 5    # Frequency (in steps) for logging metrics and losses to the console or logger.
    optimizer: AdamW
    scheduler:
      type: ReduceLROnPlateau
      factor: 0.8
      patience: 5
    pin_memory: true      # If true, preloads data into page-locked memory for faster GPU transfer.
    
    early_stopping:
      enabled: true       # Enable or disable early stopping to avoid overfitting.
      patience: 30        # Stop training if validation loss doesn’t improve for this many epochs.
      min_delta: 0.003    # Minimum change in monitored metric to qualify as improvement.
      monitor: val_loss
      # monitor: val_loss         # for schnet
      # monitor: val0_epoch/weighted_sum   # for nequip
      # (If omitted, the code auto-inserts the correct default!)

  loss:
    energy_weight: 0.05
    forces_weight: 0.95

  output:
    output_dir: ./resultsNewNewX

# ------------------------------------------------------------------------------
# Overrides section:
# - Use dot notation for all keys (e.g., model.n_rbf, trainer.callbacks[0].patience)
# - Only specify keys you want to override for a specific engine.
# - If a key is in both  common  and  overrides ,  common  wins.
# ------------------------------------------------------------------------------

overrides:

  schnet:
    model.n_rbf: 30
    model.activation: relu
    model.n_layers: 1
    model.dropout: 0.2
    logging.folder: ./resultsExpert
    trainer:
      callbacks:
        - _target_: lightning.pytorch.callbacks.EarlyStopping
          patience: 100

  nequip:
    model.num_bessels: 50                     # dot notation for nested keys
    training_module.model.parity: false        # disables parity in the model
    model.n_layers: 5                         # ignored if mp_layers is set in common
    training_module.model.num_layers: 5        # ignored if mp_layers is set in common
    model.activation: relu                     # ignored if not in template
    model.dropout: 0.1                         # ignored if not in template
    logging.folder: ./resultsNequIP
    trainer.logger[0].save_dir: logsNequIPX
    trainer.callbacks[1].filename: bestNew
    trainer.callbacks[0].patience: 100

    # Example: Add new parameter not present in template to see warning in logs
    model.new_param: 12345
    training_module.loss.coeffs.total_energy: 0.02

  painn:
    model.n_atom_basis: 50
    training.num_val: 0.3
    outputs.forces.loss_weight: 0.91
    logging.folder: ./resultsPainn
    trainer.callbacks[1].monitor: val_loss
    trainer.logger[0].save_dir: ./logsPainnX
    fine_tuning.lr: 0.05

  fusion:
    model.n_interactions: 4
    training.num_train: 0.65
    outputs.energy.loss_weight: 0.09
    logging.folder: ./resultsFusion
    trainer.callbacks[0].min_delta: 0.01
    trainer.logger[0].save_dir: ./logsFusionX

  allegro:
    training_module.model.radial_chemical_embed.num_bessels: 17
    model.n_bessels: 50
    model.n_rbf: 30
    training_module.model.num_scalar_features: 48
    training_module.model.l_max: 2
    training_module.model.parity: false
    trainer.callbacks[0].patience: 10
    trainer.callbacks[2].logging_interval: epoch
    trainer.logger[0].save_dir: ./logsAllegroX

  mace:
    num_channels: 64
    model.n_rbf: 30
    max_L: 1
    lr: 0.007
    eval_interval: 10
    valid_file: ./converted_data/mace_val.xyz


   

Q: How do I set the  platform  in  input.yaml ?
A: In  input.yaml , the  platform  parameter is located under  platform . Current default:  fusion .

Q: What is the default value for  platform  in input.yaml?
A: The default value is  fusion .

Q: How do I set the  input_xyz_file  in  input.yaml ?
A: In  input.yaml , the  input_xyz_file  parameter is located under  common.data.input_xyz_file . Current default:  ./basic_consolidated_dataset_1000CdSe.xyz .

Q: What is the default value for  common.data.input_xyz_file  in input.yaml?
A: The default value is  ./basic_consolidated_dataset_1000CdSe.xyz .

Q: How do I set the  mp_layers  in  input.yaml ?
A: In  input.yaml , the  mp_layers  parameter is located under  common.model.mp_layers . Current default:  3 .

Q: What is the default value for  common.model.mp_layers  in input.yaml?
A: The default value is  3 .

Q: How do I set the  features  in  input.yaml ?
A: In  input.yaml , the  features  parameter is located under  common.model.features . Current default:  32 .

Q: What is the default value for  common.model.features  in input.yaml?
A: The default value is  32 .

Q: How do I set the  cutoff  in  input.yaml ?
A: In  input.yaml , the  cutoff  parameter is located under  common.model.cutoff . Current default:  12.0 .

Q: What is the default value for  common.model.cutoff  in input.yaml?
A: The default value is  12.0 .

Q: How do I set the  n_rbf  in  input.yaml ?
A: In  input.yaml , the  n_rbf  parameter is located under  common.model.n_rbf . Current default:  8 .

Q: What is the default value for  common.model.n_rbf  in input.yaml?
A: The default value is  8 .

Q: How do I set the  l_max  in  input.yaml ?
A: In  input.yaml , the  l_max  parameter is located under  common.model.l_max . Current default:  1 .

Q: What is the default value for  common.model.l_max  in input.yaml?
A: The default value is  1 .

Q: How do I set the  parity  in  input.yaml ?
A: In  input.yaml , the  parity  parameter is located under  common.model.parity . Current default:  True .

Q: What is the default value for  common.model.parity  in input.yaml?
A: The default value is  True .

Q: How do I set the  model_dtype  in  input.yaml ?
A: In  input.yaml , the  model_dtype  parameter is located under  common.model.model_dtype . Current default:  float32 .

Q: What is the default value for  common.model.model_dtype  in input.yaml?
A: The default value is  float32 .

Q: How do I set the  chemical_symbols  in  input.yaml ?
A: In  input.yaml , the  chemical_symbols  parameter is located under  common.model.chemical_symbols . Current default:  ['Cd', 'Se', 'Cl'] .

Q: What is the default value for  common.model.chemical_symbols  in input.yaml?
A: The default value is  ['Cd', 'Se', 'Cl'] .

Q: How do I set the  pair_potential  in  input.yaml ?
A: In  input.yaml , the  pair_potential  parameter is located under  common.model.pair_potential . Current default:  None .

Q: What is the default value for  common.model.pair_potential  in input.yaml?
A: The default value is  None .

Q: How do I set the  seed  in  input.yaml ?
A: In  input.yaml , the  seed  parameter is located under  common.training.seed . Current default:  42 .

Q: What is the default value for  common.training.seed  in input.yaml?
A: The default value is  42 .

Q: How do I set the  train_size  in  input.yaml ?
A: In  input.yaml , the  train_size  parameter is located under  common.training.train_size . Current default:  0.8 .

Q: What is the default value for  common.training.train_size  in input.yaml?
A: The default value is  0.8 .

Q: How do I set the  val_size  in  input.yaml ?
A: In  input.yaml , the  val_size  parameter is located under  common.training.val_size . Current default:  0.1 .

Q: What is the default value for  common.training.val_size  in input.yaml?
A: The default value is  0.1 .

Q: How do I set the  test_size  in  input.yaml ?
A: In  input.yaml , the  test_size  parameter is located under  common.training.test_size . Current default:  0.1 .

Q: What is the default value for  common.training.test_size  in input.yaml?
A: The default value is  0.1 .

Q: How do I set the  batch_size  in  input.yaml ?
A: In  input.yaml , the  batch_size  parameter is located under  common.training.batch_size . Current default:  16 .

Q: What is the default value for  common.training.batch_size  in input.yaml?
A: The default value is  16 .

Q: How do I set the  epochs  in  input.yaml ?
A: In  input.yaml , the  epochs  parameter is located under  common.training.epochs . Current default:  3 .

Q: What is the default value for  common.training.epochs  in input.yaml?
A: The default value is  3 .

Q: How do I set the  learning_rate  in  input.yaml ?
A: In  input.yaml , the  learning_rate  parameter is located under  common.training.learning_rate . Current default:  0.001 .

Q: What is the default value for  common.training.learning_rate  in input.yaml?
A: The default value is  0.001 .

Q: How do I set the  num_workers  in  input.yaml ?
A: In  input.yaml , the  num_workers  parameter is located under  common.training.num_workers . Current default:  24 .

Q: What is the default value for  common.training.num_workers  in input.yaml?
A: The default value is  24 .

Q: How do I set the  accelerator  in  input.yaml ?
A: In  input.yaml , the  accelerator  parameter is located under  common.training.accelerator . Current default:  cuda .

Q: What is the default value for  common.training.accelerator  in input.yaml?
A: The default value is  cuda .

Q: How do I set the  devices  in  input.yaml ?
A: In  input.yaml , the  devices  parameter is located under  common.training.devices . Current default:  2 .

Q: What is the default value for  common.training.devices  in input.yaml?
A: The default value is  2 .

Q: How do I set the  log_every_n_steps  in  input.yaml ?
A: In  input.yaml , the  log_every_n_steps  parameter is located under  common.training.log_every_n_steps . Current default:  5 .

Q: What is the default value for  common.training.log_every_n_steps  in input.yaml?
A: The default value is  5 .

Q: How do I set the  optimizer  in  input.yaml ?
A: In  input.yaml , the  optimizer  parameter is located under  common.training.optimizer . Current default:  AdamW .

Q: What is the default value for  common.training.optimizer  in input.yaml?
A: The default value is  AdamW .

Q: How do I set the  type  in  input.yaml ?
A: In  input.yaml , the  type  parameter is located under  common.training.scheduler.type . Current default:  ReduceLROnPlateau .

Q: What is the default value for  common.training.scheduler.type  in input.yaml?
A: The default value is  ReduceLROnPlateau .

Q: How do I set the  factor  in  input.yaml ?
A: In  input.yaml , the  factor  parameter is located under  common.training.scheduler.factor . Current default:  0.8 .

Q: What is the default value for  common.training.scheduler.factor  in input.yaml?
A: The default value is  0.8 .

Q: How do I set the  patience  in  input.yaml ?
A: In  input.yaml , the  patience  parameter is located under  common.training.scheduler.patience . Current default:  5 .

Q: What is the default value for  common.training.scheduler.patience  in input.yaml?
A: The default value is  5 .

Q: How do I set the  pin_memory  in  input.yaml ?
A: In  input.yaml , the  pin_memory  parameter is located under  common.training.pin_memory . Current default:  True .

Q: What is the default value for  common.training.pin_memory  in input.yaml?
A: The default value is  True .

Q: How do I set the  enabled  in  input.yaml ?
A: In  input.yaml , the  enabled  parameter is located under  common.training.early_stopping.enabled . Current default:  True .

Q: What is the default value for  common.training.early_stopping.enabled  in input.yaml?
A: The default value is  True .

Q: How do I set the  patience  in  input.yaml ?
A: In  input.yaml , the  patience  parameter is located under  common.training.early_stopping.patience . Current default:  30 .

Q: What is the default value for  common.training.early_stopping.patience  in input.yaml?
A: The default value is  30 .

Q: How do I set the  min_delta  in  input.yaml ?
A: In  input.yaml , the  min_delta  parameter is located under  common.training.early_stopping.min_delta . Current default:  0.003 .

Q: What is the default value for  common.training.early_stopping.min_delta  in input.yaml?
A: The default value is  0.003 .

Q: How do I set the  monitor  in  input.yaml ?
A: In  input.yaml , the  monitor  parameter is located under  common.training.early_stopping.monitor . Current default:  val_loss .

Q: What is the default value for  common.training.early_stopping.monitor  in input.yaml?
A: The default value is  val_loss .

Q: How do I set the  energy_weight  in  input.yaml ?
A: In  input.yaml , the  energy_weight  parameter is located under  common.loss.energy_weight . Current default:  0.05 .

Q: What is the default value for  common.loss.energy_weight  in input.yaml?
A: The default value is  0.05 .

Q: How do I set the  forces_weight  in  input.yaml ?
A: In  input.yaml , the  forces_weight  parameter is located under  common.loss.forces_weight . Current default:  0.95 .

Q: What is the default value for  common.loss.forces_weight  in input.yaml?
A: The default value is  0.95 .

Q: How do I set the  output_dir  in  input.yaml ?
A: In  input.yaml , the  output_dir  parameter is located under  common.output.output_dir . Current default:  ./resultsNewNewX .

Q: What is the default value for  common.output.output_dir  in input.yaml?
A: The default value is  ./resultsNewNewX .

Q: How do I set the  n_rbf  in  input.yaml ?
A: In  input.yaml , the  n_rbf  parameter is located under  overrides.schnet.model.n_rbf . Current default:  30 .

Q: What is the default value for  overrides.schnet.model.n_rbf  in input.yaml?
A: The default value is  30 .

Q: How do I set the  activation  in  input.yaml ?
A: In  input.yaml , the  activation  parameter is located under  overrides.schnet.model.activation . Current default:  relu .

Q: What is the default value for  overrides.schnet.model.activation  in input.yaml?
A: The default value is  relu .

Q: How do I set the  n_layers  in  input.yaml ?
A: In  input.yaml , the  n_layers  parameter is located under  overrides.schnet.model.n_layers . Current default:  1 .

Q: What is the default value for  overrides.schnet.model.n_layers  in input.yaml?
A: The default value is  1 .

Q: How do I set the  dropout  in  input.yaml ?
A: In  input.yaml , the  dropout  parameter is located under  overrides.schnet.model.dropout . Current default:  0.2 .

Q: What is the default value for  overrides.schnet.model.dropout  in input.yaml?
A: The default value is  0.2 .

Q: How do I set the  folder  in  input.yaml ?
A: In  input.yaml , the  folder  parameter is located under  overrides.schnet.logging.folder . Current default:  ./resultsExpert .

Q: What is the default value for  overrides.schnet.logging.folder  in input.yaml?
A: The default value is  ./resultsExpert .

Q: How do I set the  callbacks  in  input.yaml ?
A: In  input.yaml , the  callbacks  parameter is located under  overrides.schnet.trainer.callbacks . Current default:  [{'_target_': 'lightning.pytorch.callbacks.EarlyStopping', 'patience': 100}] .

Q: What is the default value for  overrides.schnet.trainer.callbacks  in input.yaml?
A: The default value is  [{'_target_': 'lightning.pytorch.callbacks.EarlyStopping', 'patience': 100}] .

Q: How do I set the  num_bessels  in  input.yaml ?
A: In  input.yaml , the  num_bessels  parameter is located under  overrides.nequip.model.num_bessels . Current default:  50 .

Q: What is the default value for  overrides.nequip.model.num_bessels  in input.yaml?
A: The default value is  50 .

Q: How do I set the  parity  in  input.yaml ?
A: In  input.yaml , the  parity  parameter is located under  overrides.nequip.training_module.model.parity . Current default:  False .

Q: What is the default value for  overrides.nequip.training_module.model.parity  in input.yaml?
A: The default value is  False .

Q: How do I set the  n_layers  in  input.yaml ?
A: In  input.yaml , the  n_layers  parameter is located under  overrides.nequip.model.n_layers . Current default:  5 .

Q: What is the default value for  overrides.nequip.model.n_layers  in input.yaml?
A: The default value is  5 .

Q: How do I set the  num_layers  in  input.yaml ?
A: In  input.yaml , the  num_layers  parameter is located under  overrides.nequip.training_module.model.num_layers . Current default:  5 .

Q: What is the default value for  overrides.nequip.training_module.model.num_layers  in input.yaml?
A: The default value is  5 .

Q: How do I set the  activation  in  input.yaml ?
A: In  input.yaml , the  activation  parameter is located under  overrides.nequip.model.activation . Current default:  relu .

Q: What is the default value for  overrides.nequip.model.activation  in input.yaml?
A: The default value is  relu .

Q: How do I set the  dropout  in  input.yaml ?
A: In  input.yaml , the  dropout  parameter is located under  overrides.nequip.model.dropout . Current default:  0.1 .

Q: What is the default value for  overrides.nequip.model.dropout  in input.yaml?
A: The default value is  0.1 .

Q: How do I set the  folder  in  input.yaml ?
A: In  input.yaml , the  folder  parameter is located under  overrides.nequip.logging.folder . Current default:  ./resultsNequIP .

Q: What is the default value for  overrides.nequip.logging.folder  in input.yaml?
A: The default value is  ./resultsNequIP .

Q: How do I set the  save_dir  in  input.yaml ?
A: In  input.yaml , the  save_dir  parameter is located under  overrides.nequip.trainer.logger[0].save_dir . Current default:  logsNequIPX .

Q: What is the default value for  overrides.nequip.trainer.logger[0].save_dir  in input.yaml?
A: The default value is  logsNequIPX .

Q: How do I set the  filename  in  input.yaml ?
A: In  input.yaml , the  filename  parameter is located under  overrides.nequip.trainer.callbacks[1].filename . Current default:  bestNew .

Q: What is the default value for  overrides.nequip.trainer.callbacks[1].filename  in input.yaml?
A: The default value is  bestNew .

Q: How do I set the  patience  in  input.yaml ?
A: In  input.yaml , the  patience  parameter is located under  overrides.nequip.trainer.callbacks[0].patience . Current default:  100 .

Q: What is the default value for  overrides.nequip.trainer.callbacks[0].patience  in input.yaml?
A: The default value is  100 .

Q: How do I set the  new_param  in  input.yaml ?
A: In  input.yaml , the  new_param  parameter is located under  overrides.nequip.model.new_param . Current default:  12345 .

Q: What is the default value for  overrides.nequip.model.new_param  in input.yaml?
A: The default value is  12345 .

Q: How do I set the  total_energy  in  input.yaml ?
A: In  input.yaml , the  total_energy  parameter is located under  overrides.nequip.training_module.loss.coeffs.total_energy . Current default:  0.02 .

Q: What is the default value for  overrides.nequip.training_module.loss.coeffs.total_energy  in input.yaml?
A: The default value is  0.02 .

Q: How do I set the  n_atom_basis  in  input.yaml ?
A: In  input.yaml , the  n_atom_basis  parameter is located under  overrides.painn.model.n_atom_basis . Current default:  50 .

Q: What is the default value for  overrides.painn.model.n_atom_basis  in input.yaml?
A: The default value is  50 .

Q: How do I set the  num_val  in  input.yaml ?
A: In  input.yaml , the  num_val  parameter is located under  overrides.painn.training.num_val . Current default:  0.3 .

Q: What is the default value for  overrides.painn.training.num_val  in input.yaml?
A: The default value is  0.3 .

Q: How do I set the  loss_weight  in  input.yaml ?
A: In  input.yaml , the  loss_weight  parameter is located under  overrides.painn.outputs.forces.loss_weight . Current default:  0.91 .

Q: What is the default value for  overrides.painn.outputs.forces.loss_weight  in input.yaml?
A: The default value is  0.91 .

Q: How do I set the  folder  in  input.yaml ?
A: In  input.yaml , the  folder  parameter is located under  overrides.painn.logging.folder . Current default:  ./resultsPainn .

Q: What is the default value for  overrides.painn.logging.folder  in input.yaml?
A: The default value is  ./resultsPainn .

Q: How do I set the  monitor  in  input.yaml ?
A: In  input.yaml , the  monitor  parameter is located under  overrides.painn.trainer.callbacks[1].monitor . Current default:  val_loss .

Q: What is the default value for  overrides.painn.trainer.callbacks[1].monitor  in input.yaml?
A: The default value is  val_loss .

Q: How do I set the  save_dir  in  input.yaml ?
A: In  input.yaml , the  save_dir  parameter is located under  overrides.painn.trainer.logger[0].save_dir . Current default:  ./logsPainnX .

Q: What is the default value for  overrides.painn.trainer.logger[0].save_dir  in input.yaml?
A: The default value is  ./logsPainnX .

Q: How do I set the  lr  in  input.yaml ?
A: In  input.yaml , the  lr  parameter is located under  overrides.painn.fine_tuning.lr . Current default:  0.05 .

Q: What is the default value for  overrides.painn.fine_tuning.lr  in input.yaml?
A: The default value is  0.05 .

Q: How do I set the  n_interactions  in  input.yaml ?
A: In  input.yaml , the  n_interactions  parameter is located under  overrides.fusion.model.n_interactions . Current default:  4 .

Q: What is the default value for  overrides.fusion.model.n_interactions  in input.yaml?
A: The default value is  4 .

Q: How do I set the  num_train  in  input.yaml ?
A: In  input.yaml , the  num_train  parameter is located under  overrides.fusion.training.num_train . Current default:  0.65 .

Q: What is the default value for  overrides.fusion.training.num_train  in input.yaml?
A: The default value is  0.65 .

Q: How do I set the  loss_weight  in  input.yaml ?
A: In  input.yaml , the  loss_weight  parameter is located under  overrides.fusion.outputs.energy.loss_weight . Current default:  0.09 .

Q: What is the default value for  overrides.fusion.outputs.energy.loss_weight  in input.yaml?
A: The default value is  0.09 .

Q: How do I set the  folder  in  input.yaml ?
A: In  input.yaml , the  folder  parameter is located under  overrides.fusion.logging.folder . Current default:  ./resultsFusion .

Q: What is the default value for  overrides.fusion.logging.folder  in input.yaml?
A: The default value is  ./resultsFusion .

Q: How do I set the  min_delta  in  input.yaml ?
A: In  input.yaml , the  min_delta  parameter is located under  overrides.fusion.trainer.callbacks[0].min_delta . Current default:  0.01 .

Q: What is the default value for  overrides.fusion.trainer.callbacks[0].min_delta  in input.yaml?
A: The default value is  0.01 .

Q: How do I set the  save_dir  in  input.yaml ?
A: In  input.yaml , the  save_dir  parameter is located under  overrides.fusion.trainer.logger[0].save_dir . Current default:  ./logsFusionX .

Q: What is the default value for  overrides.fusion.trainer.logger[0].save_dir  in input.yaml?
A: The default value is  ./logsFusionX .

Q: How do I set the  num_bessels  in  input.yaml ?
A: In  input.yaml , the  num_bessels  parameter is located under  overrides.allegro.training_module.model.radial_chemical_embed.num_bessels . Current default:  17 .

Q: What is the default value for  overrides.allegro.training_module.model.radial_chemical_embed.num_bessels  in input.yaml?
A: The default value is  17 .

Q: How do I set the  n_bessels  in  input.yaml ?
A: In  input.yaml , the  n_bessels  parameter is located under  overrides.allegro.model.n_bessels . Current default:  50 .

Q: What is the default value for  overrides.allegro.model.n_bessels  in input.yaml?
A: The default value is  50 .

Q: How do I set the  n_rbf  in  input.yaml ?
A: In  input.yaml , the  n_rbf  parameter is located under  overrides.allegro.model.n_rbf . Current default:  30 .

Q: What is the default value for  overrides.allegro.model.n_rbf  in input.yaml?
A: The default value is  30 .

Q: How do I set the  num_scalar_features  in  input.yaml ?
A: In  input.yaml , the  num_scalar_features  parameter is located under  overrides.allegro.training_module.model.num_scalar_features . Current default:  48 .

Q: What is the default value for  overrides.allegro.training_module.model.num_scalar_features  in input.yaml?
A: The default value is  48 .

Q: How do I set the  l_max  in  input.yaml ?
A: In  input.yaml , the  l_max  parameter is located under  overrides.allegro.training_module.model.l_max . Current default:  2 .

Q: What is the default value for  overrides.allegro.training_module.model.l_max  in input.yaml?
A: The default value is  2 .

Q: How do I set the  parity  in  input.yaml ?
A: In  input.yaml , the  parity  parameter is located under  overrides.allegro.training_module.model.parity . Current default:  False .

Q: What is the default value for  overrides.allegro.training_module.model.parity  in input.yaml?
A: The default value is  False .

Q: How do I set the  patience  in  input.yaml ?
A: In  input.yaml , the  patience  parameter is located under  overrides.allegro.trainer.callbacks[0].patience . Current default:  10 .

Q: What is the default value for  overrides.allegro.trainer.callbacks[0].patience  in input.yaml?
A: The default value is  10 .

Q: How do I set the  logging_interval  in  input.yaml ?
A: In  input.yaml , the  logging_interval  parameter is located under  overrides.allegro.trainer.callbacks[2].logging_interval . Current default:  epoch .

Q: What is the default value for  overrides.allegro.trainer.callbacks[2].logging_interval  in input.yaml?
A: The default value is  epoch .

Q: How do I set the  save_dir  in  input.yaml ?
A: In  input.yaml , the  save_dir  parameter is located under  overrides.allegro.trainer.logger[0].save_dir . Current default:  ./logsAllegroX .

Q: What is the default value for  overrides.allegro.trainer.logger[0].save_dir  in input.yaml?
A: The default value is  ./logsAllegroX .

Q: How do I set the  num_channels  in  input.yaml ?
A: In  input.yaml , the  num_channels  parameter is located under  overrides.mace.num_channels . Current default:  64 .

Q: What is the default value for  overrides.mace.num_channels  in input.yaml?
A: The default value is  64 .

Q: How do I set the  n_rbf  in  input.yaml ?
A: In  input.yaml , the  n_rbf  parameter is located under  overrides.mace.model.n_rbf . Current default:  30 .

Q: What is the default value for  overrides.mace.model.n_rbf  in input.yaml?
A: The default value is  30 .

Q: How do I set the  max_L  in  input.yaml ?
A: In  input.yaml , the  max_L  parameter is located under  overrides.mace.max_L . Current default:  1 .

Q: What is the default value for  overrides.mace.max_L  in input.yaml?
A: The default value is  1 .

Q: How do I set the  lr  in  input.yaml ?
A: In  input.yaml , the  lr  parameter is located under  overrides.mace.lr . Current default:  0.007 .

Q: What is the default value for  overrides.mace.lr  in input.yaml?
A: The default value is  0.007 .

Q: How do I set the  eval_interval  in  input.yaml ?
A: In  input.yaml , the  eval_interval  parameter is located under  overrides.mace.eval_interval . Current default:  10 .

Q: What is the default value for  overrides.mace.eval_interval  in input.yaml?
A: The default value is  10 .

Q: How do I set the  valid_file  in  input.yaml ?
A: In  input.yaml , the  valid_file  parameter is located under  overrides.mace.valid_file . Current default:  ./converted_data/mace_val.xyz .

Q: What is the default value for  overrides.mace.valid_file  in input.yaml?
A: The default value is  ./converted_data/mace_val.xyz .

Q: Show me the full content of mace.yaml.
A: Here is the content of  mace.yaml :
   yaml
# ===========================
# Experiment & Paths
# ===========================
name: mace_cdsecl_model
seed: 42
log_level: INFO
error_table: PerAtomMAE   # Report validation metrics using MAE

# ===========================
# Hardware & Precision
# ===========================
device: cuda              # Options: cpu, cuda, mps, xpu
default_dtype: float32

# ===========================
# Dataset & Keys
# ===========================
train_file:   #consolidate-cdse35_1000.xyz consolidated_dataset_1000_CdSe_new.xyz
valid_file: null
test_file: null

energy_key: energy  #REF_energy
forces_key: forces  #REF_forces
stress_key: null

valid_fraction: 0.2
batch_size: 8
num_workers: 24
pin_memory: true        # Enables faster CPU → GPU transfer

# ===========================
# Model Configuration
# ===========================
model: MACE
r_max: 12                  #cutoff
num_radial_basis: 20           #n-rbf
num_cutoff_basis: 6
max_ell: 3
num_channels: 64             #n_atom_basis
max_L: 2
num_interactions: 3
correlation: 3
avg_num_neighbors: 100.80

# ===========================
# Training Parameters
# ===========================
max_num_epochs: 3
ema: true
ema_decay: 0.99

# ===========================
# Validation & Early Stopping
# ===========================
valid_batch_size: 16         # Match GPU capacity
eval_interval: 1           # Check validation every 1 epochs
patience: 30                # Early stop if no val improvement in 30 checks

# ===========================
# Stochastic Weight Averaging
# ===========================
swa: true
start_swa: 400
swa_energy_weight: 1.0
swa_forces_weight: 100.0

# ===========================
# Loss Weights
# ===========================
forces_weight: 0.95
energy_weight: 0.05

# ===========================
# Optimizer & Scheduler
# ===========================
optimizer: adam
lr: 0.001
weight_decay: 1e-5

scheduler: ReduceLROnPlateau
lr_factor: 0.8
scheduler_patience: 5
lr_scheduler_gamma: 0.9993

# ===========================
# Energy Baseline & Scaling
# ===========================
E0s: "average"                         # Use average per-atom energy (like --E0s=average)
scaling: rms_forces_scaling
compute_avg_num_neighbors: true

   

Q: How do I set the  name  in  mace.yaml ?
A: In  mace.yaml , the  name  parameter is located under  name . Current default:  mace_cdsecl_model .

Q: What is the default value for  name  in mace.yaml?
A: The default value is  mace_cdsecl_model .

Q: How do I set the  seed  in  mace.yaml ?
A: In  mace.yaml , the  seed  parameter is located under  seed . Current default:  42 .

Q: What is the default value for  seed  in mace.yaml?
A: The default value is  42 .

Q: How do I set the  log_level  in  mace.yaml ?
A: In  mace.yaml , the  log_level  parameter is located under  log_level . Current default:  INFO .

Q: What is the default value for  log_level  in mace.yaml?
A: The default value is  INFO .

Q: How do I set the  error_table  in  mace.yaml ?
A: In  mace.yaml , the  error_table  parameter is located under  error_table . Current default:  PerAtomMAE .

Q: What is the default value for  error_table  in mace.yaml?
A: The default value is  PerAtomMAE .

Q: How do I set the  device  in  mace.yaml ?
A: In  mace.yaml , the  device  parameter is located under  device . Current default:  cuda .

Q: What is the default value for  device  in mace.yaml?
A: The default value is  cuda .

Q: How do I set the  default_dtype  in  mace.yaml ?
A: In  mace.yaml , the  default_dtype  parameter is located under  default_dtype . Current default:  float32 .

Q: What is the default value for  default_dtype  in mace.yaml?
A: The default value is  float32 .

Q: How do I set the  train_file  in  mace.yaml ?
A: In  mace.yaml , the  train_file  parameter is located under  train_file . Current default:  None .

Q: What is the default value for  train_file  in mace.yaml?
A: The default value is  None .

Q: How do I set the  valid_file  in  mace.yaml ?
A: In  mace.yaml , the  valid_file  parameter is located under  valid_file . Current default:  None .

Q: What is the default value for  valid_file  in mace.yaml?
A: The default value is  None .

Q: How do I set the  test_file  in  mace.yaml ?
A: In  mace.yaml , the  test_file  parameter is located under  test_file . Current default:  None .

Q: What is the default value for  test_file  in mace.yaml?
A: The default value is  None .

Q: How do I set the  energy_key  in  mace.yaml ?
A: In  mace.yaml , the  energy_key  parameter is located under  energy_key . Current default:  energy .

Q: What is the default value for  energy_key  in mace.yaml?
A: The default value is  energy .

Q: How do I set the  forces_key  in  mace.yaml ?
A: In  mace.yaml , the  forces_key  parameter is located under  forces_key . Current default:  forces .

Q: What is the default value for  forces_key  in mace.yaml?
A: The default value is  forces .

Q: How do I set the  stress_key  in  mace.yaml ?
A: In  mace.yaml , the  stress_key  parameter is located under  stress_key . Current default:  None .

Q: What is the default value for  stress_key  in mace.yaml?
A: The default value is  None .

Q: How do I set the  valid_fraction  in  mace.yaml ?
A: In  mace.yaml , the  valid_fraction  parameter is located under  valid_fraction . Current default:  0.2 .

Q: What is the default value for  valid_fraction  in mace.yaml?
A: The default value is  0.2 .

Q: How do I set the  batch_size  in  mace.yaml ?
A: In  mace.yaml , the  batch_size  parameter is located under  batch_size . Current default:  8 .

Q: What is the default value for  batch_size  in mace.yaml?
A: The default value is  8 .

Q: How do I set the  num_workers  in  mace.yaml ?
A: In  mace.yaml , the  num_workers  parameter is located under  num_workers . Current default:  24 .

Q: What is the default value for  num_workers  in mace.yaml?
A: The default value is  24 .

Q: How do I set the  pin_memory  in  mace.yaml ?
A: In  mace.yaml , the  pin_memory  parameter is located under  pin_memory . Current default:  True .

Q: What is the default value for  pin_memory  in mace.yaml?
A: The default value is  True .

Q: How do I set the  model  in  mace.yaml ?
A: In  mace.yaml , the  model  parameter is located under  model . Current default:  MACE .

Q: What is the default value for  model  in mace.yaml?
A: The default value is  MACE .

Q: How do I set the  r_max  in  mace.yaml ?
A: In  mace.yaml , the  r_max  parameter is located under  r_max . Current default:  12 .

Q: What is the default value for  r_max  in mace.yaml?
A: The default value is  12 .

Q: How do I set the  num_radial_basis  in  mace.yaml ?
A: In  mace.yaml , the  num_radial_basis  parameter is located under  num_radial_basis . Current default:  20 .

Q: What is the default value for  num_radial_basis  in mace.yaml?
A: The default value is  20 .

Q: How do I set the  num_cutoff_basis  in  mace.yaml ?
A: In  mace.yaml , the  num_cutoff_basis  parameter is located under  num_cutoff_basis . Current default:  6 .

Q: What is the default value for  num_cutoff_basis  in mace.yaml?
A: The default value is  6 .

Q: How do I set the  max_ell  in  mace.yaml ?
A: In  mace.yaml , the  max_ell  parameter is located under  max_ell . Current default:  3 .

Q: What is the default value for  max_ell  in mace.yaml?
A: The default value is  3 .

Q: How do I set the  num_channels  in  mace.yaml ?
A: In  mace.yaml , the  num_channels  parameter is located under  num_channels . Current default:  64 .

Q: What is the default value for  num_channels  in mace.yaml?
A: The default value is  64 .

Q: How do I set the  max_L  in  mace.yaml ?
A: In  mace.yaml , the  max_L  parameter is located under  max_L . Current default:  2 .

Q: What is the default value for  max_L  in mace.yaml?
A: The default value is  2 .

Q: How do I set the  num_interactions  in  mace.yaml ?
A: In  mace.yaml , the  num_interactions  parameter is located under  num_interactions . Current default:  3 .

Q: What is the default value for  num_interactions  in mace.yaml?
A: The default value is  3 .

Q: How do I set the  correlation  in  mace.yaml ?
A: In  mace.yaml , the  correlation  parameter is located under  correlation . Current default:  3 .

Q: What is the default value for  correlation  in mace.yaml?
A: The default value is  3 .

Q: How do I set the  avg_num_neighbors  in  mace.yaml ?
A: In  mace.yaml , the  avg_num_neighbors  parameter is located under  avg_num_neighbors . Current default:  100.8 .

Q: What is the default value for  avg_num_neighbors  in mace.yaml?
A: The default value is  100.8 .

Q: How do I set the  max_num_epochs  in  mace.yaml ?
A: In  mace.yaml , the  max_num_epochs  parameter is located under  max_num_epochs . Current default:  3 .

Q: What is the default value for  max_num_epochs  in mace.yaml?
A: The default value is  3 .

Q: How do I set the  ema  in  mace.yaml ?
A: In  mace.yaml , the  ema  parameter is located under  ema . Current default:  True .

Q: What is the default value for  ema  in mace.yaml?
A: The default value is  True .

Q: How do I set the  ema_decay  in  mace.yaml ?
A: In  mace.yaml , the  ema_decay  parameter is located under  ema_decay . Current default:  0.99 .

Q: What is the default value for  ema_decay  in mace.yaml?
A: The default value is  0.99 .

Q: How do I set the  valid_batch_size  in  mace.yaml ?
A: In  mace.yaml , the  valid_batch_size  parameter is located under  valid_batch_size . Current default:  16 .

Q: What is the default value for  valid_batch_size  in mace.yaml?
A: The default value is  16 .

Q: How do I set the  eval_interval  in  mace.yaml ?
A: In  mace.yaml , the  eval_interval  parameter is located under  eval_interval . Current default:  1 .

Q: What is the default value for  eval_interval  in mace.yaml?
A: The default value is  1 .

Q: How do I set the  patience  in  mace.yaml ?
A: In  mace.yaml , the  patience  parameter is located under  patience . Current default:  30 .

Q: What is the default value for  patience  in mace.yaml?
A: The default value is  30 .

Q: How do I set the  swa  in  mace.yaml ?
A: In  mace.yaml , the  swa  parameter is located under  swa . Current default:  True .

Q: What is the default value for  swa  in mace.yaml?
A: The default value is  True .

Q: How do I set the  start_swa  in  mace.yaml ?
A: In  mace.yaml , the  start_swa  parameter is located under  start_swa . Current default:  400 .

Q: What is the default value for  start_swa  in mace.yaml?
A: The default value is  400 .

Q: How do I set the  swa_energy_weight  in  mace.yaml ?
A: In  mace.yaml , the  swa_energy_weight  parameter is located under  swa_energy_weight . Current default:  1.0 .

Q: What is the default value for  swa_energy_weight  in mace.yaml?
A: The default value is  1.0 .

Q: How do I set the  swa_forces_weight  in  mace.yaml ?
A: In  mace.yaml , the  swa_forces_weight  parameter is located under  swa_forces_weight . Current default:  100.0 .

Q: What is the default value for  swa_forces_weight  in mace.yaml?
A: The default value is  100.0 .

Q: How do I set the  forces_weight  in  mace.yaml ?
A: In  mace.yaml , the  forces_weight  parameter is located under  forces_weight . Current default:  0.95 .

Q: What is the default value for  forces_weight  in mace.yaml?
A: The default value is  0.95 .

Q: How do I set the  energy_weight  in  mace.yaml ?
A: In  mace.yaml , the  energy_weight  parameter is located under  energy_weight . Current default:  0.05 .

Q: What is the default value for  energy_weight  in mace.yaml?
A: The default value is  0.05 .

Q: How do I set the  optimizer  in  mace.yaml ?
A: In  mace.yaml , the  optimizer  parameter is located under  optimizer . Current default:  adam .

Q: What is the default value for  optimizer  in mace.yaml?
A: The default value is  adam .

Q: How do I set the  lr  in  mace.yaml ?
A: In  mace.yaml , the  lr  parameter is located under  lr . Current default:  0.001 .

Q: What is the default value for  lr  in mace.yaml?
A: The default value is  0.001 .

Q: How do I set the  weight_decay  in  mace.yaml ?
A: In  mace.yaml , the  weight_decay  parameter is located under  weight_decay . Current default:  1e-5 .

Q: What is the default value for  weight_decay  in mace.yaml?
A: The default value is  1e-5 .

Q: How do I set the  scheduler  in  mace.yaml ?
A: In  mace.yaml , the  scheduler  parameter is located under  scheduler . Current default:  ReduceLROnPlateau .

Q: What is the default value for  scheduler  in mace.yaml?
A: The default value is  ReduceLROnPlateau .

Q: How do I set the  lr_factor  in  mace.yaml ?
A: In  mace.yaml , the  lr_factor  parameter is located under  lr_factor . Current default:  0.8 .

Q: What is the default value for  lr_factor  in mace.yaml?
A: The default value is  0.8 .

Q: How do I set the  scheduler_patience  in  mace.yaml ?
A: In  mace.yaml , the  scheduler_patience  parameter is located under  scheduler_patience . Current default:  5 .

Q: What is the default value for  scheduler_patience  in mace.yaml?
A: The default value is  5 .

Q: How do I set the  lr_scheduler_gamma  in  mace.yaml ?
A: In  mace.yaml , the  lr_scheduler_gamma  parameter is located under  lr_scheduler_gamma . Current default:  0.9993 .

Q: What is the default value for  lr_scheduler_gamma  in mace.yaml?
A: The default value is  0.9993 .

Q: How do I set the  E0s  in  mace.yaml ?
A: In  mace.yaml , the  E0s  parameter is located under  E0s . Current default:  average .

Q: What is the default value for  E0s  in mace.yaml?
A: The default value is  average .

Q: How do I set the  scaling  in  mace.yaml ?
A: In  mace.yaml , the  scaling  parameter is located under  scaling . Current default:  rms_forces_scaling .

Q: What is the default value for  scaling  in mace.yaml?
A: The default value is  rms_forces_scaling .

Q: How do I set the  compute_avg_num_neighbors  in  mace.yaml ?
A: In  mace.yaml , the  compute_avg_num_neighbors  parameter is located under  compute_avg_num_neighbors . Current default:  True .

Q: What is the default value for  compute_avg_num_neighbors  in mace.yaml?
A: The default value is  True .

Q: Show me the full content of nequip.yaml.
A: Here is the content of  nequip.yaml :
   yaml
# The config file is divided into 4 sections --  data ,  train ,  model , and  global_options 
# The config system relies on omegaconf (https://omegaconf.readthedocs.io/en/2.3_branch/index.html)
# and hydra (https://hydra.cc/docs/intro/) functionalities, such as
# - omegaconf's variable interpolation (https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#variable-interpolation)
# - omegaconf's resolvers (https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#resolvers)
# - hydra's instantiate (https://hydra.cc/docs/advanced/instantiate_objects/overview/)
# With hydra's instantiation (notice the  _target_ s everywhere), the config file (almost) directly corresponds to instantiating objects as one would normally do in Python.
# Much of the infrastructure is based on PyTorch Lightning (https://lightning.ai/docs/pytorch/stable/), such as the use of Lightning's Trainer, DataModule, LightningModule, Callback objects.

# ===========
#     RUN
# ===========
# the run types will be completed in sequence
# one can do  train ,  val ,  test  run types
run: [train, val, test]


# the following parameters (cutoff_radius, chemical_symbols, model_type_names) are not used direcly by the code
# parameters that take thier values show up multiple times in the config, so this allows us to use
# variable interpolation to keep their multiple instances consistent

# data and model r_max can be different (model's r_max should be smaller), but we try to make them the same
cutoff_radius: 12.0

# There are two sets of atomic types to keep track of in most applications
# -- there is the conventional atomic species (e.g. C, H), and a separate  type_names  known to the model.
# The model only knows types based on a set of zero-based indices and user-given  type_names  argument.
# An example where this distinction is necessary include datasets with the same atomic species with different charge states:
# we could define  chemical_symbols: [C, C]  and model  type_names: [C3, C4]  for +3 and +4 charge states.
# There could also be instances such as coarse graining we only care about the model's  type_names  (no need to define chemical species).
# Because of this distinction, these variables show up as arguments across different categories, including, data, model, metrics and even callbacks.
# In this case, we fix both to be the same, so we define a single set of each here and use variable interpolation to retrieve them below.
# This ensures a single location where the values are set to reduce the chances of mis-configuring runs.
chemical_symbols: [Cd, Cl, Se] 
model_type_names: ${chemical_symbols}


# ============
#     DATA
# ============
#  data  is managed by  LightningDataModule s
# NequIP provides some standard datamodules that can be found in  nequip.data.datamodule 
# Users are free to define and use their own datamodules that subclass nequip.data.datamodule.NequIPDataModule
data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 456             # dataset seed for reproducibility
  
  # here we take an ASE-readable file (in extxyz format) and split it into train:val:test = 80:10:10
  split_dataset: 
    file_path:  #./basic_consolidated_dataset_1000CdSe.xyz
    train: 0.8
    val: 0.1
    test: 0.1

  #  transforms  convert data from the Dataset to a form that can be used by the ML model
  # the transforms are only performed right before data is given to the model
  # data is kept in its untransformed form
  
  transforms:
    # data doesn't usually come with a neighborlist -- this tranforms prepares the neighborlist
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}
    # the models only know atom types, which can be different from the chemical species (e.g. C, H)
    # for instance we can have data with different charge states of carbon, which means they are
    # all labeled by chemical species  C , but may have different atom type labels based on the charge states
    # in this case, the atom types are the same as the chemical species, but we still have to include this
    # transformation to ensure that the data has 0-indexed atom type lists used in the various model operations 
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}

  # the following are torch.utils.data.DataLoader configs excluding the arguments  dataset  and  collate_fn 
  # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader
  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
    num_workers: 5
    shuffle: true
  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
    num_workers: ${data.train_dataloader.num_workers}  # we want to use the same num_workers -- variable interpolation helps
  test_dataloader: ${data.val_dataloader}  # variable interpolation comes in handy again

  # dataset statistics can be calculated to be used for model initialization such as for shifting, scaling and standardizing.
  # it is advised to provide custom names -- you will have to retrieve them later under model to initialize certain parameters to the dataset statistics computed
  stats_manager:
    # dataset statistics is handled by the  DataStatisticsManager 
    # here, we use  CommonDataStatisticsManager  for a basic set of dataset statistics for general use cases
    # the dataset statistics include  num_neighbors_mean ,  per_atom_energy_mean ,  forces_rms ,  per_type_forces_rms 
    _target_: nequip.data.CommonDataStatisticsManager
    # dataloader kwargs for data statistics computation
    #  batch_size  should ideally be as large as possible without trigerring OOM
    dataloader_kwargs:
      batch_size: 16
    # we need to provide the same type names that correspond to the model's  type_names 
    # so we interpolate the "central source of truth" model type names from above
    type_names: ${model_type_names}

#  trainer  (mandatory) is a Lightning.Trainer object (https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-class-api)
trainer:
  _target_: lightning.Trainer
  accelerator: auto
  devices: 1
  enable_checkpointing: true
  max_epochs: 3
  max_time: 03:00:00:00
  check_val_every_n_epoch: 1  # how often to validate
  log_every_n_steps: 1       # how often to log

  # use any Lightning supported logger
  logger:
    - _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ./logs
      name: tutorial_log
      version: null
      default_hp_metric: false

  # use any Lightning callbacks https://lightning.ai/docs/pytorch/stable/api_references.html#callbacks
  # and any custom callbakcs that subclass Lightning's Callback parent class
  callbacks:
    # Common callbacks used in ML

    # stop training when some criterion is met
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      min_delta: 1e-3                         # how much to be considered a "change"
      patience: 20                            # how many instances of "no change" before stopping

    # checkpoint based on some criterion
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      dirpath: ./results    
      filename: best                          # best.ckpt is the checkpoint name
      save_last: true                         # last.ckpt will be saved
      
    # log learning rate, e.g. to monitor what the learning rate scheduler is doing
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: epoch

# training_module refers to a NequIPLightningModule
training_module:
  _target_: nequip.train.EMALightningModule

  # We are using an EMA model (i.e. we keep a separate model whose weights are an exponential moving average of the base model's weights)
  # The use of an EMA model is configured by setting  ema_decay  to be a float (e.g. 0.999) under  training_module  (it is a  NequIPLightningModule  argument). The default of  ema_decay  is None, which means an EMA model is not used, if  ema_decay  is not explicitly configured
  # EMA allows for smoother validation curves and thus more reliable metrics for monitoring
  # Loading from a checkpoint for use in the  nequip.ase.NequIPCalculator  or during  nequip-compile  and  nequip-package  will always load the EMA model if it's present
  ema_decay: 0.999

  # here, we use a simplified MetricsManager wrapper (see docs) to construct the energy-force loss function
  # the more general  nequip.train.MetricsManager  could also be used to configure a custom loss function
  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 0.05
      forces: 0.95

  # again, we use a simplified MetricsManager wrapper (see docs) to construct the energy-force metrics
  # the more general  nequip.train.MetricsManager  could also be used in this case
  # validation metrics are used for monitoring and influencing training, e.g. with LR schedulers or early stopping, etc
  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      total_energy_mae: 1.0
      forces_mae: 1.0
      # keys  total_energy_rmse  and  forces_rmse ,  per_atom_energy_rmse  and  per_atom_energy_mae  are also available

  # we could have train_metrics and test_metrics be different from val_metrics, but it makes sense to have them be the same
  train_metrics: ${training_module.val_metrics}  # use variable interpolation
  test_metrics: ${training_module.val_metrics}  # use variable interpolation

  # any torch compatible optimizer: https://pytorch.org/docs/stable/optim.html#algorithms
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.03

  # see options for lr_scheduler_config
  # https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule.configure_optimizers
  lr_scheduler:
    # any torch compatible lr sceduler
    scheduler:
      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
      factor: 0.6
      patience: 5
      threshold: 0.2
      min_lr: 1e-6
    monitor: val0_epoch/weighted_sum
    interval: epoch
    frequency: 1

  # model details
  model:
    _target_: nequip.model.NequIPGNNModel

    # == basic model params ==
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    # == bessel encoding ==
    num_bessels: 8                # number of basis functions used in the radial Bessel basis, the default of 8 usually works well
    bessel_trainable: false       # set true to train the bessel weights (default false)
    polynomial_cutoff_p: 6        # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance

    # == convnet layers ==
    num_layers: 3       # number of interaction blocks, we find 3-5 to work best
    l_max: 1            # the maximum irrep order (rotation order) for the network's features, l=1 is a good default, l=2 is more accurate but slower
    parity: true        # whether to include features with odd mirror parity; often turning parity off gives equally good results but faster networks, so do consider this
    num_features: 32    # the multiplicity of the features, 32 is a good default for accurate network, if you want to be more accurate, go larger, if you want to be faster, go lower

    # == radial network ==
    radial_mlp_depth: 2         # number of radial layers, usually 1-3 works best, smaller is faster
    radial_mlp_width: 64        # number of hidden neurons in radial function, smaller is faster

    # dataset statistics used to inform the model's initial parameters for normalization, shifting and rescaling
    # we use omegaconf's resolvers (https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#resolvers)
    # to facilitate getting the dataset statistics from the DataStatisticsManager
    
    # average number of neighbors for edge sum normalization
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}
    
    # == per-type per-atom scales and shifts ==
    per_type_energy_scales: ${training_data_stats:per_type_forces_rms}
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    # == ZBL pair potential ==
    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: metal     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types

# global options
global_options:
  allow_tf32: false
   

Q: How do I set the  run  in  nequip.yaml ?
A: In  nequip.yaml , the  run  parameter is located under  run . Current default:  ['train', 'val', 'test'] .

Q: What is the default value for  run  in nequip.yaml?
A: The default value is  ['train', 'val', 'test'] .

Q: How do I set the  cutoff_radius  in  nequip.yaml ?
A: In  nequip.yaml , the  cutoff_radius  parameter is located under  cutoff_radius . Current default:  12.0 .

Q: What is the default value for  cutoff_radius  in nequip.yaml?
A: The default value is  12.0 .

Q: How do I set the  chemical_symbols  in  nequip.yaml ?
A: In  nequip.yaml , the  chemical_symbols  parameter is located under  chemical_symbols . Current default:  ['Cd', 'Cl', 'Se'] .

Q: What is the default value for  chemical_symbols  in nequip.yaml?
A: The default value is  ['Cd', 'Cl', 'Se'] .

Q: How do I set the  model_type_names  in  nequip.yaml ?
A: In  nequip.yaml , the  model_type_names  parameter is located under  model_type_names . Current default:  ${chemical_symbols} .

Q: What is the default value for  model_type_names  in nequip.yaml?
A: The default value is  ${chemical_symbols} .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  data._target_ . Current default:  nequip.data.datamodule.ASEDataModule .

Q: What is the default value for  data._target_  in nequip.yaml?
A: The default value is  nequip.data.datamodule.ASEDataModule .

Q: How do I set the  seed  in  nequip.yaml ?
A: In  nequip.yaml , the  seed  parameter is located under  data.seed . Current default:  456 .

Q: What is the default value for  data.seed  in nequip.yaml?
A: The default value is  456 .

Q: How do I set the  file_path  in  nequip.yaml ?
A: In  nequip.yaml , the  file_path  parameter is located under  data.split_dataset.file_path . Current default:  None .

Q: What is the default value for  data.split_dataset.file_path  in nequip.yaml?
A: The default value is  None .

Q: How do I set the  train  in  nequip.yaml ?
A: In  nequip.yaml , the  train  parameter is located under  data.split_dataset.train . Current default:  0.8 .

Q: What is the default value for  data.split_dataset.train  in nequip.yaml?
A: The default value is  0.8 .

Q: How do I set the  val  in  nequip.yaml ?
A: In  nequip.yaml , the  val  parameter is located under  data.split_dataset.val . Current default:  0.1 .

Q: What is the default value for  data.split_dataset.val  in nequip.yaml?
A: The default value is  0.1 .

Q: How do I set the  test  in  nequip.yaml ?
A: In  nequip.yaml , the  test  parameter is located under  data.split_dataset.test . Current default:  0.1 .

Q: What is the default value for  data.split_dataset.test  in nequip.yaml?
A: The default value is  0.1 .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  data.train_dataloader._target_ . Current default:  torch.utils.data.DataLoader .

Q: What is the default value for  data.train_dataloader._target_  in nequip.yaml?
A: The default value is  torch.utils.data.DataLoader .

Q: How do I set the  batch_size  in  nequip.yaml ?
A: In  nequip.yaml , the  batch_size  parameter is located under  data.train_dataloader.batch_size . Current default:  16 .

Q: What is the default value for  data.train_dataloader.batch_size  in nequip.yaml?
A: The default value is  16 .

Q: How do I set the  num_workers  in  nequip.yaml ?
A: In  nequip.yaml , the  num_workers  parameter is located under  data.train_dataloader.num_workers . Current default:  5 .

Q: What is the default value for  data.train_dataloader.num_workers  in nequip.yaml?
A: The default value is  5 .

Q: How do I set the  shuffle  in  nequip.yaml ?
A: In  nequip.yaml , the  shuffle  parameter is located under  data.train_dataloader.shuffle . Current default:  True .

Q: What is the default value for  data.train_dataloader.shuffle  in nequip.yaml?
A: The default value is  True .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  data.val_dataloader._target_ . Current default:  torch.utils.data.DataLoader .

Q: What is the default value for  data.val_dataloader._target_  in nequip.yaml?
A: The default value is  torch.utils.data.DataLoader .

Q: How do I set the  batch_size  in  nequip.yaml ?
A: In  nequip.yaml , the  batch_size  parameter is located under  data.val_dataloader.batch_size . Current default:  16 .

Q: What is the default value for  data.val_dataloader.batch_size  in nequip.yaml?
A: The default value is  16 .

Q: How do I set the  num_workers  in  nequip.yaml ?
A: In  nequip.yaml , the  num_workers  parameter is located under  data.val_dataloader.num_workers . Current default:  ${data.train_dataloader.num_workers} .

Q: What is the default value for  data.val_dataloader.num_workers  in nequip.yaml?
A: The default value is  ${data.train_dataloader.num_workers} .

Q: How do I set the  test_dataloader  in  nequip.yaml ?
A: In  nequip.yaml , the  test_dataloader  parameter is located under  data.test_dataloader . Current default:  ${data.val_dataloader} .

Q: What is the default value for  data.test_dataloader  in nequip.yaml?
A: The default value is  ${data.val_dataloader} .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  data.stats_manager._target_ . Current default:  nequip.data.CommonDataStatisticsManager .

Q: What is the default value for  data.stats_manager._target_  in nequip.yaml?
A: The default value is  nequip.data.CommonDataStatisticsManager .

Q: How do I set the  batch_size  in  nequip.yaml ?
A: In  nequip.yaml , the  batch_size  parameter is located under  data.stats_manager.dataloader_kwargs.batch_size . Current default:  16 .

Q: What is the default value for  data.stats_manager.dataloader_kwargs.batch_size  in nequip.yaml?
A: The default value is  16 .

Q: How do I set the  type_names  in  nequip.yaml ?
A: In  nequip.yaml , the  type_names  parameter is located under  data.stats_manager.type_names . Current default:  ${model_type_names} .

Q: What is the default value for  data.stats_manager.type_names  in nequip.yaml?
A: The default value is  ${model_type_names} .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  trainer._target_ . Current default:  lightning.Trainer .

Q: What is the default value for  trainer._target_  in nequip.yaml?
A: The default value is  lightning.Trainer .

Q: How do I set the  accelerator  in  nequip.yaml ?
A: In  nequip.yaml , the  accelerator  parameter is located under  trainer.accelerator . Current default:  auto .

Q: What is the default value for  trainer.accelerator  in nequip.yaml?
A: The default value is  auto .

Q: How do I set the  devices  in  nequip.yaml ?
A: In  nequip.yaml , the  devices  parameter is located under  trainer.devices . Current default:  1 .

Q: What is the default value for  trainer.devices  in nequip.yaml?
A: The default value is  1 .

Q: How do I set the  enable_checkpointing  in  nequip.yaml ?
A: In  nequip.yaml , the  enable_checkpointing  parameter is located under  trainer.enable_checkpointing . Current default:  True .

Q: What is the default value for  trainer.enable_checkpointing  in nequip.yaml?
A: The default value is  True .

Q: How do I set the  max_epochs  in  nequip.yaml ?
A: In  nequip.yaml , the  max_epochs  parameter is located under  trainer.max_epochs . Current default:  3 .

Q: What is the default value for  trainer.max_epochs  in nequip.yaml?
A: The default value is  3 .

Q: How do I set the  max_time  in  nequip.yaml ?
A: In  nequip.yaml , the  max_time  parameter is located under  trainer.max_time . Current default:  03:00:00:00 .

Q: What is the default value for  trainer.max_time  in nequip.yaml?
A: The default value is  03:00:00:00 .

Q: How do I set the  check_val_every_n_epoch  in  nequip.yaml ?
A: In  nequip.yaml , the  check_val_every_n_epoch  parameter is located under  trainer.check_val_every_n_epoch . Current default:  1 .

Q: What is the default value for  trainer.check_val_every_n_epoch  in nequip.yaml?
A: The default value is  1 .

Q: How do I set the  log_every_n_steps  in  nequip.yaml ?
A: In  nequip.yaml , the  log_every_n_steps  parameter is located under  trainer.log_every_n_steps . Current default:  1 .

Q: What is the default value for  trainer.log_every_n_steps  in nequip.yaml?
A: The default value is  1 .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module._target_ . Current default:  nequip.train.EMALightningModule .

Q: What is the default value for  training_module._target_  in nequip.yaml?
A: The default value is  nequip.train.EMALightningModule .

Q: How do I set the  ema_decay  in  nequip.yaml ?
A: In  nequip.yaml , the  ema_decay  parameter is located under  training_module.ema_decay . Current default:  0.999 .

Q: What is the default value for  training_module.ema_decay  in nequip.yaml?
A: The default value is  0.999 .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module.loss._target_ . Current default:  nequip.train.EnergyForceLoss .

Q: What is the default value for  training_module.loss._target_  in nequip.yaml?
A: The default value is  nequip.train.EnergyForceLoss .

Q: How do I set the  per_atom_energy  in  nequip.yaml ?
A: In  nequip.yaml , the  per_atom_energy  parameter is located under  training_module.loss.per_atom_energy . Current default:  True .

Q: What is the default value for  training_module.loss.per_atom_energy  in nequip.yaml?
A: The default value is  True .

Q: How do I set the  total_energy  in  nequip.yaml ?
A: In  nequip.yaml , the  total_energy  parameter is located under  training_module.loss.coeffs.total_energy . Current default:  0.05 .

Q: What is the default value for  training_module.loss.coeffs.total_energy  in nequip.yaml?
A: The default value is  0.05 .

Q: How do I set the  forces  in  nequip.yaml ?
A: In  nequip.yaml , the  forces  parameter is located under  training_module.loss.coeffs.forces . Current default:  0.95 .

Q: What is the default value for  training_module.loss.coeffs.forces  in nequip.yaml?
A: The default value is  0.95 .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module.val_metrics._target_ . Current default:  nequip.train.EnergyForceMetrics .

Q: What is the default value for  training_module.val_metrics._target_  in nequip.yaml?
A: The default value is  nequip.train.EnergyForceMetrics .

Q: How do I set the  total_energy_mae  in  nequip.yaml ?
A: In  nequip.yaml , the  total_energy_mae  parameter is located under  training_module.val_metrics.coeffs.total_energy_mae . Current default:  1.0 .

Q: What is the default value for  training_module.val_metrics.coeffs.total_energy_mae  in nequip.yaml?
A: The default value is  1.0 .

Q: How do I set the  forces_mae  in  nequip.yaml ?
A: In  nequip.yaml , the  forces_mae  parameter is located under  training_module.val_metrics.coeffs.forces_mae . Current default:  1.0 .

Q: What is the default value for  training_module.val_metrics.coeffs.forces_mae  in nequip.yaml?
A: The default value is  1.0 .

Q: How do I set the  train_metrics  in  nequip.yaml ?
A: In  nequip.yaml , the  train_metrics  parameter is located under  training_module.train_metrics . Current default:  ${training_module.val_metrics} .

Q: What is the default value for  training_module.train_metrics  in nequip.yaml?
A: The default value is  ${training_module.val_metrics} .

Q: How do I set the  test_metrics  in  nequip.yaml ?
A: In  nequip.yaml , the  test_metrics  parameter is located under  training_module.test_metrics . Current default:  ${training_module.val_metrics} .

Q: What is the default value for  training_module.test_metrics  in nequip.yaml?
A: The default value is  ${training_module.val_metrics} .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module.optimizer._target_ . Current default:  torch.optim.Adam .

Q: What is the default value for  training_module.optimizer._target_  in nequip.yaml?
A: The default value is  torch.optim.Adam .

Q: How do I set the  lr  in  nequip.yaml ?
A: In  nequip.yaml , the  lr  parameter is located under  training_module.optimizer.lr . Current default:  0.03 .

Q: What is the default value for  training_module.optimizer.lr  in nequip.yaml?
A: The default value is  0.03 .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module.lr_scheduler.scheduler._target_ . Current default:  torch.optim.lr_scheduler.ReduceLROnPlateau .

Q: What is the default value for  training_module.lr_scheduler.scheduler._target_  in nequip.yaml?
A: The default value is  torch.optim.lr_scheduler.ReduceLROnPlateau .

Q: How do I set the  factor  in  nequip.yaml ?
A: In  nequip.yaml , the  factor  parameter is located under  training_module.lr_scheduler.scheduler.factor . Current default:  0.6 .

Q: What is the default value for  training_module.lr_scheduler.scheduler.factor  in nequip.yaml?
A: The default value is  0.6 .

Q: How do I set the  patience  in  nequip.yaml ?
A: In  nequip.yaml , the  patience  parameter is located under  training_module.lr_scheduler.scheduler.patience . Current default:  5 .

Q: What is the default value for  training_module.lr_scheduler.scheduler.patience  in nequip.yaml?
A: The default value is  5 .

Q: How do I set the  threshold  in  nequip.yaml ?
A: In  nequip.yaml , the  threshold  parameter is located under  training_module.lr_scheduler.scheduler.threshold . Current default:  0.2 .

Q: What is the default value for  training_module.lr_scheduler.scheduler.threshold  in nequip.yaml?
A: The default value is  0.2 .

Q: How do I set the  min_lr  in  nequip.yaml ?
A: In  nequip.yaml , the  min_lr  parameter is located under  training_module.lr_scheduler.scheduler.min_lr . Current default:  1e-6 .

Q: What is the default value for  training_module.lr_scheduler.scheduler.min_lr  in nequip.yaml?
A: The default value is  1e-6 .

Q: How do I set the  monitor  in  nequip.yaml ?
A: In  nequip.yaml , the  monitor  parameter is located under  training_module.lr_scheduler.monitor . Current default:  val0_epoch/weighted_sum .

Q: What is the default value for  training_module.lr_scheduler.monitor  in nequip.yaml?
A: The default value is  val0_epoch/weighted_sum .

Q: How do I set the  interval  in  nequip.yaml ?
A: In  nequip.yaml , the  interval  parameter is located under  training_module.lr_scheduler.interval . Current default:  epoch .

Q: What is the default value for  training_module.lr_scheduler.interval  in nequip.yaml?
A: The default value is  epoch .

Q: How do I set the  frequency  in  nequip.yaml ?
A: In  nequip.yaml , the  frequency  parameter is located under  training_module.lr_scheduler.frequency . Current default:  1 .

Q: What is the default value for  training_module.lr_scheduler.frequency  in nequip.yaml?
A: The default value is  1 .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module.model._target_ . Current default:  nequip.model.NequIPGNNModel .

Q: What is the default value for  training_module.model._target_  in nequip.yaml?
A: The default value is  nequip.model.NequIPGNNModel .

Q: How do I set the  seed  in  nequip.yaml ?
A: In  nequip.yaml , the  seed  parameter is located under  training_module.model.seed . Current default:  456 .

Q: What is the default value for  training_module.model.seed  in nequip.yaml?
A: The default value is  456 .

Q: How do I set the  model_dtype  in  nequip.yaml ?
A: In  nequip.yaml , the  model_dtype  parameter is located under  training_module.model.model_dtype . Current default:  float32 .

Q: What is the default value for  training_module.model.model_dtype  in nequip.yaml?
A: The default value is  float32 .

Q: How do I set the  type_names  in  nequip.yaml ?
A: In  nequip.yaml , the  type_names  parameter is located under  training_module.model.type_names . Current default:  ${model_type_names} .

Q: What is the default value for  training_module.model.type_names  in nequip.yaml?
A: The default value is  ${model_type_names} .

Q: How do I set the  r_max  in  nequip.yaml ?
A: In  nequip.yaml , the  r_max  parameter is located under  training_module.model.r_max . Current default:  ${cutoff_radius} .

Q: What is the default value for  training_module.model.r_max  in nequip.yaml?
A: The default value is  ${cutoff_radius} .

Q: How do I set the  num_bessels  in  nequip.yaml ?
A: In  nequip.yaml , the  num_bessels  parameter is located under  training_module.model.num_bessels . Current default:  8 .

Q: What is the default value for  training_module.model.num_bessels  in nequip.yaml?
A: The default value is  8 .

Q: How do I set the  bessel_trainable  in  nequip.yaml ?
A: In  nequip.yaml , the  bessel_trainable  parameter is located under  training_module.model.bessel_trainable . Current default:  False .

Q: What is the default value for  training_module.model.bessel_trainable  in nequip.yaml?
A: The default value is  False .

Q: How do I set the  polynomial_cutoff_p  in  nequip.yaml ?
A: In  nequip.yaml , the  polynomial_cutoff_p  parameter is located under  training_module.model.polynomial_cutoff_p . Current default:  6 .

Q: What is the default value for  training_module.model.polynomial_cutoff_p  in nequip.yaml?
A: The default value is  6 .

Q: How do I set the  num_layers  in  nequip.yaml ?
A: In  nequip.yaml , the  num_layers  parameter is located under  training_module.model.num_layers . Current default:  3 .

Q: What is the default value for  training_module.model.num_layers  in nequip.yaml?
A: The default value is  3 .

Q: How do I set the  l_max  in  nequip.yaml ?
A: In  nequip.yaml , the  l_max  parameter is located under  training_module.model.l_max . Current default:  1 .

Q: What is the default value for  training_module.model.l_max  in nequip.yaml?
A: The default value is  1 .

Q: How do I set the  parity  in  nequip.yaml ?
A: In  nequip.yaml , the  parity  parameter is located under  training_module.model.parity . Current default:  True .

Q: What is the default value for  training_module.model.parity  in nequip.yaml?
A: The default value is  True .

Q: How do I set the  num_features  in  nequip.yaml ?
A: In  nequip.yaml , the  num_features  parameter is located under  training_module.model.num_features . Current default:  32 .

Q: What is the default value for  training_module.model.num_features  in nequip.yaml?
A: The default value is  32 .

Q: How do I set the  radial_mlp_depth  in  nequip.yaml ?
A: In  nequip.yaml , the  radial_mlp_depth  parameter is located under  training_module.model.radial_mlp_depth . Current default:  2 .

Q: What is the default value for  training_module.model.radial_mlp_depth  in nequip.yaml?
A: The default value is  2 .

Q: How do I set the  radial_mlp_width  in  nequip.yaml ?
A: In  nequip.yaml , the  radial_mlp_width  parameter is located under  training_module.model.radial_mlp_width . Current default:  64 .

Q: What is the default value for  training_module.model.radial_mlp_width  in nequip.yaml?
A: The default value is  64 .

Q: How do I set the  avg_num_neighbors  in  nequip.yaml ?
A: In  nequip.yaml , the  avg_num_neighbors  parameter is located under  training_module.model.avg_num_neighbors . Current default:  ${training_data_stats:num_neighbors_mean} .

Q: What is the default value for  training_module.model.avg_num_neighbors  in nequip.yaml?
A: The default value is  ${training_data_stats:num_neighbors_mean} .

Q: How do I set the  per_type_energy_scales  in  nequip.yaml ?
A: In  nequip.yaml , the  per_type_energy_scales  parameter is located under  training_module.model.per_type_energy_scales . Current default:  ${training_data_stats:per_type_forces_rms} .

Q: What is the default value for  training_module.model.per_type_energy_scales  in nequip.yaml?
A: The default value is  ${training_data_stats:per_type_forces_rms} .

Q: How do I set the  per_type_energy_shifts  in  nequip.yaml ?
A: In  nequip.yaml , the  per_type_energy_shifts  parameter is located under  training_module.model.per_type_energy_shifts . Current default:  ${training_data_stats:per_atom_energy_mean} .

Q: What is the default value for  training_module.model.per_type_energy_shifts  in nequip.yaml?
A: The default value is  ${training_data_stats:per_atom_energy_mean} .

Q: How do I set the  per_type_energy_scales_trainable  in  nequip.yaml ?
A: In  nequip.yaml , the  per_type_energy_scales_trainable  parameter is located under  training_module.model.per_type_energy_scales_trainable . Current default:  False .

Q: What is the default value for  training_module.model.per_type_energy_scales_trainable  in nequip.yaml?
A: The default value is  False .

Q: How do I set the  per_type_energy_shifts_trainable  in  nequip.yaml ?
A: In  nequip.yaml , the  per_type_energy_shifts_trainable  parameter is located under  training_module.model.per_type_energy_shifts_trainable . Current default:  False .

Q: What is the default value for  training_module.model.per_type_energy_shifts_trainable  in nequip.yaml?
A: The default value is  False .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module.model.pair_potential._target_ . Current default:  nequip.nn.pair_potential.ZBL .

Q: What is the default value for  training_module.model.pair_potential._target_  in nequip.yaml?
A: The default value is  nequip.nn.pair_potential.ZBL .

Q: How do I set the  units  in  nequip.yaml ?
A: In  nequip.yaml , the  units  parameter is located under  training_module.model.pair_potential.units . Current default:  metal .

Q: What is the default value for  training_module.model.pair_potential.units  in nequip.yaml?
A: The default value is  metal .

Q: How do I set the  chemical_species  in  nequip.yaml ?
A: In  nequip.yaml , the  chemical_species  parameter is located under  training_module.model.pair_potential.chemical_species . Current default:  ${chemical_symbols} .

Q: What is the default value for  training_module.model.pair_potential.chemical_species  in nequip.yaml?
A: The default value is  ${chemical_symbols} .

Q: How do I set the  allow_tf32  in  nequip.yaml ?
A: In  nequip.yaml , the  allow_tf32  parameter is located under  global_options.allow_tf32 . Current default:  False .

Q: What is the default value for  global_options.allow_tf32  in nequip.yaml?
A: The default value is  False .

Q: Show me the full content of painn.yaml.
A: Here is the content of  painn.yaml :
   yaml
# General settings
general:
    seed: 42  # Random seed for reproducibility
    database_name: 'Database.db'  # Name of the database file

# Data handling settings
data:
    dataset_path: 
    use_last_n: 100  # current testing
# Model architecture settings
model:
    model_type: painn   # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.
    n_rbf: 40
    n_atom_basis: 192
    n_interactions: 2
    
    dropout_rate: null # Default is 0.1 but it will not work when you select only 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null # [256, 128, 64, 32] (if null then n_neurons == n_atom_basis)
    distance_unit: 'Ang'
    property_unit_dict:
      energy: 'eV'
      forces: 'eV/Ang'

# Output settings
outputs:
    energy:
      loss_weight: 0.05
      metrics: "MAE"
    forces:
      loss_weight: 0.95
      metrics: "MAE"

# Training settings
training:
    accelerator: 'gpu'
    devices: 1
    precision: 32
    batch_size: 16
    num_train: 800
    num_val: 100
    num_test: 100
    max_epochs: 3
    num_workers: 24
    pin_memory: true
    log_every_n_steps: 1
    optimizer:
      type: 'AdamW'
      lr: 0.0001 #changed 0.0001 to 0.001
    scheduler:
      type: 'ReduceLROnPlateau'
      factor: 0.8
      patience: 30
      verbose: true
    early_stopping:      
      monitor: val_loss
      patience: 20
      min_delta: 0.001
      mode: min

# Logging and checkpoint settings
logging:
    folder: './results'
    log_dir: "lightning_logs"
    checkpoint_dir: "best_inference_model"
    monitor: "val_loss"

# Testing settings
testing:
    trained_model_path: './results'  # Path to load the trained model
    csv_file_name: 'actual_vs_predicted_enrgforc.csv'  # Path to save the predictions CSV

# Resume training settings
resume_training:
    resume_checkpoint_dir: null # Path to model checkpoint file if required 

# Fine Tuning settings
fine_tuning:
    pretrained_checkpoint: # checkpoint file path
    freeze_embedding: true
    freeze_interactions_up_to: 2 # add no of layers
    freeze_all_representation: true
    lr: 0.00005
    early_stopping_patience: 10
    best_model_dir: fine_tuned_best_model  # Subdirectory only
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory only
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
   

Q: How do I set the  seed  in  painn.yaml ?
A: In  painn.yaml , the  seed  parameter is located under  general.seed . Current default:  42 .

Q: What is the default value for  general.seed  in painn.yaml?
A: The default value is  42 .

Q: How do I set the  database_name  in  painn.yaml ?
A: In  painn.yaml , the  database_name  parameter is located under  general.database_name . Current default:  Database.db .

Q: What is the default value for  general.database_name  in painn.yaml?
A: The default value is  Database.db .

Q: How do I set the  dataset_path  in  painn.yaml ?
A: In  painn.yaml , the  dataset_path  parameter is located under  data.dataset_path . Current default:  None .

Q: What is the default value for  data.dataset_path  in painn.yaml?
A: The default value is  None .

Q: How do I set the  use_last_n  in  painn.yaml ?
A: In  painn.yaml , the  use_last_n  parameter is located under  data.use_last_n . Current default:  100 .

Q: What is the default value for  data.use_last_n  in painn.yaml?
A: The default value is  100 .

Q: How do I set the  model_type  in  painn.yaml ?
A: In  painn.yaml , the  model_type  parameter is located under  model.model_type . Current default:  painn .

Q: What is the default value for  model.model_type  in painn.yaml?
A: The default value is  painn .

Q: How do I set the  cutoff  in  painn.yaml ?
A: In  painn.yaml , the  cutoff  parameter is located under  model.cutoff . Current default:  12.0 .

Q: What is the default value for  model.cutoff  in painn.yaml?
A: The default value is  12.0 .

Q: How do I set the  n_rbf  in  painn.yaml ?
A: In  painn.yaml , the  n_rbf  parameter is located under  model.n_rbf . Current default:  40 .

Q: What is the default value for  model.n_rbf  in painn.yaml?
A: The default value is  40 .

Q: How do I set the  n_atom_basis  in  painn.yaml ?
A: In  painn.yaml , the  n_atom_basis  parameter is located under  model.n_atom_basis . Current default:  192 .

Q: What is the default value for  model.n_atom_basis  in painn.yaml?
A: The default value is  192 .

Q: How do I set the  n_interactions  in  painn.yaml ?
A: In  painn.yaml , the  n_interactions  parameter is located under  model.n_interactions . Current default:  2 .

Q: What is the default value for  model.n_interactions  in painn.yaml?
A: The default value is  2 .

Q: How do I set the  dropout_rate  in  painn.yaml ?
A: In  painn.yaml , the  dropout_rate  parameter is located under  model.dropout_rate . Current default:  None .

Q: What is the default value for  model.dropout_rate  in painn.yaml?
A: The default value is  None .

Q: How do I set the  n_layers  in  painn.yaml ?
A: In  painn.yaml , the  n_layers  parameter is located under  model.n_layers . Current default:  1 .

Q: What is the default value for  model.n_layers  in painn.yaml?
A: The default value is  1 .

Q: How do I set the  n_neurons  in  painn.yaml ?
A: In  painn.yaml , the  n_neurons  parameter is located under  model.n_neurons . Current default:  None .

Q: What is the default value for  model.n_neurons  in painn.yaml?
A: The default value is  None .

Q: How do I set the  distance_unit  in  painn.yaml ?
A: In  painn.yaml , the  distance_unit  parameter is located under  model.distance_unit . Current default:  Ang .

Q: What is the default value for  model.distance_unit  in painn.yaml?
A: The default value is  Ang .

Q: How do I set the  energy  in  painn.yaml ?
A: In  painn.yaml , the  energy  parameter is located under  model.property_unit_dict.energy . Current default:  eV .

Q: What is the default value for  model.property_unit_dict.energy  in painn.yaml?
A: The default value is  eV .

Q: How do I set the  forces  in  painn.yaml ?
A: In  painn.yaml , the  forces  parameter is located under  model.property_unit_dict.forces . Current default:  eV/Ang .

Q: What is the default value for  model.property_unit_dict.forces  in painn.yaml?
A: The default value is  eV/Ang .

Q: How do I set the  loss_weight  in  painn.yaml ?
A: In  painn.yaml , the  loss_weight  parameter is located under  outputs.energy.loss_weight . Current default:  0.05 .

Q: What is the default value for  outputs.energy.loss_weight  in painn.yaml?
A: The default value is  0.05 .

Q: How do I set the  metrics  in  painn.yaml ?
A: In  painn.yaml , the  metrics  parameter is located under  outputs.energy.metrics . Current default:  MAE .

Q: What is the default value for  outputs.energy.metrics  in painn.yaml?
A: The default value is  MAE .

Q: How do I set the  loss_weight  in  painn.yaml ?
A: In  painn.yaml , the  loss_weight  parameter is located under  outputs.forces.loss_weight . Current default:  0.95 .

Q: What is the default value for  outputs.forces.loss_weight  in painn.yaml?
A: The default value is  0.95 .

Q: How do I set the  metrics  in  painn.yaml ?
A: In  painn.yaml , the  metrics  parameter is located under  outputs.forces.metrics . Current default:  MAE .

Q: What is the default value for  outputs.forces.metrics  in painn.yaml?
A: The default value is  MAE .

Q: How do I set the  accelerator  in  painn.yaml ?
A: In  painn.yaml , the  accelerator  parameter is located under  training.accelerator . Current default:  gpu .

Q: What is the default value for  training.accelerator  in painn.yaml?
A: The default value is  gpu .

Q: How do I set the  devices  in  painn.yaml ?
A: In  painn.yaml , the  devices  parameter is located under  training.devices . Current default:  1 .

Q: What is the default value for  training.devices  in painn.yaml?
A: The default value is  1 .

Q: How do I set the  precision  in  painn.yaml ?
A: In  painn.yaml , the  precision  parameter is located under  training.precision . Current default:  32 .

Q: What is the default value for  training.precision  in painn.yaml?
A: The default value is  32 .

Q: How do I set the  batch_size  in  painn.yaml ?
A: In  painn.yaml , the  batch_size  parameter is located under  training.batch_size . Current default:  16 .

Q: What is the default value for  training.batch_size  in painn.yaml?
A: The default value is  16 .

Q: How do I set the  num_train  in  painn.yaml ?
A: In  painn.yaml , the  num_train  parameter is located under  training.num_train . Current default:  800 .

Q: What is the default value for  training.num_train  in painn.yaml?
A: The default value is  800 .

Q: How do I set the  num_val  in  painn.yaml ?
A: In  painn.yaml , the  num_val  parameter is located under  training.num_val . Current default:  100 .

Q: What is the default value for  training.num_val  in painn.yaml?
A: The default value is  100 .

Q: How do I set the  num_test  in  painn.yaml ?
A: In  painn.yaml , the  num_test  parameter is located under  training.num_test . Current default:  100 .

Q: What is the default value for  training.num_test  in painn.yaml?
A: The default value is  100 .

Q: How do I set the  max_epochs  in  painn.yaml ?
A: In  painn.yaml , the  max_epochs  parameter is located under  training.max_epochs . Current default:  3 .

Q: What is the default value for  training.max_epochs  in painn.yaml?
A: The default value is  3 .

Q: How do I set the  num_workers  in  painn.yaml ?
A: In  painn.yaml , the  num_workers  parameter is located under  training.num_workers . Current default:  24 .

Q: What is the default value for  training.num_workers  in painn.yaml?
A: The default value is  24 .

Q: How do I set the  pin_memory  in  painn.yaml ?
A: In  painn.yaml , the  pin_memory  parameter is located under  training.pin_memory . Current default:  True .

Q: What is the default value for  training.pin_memory  in painn.yaml?
A: The default value is  True .

Q: How do I set the  log_every_n_steps  in  painn.yaml ?
A: In  painn.yaml , the  log_every_n_steps  parameter is located under  training.log_every_n_steps . Current default:  1 .

Q: What is the default value for  training.log_every_n_steps  in painn.yaml?
A: The default value is  1 .

Q: How do I set the  type  in  painn.yaml ?
A: In  painn.yaml , the  type  parameter is located under  training.optimizer.type . Current default:  AdamW .

Q: What is the default value for  training.optimizer.type  in painn.yaml?
A: The default value is  AdamW .

Q: How do I set the  lr  in  painn.yaml ?
A: In  painn.yaml , the  lr  parameter is located under  training.optimizer.lr . Current default:  0.0001 .

Q: What is the default value for  training.optimizer.lr  in painn.yaml?
A: The default value is  0.0001 .

Q: How do I set the  type  in  painn.yaml ?
A: In  painn.yaml , the  type  parameter is located under  training.scheduler.type . Current default:  ReduceLROnPlateau .

Q: What is the default value for  training.scheduler.type  in painn.yaml?
A: The default value is  ReduceLROnPlateau .

Q: How do I set the  factor  in  painn.yaml ?
A: In  painn.yaml , the  factor  parameter is located under  training.scheduler.factor . Current default:  0.8 .

Q: What is the default value for  training.scheduler.factor  in painn.yaml?
A: The default value is  0.8 .

Q: How do I set the  patience  in  painn.yaml ?
A: In  painn.yaml , the  patience  parameter is located under  training.scheduler.patience . Current default:  30 .

Q: What is the default value for  training.scheduler.patience  in painn.yaml?
A: The default value is  30 .

Q: How do I set the  verbose  in  painn.yaml ?
A: In  painn.yaml , the  verbose  parameter is located under  training.scheduler.verbose . Current default:  True .

Q: What is the default value for  training.scheduler.verbose  in painn.yaml?
A: The default value is  True .

Q: How do I set the  monitor  in  painn.yaml ?
A: In  painn.yaml , the  monitor  parameter is located under  training.early_stopping.monitor . Current default:  val_loss .

Q: What is the default value for  training.early_stopping.monitor  in painn.yaml?
A: The default value is  val_loss .

Q: How do I set the  patience  in  painn.yaml ?
A: In  painn.yaml , the  patience  parameter is located under  training.early_stopping.patience . Current default:  20 .

Q: What is the default value for  training.early_stopping.patience  in painn.yaml?
A: The default value is  20 .

Q: How do I set the  min_delta  in  painn.yaml ?
A: In  painn.yaml , the  min_delta  parameter is located under  training.early_stopping.min_delta . Current default:  0.001 .

Q: What is the default value for  training.early_stopping.min_delta  in painn.yaml?
A: The default value is  0.001 .

Q: How do I set the  mode  in  painn.yaml ?
A: In  painn.yaml , the  mode  parameter is located under  training.early_stopping.mode . Current default:  min .

Q: What is the default value for  training.early_stopping.mode  in painn.yaml?
A: The default value is  min .

Q: How do I set the  folder  in  painn.yaml ?
A: In  painn.yaml , the  folder  parameter is located under  logging.folder . Current default:  ./results .

Q: What is the default value for  logging.folder  in painn.yaml?
A: The default value is  ./results .

Q: How do I set the  log_dir  in  painn.yaml ?
A: In  painn.yaml , the  log_dir  parameter is located under  logging.log_dir . Current default:  lightning_logs .

Q: What is the default value for  logging.log_dir  in painn.yaml?
A: The default value is  lightning_logs .

Q: How do I set the  checkpoint_dir  in  painn.yaml ?
A: In  painn.yaml , the  checkpoint_dir  parameter is located under  logging.checkpoint_dir . Current default:  best_inference_model .

Q: What is the default value for  logging.checkpoint_dir  in painn.yaml?
A: The default value is  best_inference_model .

Q: How do I set the  monitor  in  painn.yaml ?
A: In  painn.yaml , the  monitor  parameter is located under  logging.monitor . Current default:  val_loss .

Q: What is the default value for  logging.monitor  in painn.yaml?
A: The default value is  val_loss .

Q: How do I set the  trained_model_path  in  painn.yaml ?
A: In  painn.yaml , the  trained_model_path  parameter is located under  testing.trained_model_path . Current default:  ./results .

Q: What is the default value for  testing.trained_model_path  in painn.yaml?
A: The default value is  ./results .

Q: How do I set the  csv_file_name  in  painn.yaml ?
A: In  painn.yaml , the  csv_file_name  parameter is located under  testing.csv_file_name . Current default:  actual_vs_predicted_enrgforc.csv .

Q: What is the default value for  testing.csv_file_name  in painn.yaml?
A: The default value is  actual_vs_predicted_enrgforc.csv .

Q: How do I set the  resume_checkpoint_dir  in  painn.yaml ?
A: In  painn.yaml , the  resume_checkpoint_dir  parameter is located under  resume_training.resume_checkpoint_dir . Current default:  None .

Q: What is the default value for  resume_training.resume_checkpoint_dir  in painn.yaml?
A: The default value is  None .

Q: How do I set the  pretrained_checkpoint  in  painn.yaml ?
A: In  painn.yaml , the  pretrained_checkpoint  parameter is located under  fine_tuning.pretrained_checkpoint . Current default:  None .

Q: What is the default value for  fine_tuning.pretrained_checkpoint  in painn.yaml?
A: The default value is  None .

Q: How do I set the  freeze_embedding  in  painn.yaml ?
A: In  painn.yaml , the  freeze_embedding  parameter is located under  fine_tuning.freeze_embedding . Current default:  True .

Q: What is the default value for  fine_tuning.freeze_embedding  in painn.yaml?
A: The default value is  True .

Q: How do I set the  freeze_interactions_up_to  in  painn.yaml ?
A: In  painn.yaml , the  freeze_interactions_up_to  parameter is located under  fine_tuning.freeze_interactions_up_to . Current default:  2 .

Q: What is the default value for  fine_tuning.freeze_interactions_up_to  in painn.yaml?
A: The default value is  2 .

Q: How do I set the  freeze_all_representation  in  painn.yaml ?
A: In  painn.yaml , the  freeze_all_representation  parameter is located under  fine_tuning.freeze_all_representation . Current default:  True .

Q: What is the default value for  fine_tuning.freeze_all_representation  in painn.yaml?
A: The default value is  True .

Q: How do I set the  lr  in  painn.yaml ?
A: In  painn.yaml , the  lr  parameter is located under  fine_tuning.lr . Current default:  5e-05 .

Q: What is the default value for  fine_tuning.lr  in painn.yaml?
A: The default value is  5e-05 .

Q: How do I set the  early_stopping_patience  in  painn.yaml ?
A: In  painn.yaml , the  early_stopping_patience  parameter is located under  fine_tuning.early_stopping_patience . Current default:  10 .

Q: What is the default value for  fine_tuning.early_stopping_patience  in painn.yaml?
A: The default value is  10 .

Q: How do I set the  best_model_dir  in  painn.yaml ?
A: In  painn.yaml , the  best_model_dir  parameter is located under  fine_tuning.best_model_dir . Current default:  fine_tuned_best_model .

Q: What is the default value for  fine_tuning.best_model_dir  in painn.yaml?
A: The default value is  fine_tuned_best_model .

Q: How do I set the  checkpoint_dir  in  painn.yaml ?
A: In  painn.yaml , the  checkpoint_dir  parameter is located under  fine_tuning.checkpoint_dir . Current default:  fine_tuned_checkpoints .

Q: What is the default value for  fine_tuning.checkpoint_dir  in painn.yaml?
A: The default value is  fine_tuned_checkpoints .

Q: How do I set the  log_name  in  painn.yaml ?
A: In  painn.yaml , the  log_name  parameter is located under  fine_tuning.log_name . Current default:  fine_tune_logs .

Q: What is the default value for  fine_tuning.log_name  in painn.yaml?
A: The default value is  fine_tune_logs .

Q: Show me the full content of schnet.yaml.
A: Here is the content of  schnet.yaml :
   yaml
# General settings
general:
    seed: 42  # Random seed for reproducibility
    database_name: 'Database.db'  # Name of the database file

# Data handling settings
data:
    dataset_path: 
    use_last_n: 100  # current testing
# Model architecture settings
model:
    model_type: schnet   # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.
    n_rbf: 40
    n_atom_basis: 192
    n_interactions: 2
    
    dropout_rate: null # Default is 0.1 but it will not work when you select only 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null # [256, 128, 64, 32] (if null then n_neurons == n_atom_basis)
    distance_unit: 'Ang'
    property_unit_dict:
      energy: 'eV'
      forces: 'eV/Ang'

# Output settings
outputs:
    energy:
      loss_weight: 0.05
      metrics: "MAE"
    forces:
      loss_weight: 0.95
      metrics: "MAE"

# Training settings
training:
    accelerator: 'gpu'
    devices: 1
    precision: 32
    batch_size: 16
    num_train: 800
    num_val: 100
    num_test: 100
    max_epochs: 3
    num_workers: 24
    pin_memory: true
    log_every_n_steps: 1
    optimizer:
      type: 'AdamW'
      lr: 0.0001 #changed 0.0001 to 0.001
    scheduler:
      type: 'ReduceLROnPlateau'
      factor: 0.8
      patience: 30
      verbose: true
    early_stopping:      
      monitor: val_loss
      patience: 20
      min_delta: 0.001
      mode: min

# Logging and checkpoint settings
logging:
    folder: './results'
    log_dir: "lightning_logs"
    checkpoint_dir: "best_inference_model"
    monitor: "val_loss"

# Testing settings
testing:
    trained_model_path: './results'  # Path to load the trained model
    csv_file_name: 'actual_vs_predicted_enrgforc.csv'  # Path to save the predictions CSV

# Resume training settings
resume_training:
    resume_checkpoint_dir: null # Path to model checkpoint file if required 

# Fine Tuning settings
fine_tuning:
    pretrained_checkpoint: # checkpoint file path
    freeze_embedding: true
    freeze_interactions_up_to: 2 # add no of layers
    freeze_all_representation: true
    lr: 0.00005
    early_stopping_patience: 10
    best_model_dir: fine_tuned_best_model  # Subdirectory only
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory only
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
   

Q: How do I set the  seed  in  schnet.yaml ?
A: In  schnet.yaml , the  seed  parameter is located under  general.seed . Current default:  42 .

Q: What is the default value for  general.seed  in schnet.yaml?
A: The default value is  42 .

Q: How do I set the  database_name  in  schnet.yaml ?
A: In  schnet.yaml , the  database_name  parameter is located under  general.database_name . Current default:  Database.db .

Q: What is the default value for  general.database_name  in schnet.yaml?
A: The default value is  Database.db .

Q: How do I set the  dataset_path  in  schnet.yaml ?
A: In  schnet.yaml , the  dataset_path  parameter is located under  data.dataset_path . Current default:  None .

Q: What is the default value for  data.dataset_path  in schnet.yaml?
A: The default value is  None .

Q: How do I set the  use_last_n  in  schnet.yaml ?
A: In  schnet.yaml , the  use_last_n  parameter is located under  data.use_last_n . Current default:  100 .

Q: What is the default value for  data.use_last_n  in schnet.yaml?
A: The default value is  100 .

Q: How do I set the  model_type  in  schnet.yaml ?
A: In  schnet.yaml , the  model_type  parameter is located under  model.model_type . Current default:  schnet .

Q: What is the default value for  model.model_type  in schnet.yaml?
A: The default value is  schnet .

Q: How do I set the  cutoff  in  schnet.yaml ?
A: In  schnet.yaml , the  cutoff  parameter is located under  model.cutoff . Current default:  12.0 .

Q: What is the default value for  model.cutoff  in schnet.yaml?
A: The default value is  12.0 .

Q: How do I set the  n_rbf  in  schnet.yaml ?
A: In  schnet.yaml , the  n_rbf  parameter is located under  model.n_rbf . Current default:  40 .

Q: What is the default value for  model.n_rbf  in schnet.yaml?
A: The default value is  40 .

Q: How do I set the  n_atom_basis  in  schnet.yaml ?
A: In  schnet.yaml , the  n_atom_basis  parameter is located under  model.n_atom_basis . Current default:  192 .

Q: What is the default value for  model.n_atom_basis  in schnet.yaml?
A: The default value is  192 .

Q: How do I set the  n_interactions  in  schnet.yaml ?
A: In  schnet.yaml , the  n_interactions  parameter is located under  model.n_interactions . Current default:  2 .

Q: What is the default value for  model.n_interactions  in schnet.yaml?
A: The default value is  2 .

Q: How do I set the  dropout_rate  in  schnet.yaml ?
A: In  schnet.yaml , the  dropout_rate  parameter is located under  model.dropout_rate . Current default:  None .

Q: What is the default value for  model.dropout_rate  in schnet.yaml?
A: The default value is  None .

Q: How do I set the  n_layers  in  schnet.yaml ?
A: In  schnet.yaml , the  n_layers  parameter is located under  model.n_layers . Current default:  1 .

Q: What is the default value for  model.n_layers  in schnet.yaml?
A: The default value is  1 .

Q: How do I set the  n_neurons  in  schnet.yaml ?
A: In  schnet.yaml , the  n_neurons  parameter is located under  model.n_neurons . Current default:  None .

Q: What is the default value for  model.n_neurons  in schnet.yaml?
A: The default value is  None .

Q: How do I set the  distance_unit  in  schnet.yaml ?
A: In  schnet.yaml , the  distance_unit  parameter is located under  model.distance_unit . Current default:  Ang .

Q: What is the default value for  model.distance_unit  in schnet.yaml?
A: The default value is  Ang .

Q: How do I set the  energy  in  schnet.yaml ?
A: In  schnet.yaml , the  energy  parameter is located under  model.property_unit_dict.energy . Current default:  eV .

Q: What is the default value for  model.property_unit_dict.energy  in schnet.yaml?
A: The default value is  eV .

Q: How do I set the  forces  in  schnet.yaml ?
A: In  schnet.yaml , the  forces  parameter is located under  model.property_unit_dict.forces . Current default:  eV/Ang .

Q: What is the default value for  model.property_unit_dict.forces  in schnet.yaml?
A: The default value is  eV/Ang .

Q: How do I set the  loss_weight  in  schnet.yaml ?
A: In  schnet.yaml , the  loss_weight  parameter is located under  outputs.energy.loss_weight . Current default:  0.05 .

Q: What is the default value for  outputs.energy.loss_weight  in schnet.yaml?
A: The default value is  0.05 .

Q: How do I set the  metrics  in  schnet.yaml ?
A: In  schnet.yaml , the  metrics  parameter is located under  outputs.energy.metrics . Current default:  MAE .

Q: What is the default value for  outputs.energy.metrics  in schnet.yaml?
A: The default value is  MAE .

Q: How do I set the  loss_weight  in  schnet.yaml ?
A: In  schnet.yaml , the  loss_weight  parameter is located under  outputs.forces.loss_weight . Current default:  0.95 .

Q: What is the default value for  outputs.forces.loss_weight  in schnet.yaml?
A: The default value is  0.95 .

Q: How do I set the  metrics  in  schnet.yaml ?
A: In  schnet.yaml , the  metrics  parameter is located under  outputs.forces.metrics . Current default:  MAE .

Q: What is the default value for  outputs.forces.metrics  in schnet.yaml?
A: The default value is  MAE .

Q: How do I set the  accelerator  in  schnet.yaml ?
A: In  schnet.yaml , the  accelerator  parameter is located under  training.accelerator . Current default:  gpu .

Q: What is the default value for  training.accelerator  in schnet.yaml?
A: The default value is  gpu .

Q: How do I set the  devices  in  schnet.yaml ?
A: In  schnet.yaml , the  devices  parameter is located under  training.devices . Current default:  1 .

Q: What is the default value for  training.devices  in schnet.yaml?
A: The default value is  1 .

Q: How do I set the  precision  in  schnet.yaml ?
A: In  schnet.yaml , the  precision  parameter is located under  training.precision . Current default:  32 .

Q: What is the default value for  training.precision  in schnet.yaml?
A: The default value is  32 .

Q: How do I set the  batch_size  in  schnet.yaml ?
A: In  schnet.yaml , the  batch_size  parameter is located under  training.batch_size . Current default:  16 .

Q: What is the default value for  training.batch_size  in schnet.yaml?
A: The default value is  16 .

Q: How do I set the  num_train  in  schnet.yaml ?
A: In  schnet.yaml , the  num_train  parameter is located under  training.num_train . Current default:  800 .

Q: What is the default value for  training.num_train  in schnet.yaml?
A: The default value is  800 .

Q: How do I set the  num_val  in  schnet.yaml ?
A: In  schnet.yaml , the  num_val  parameter is located under  training.num_val . Current default:  100 .

Q: What is the default value for  training.num_val  in schnet.yaml?
A: The default value is  100 .

Q: How do I set the  num_test  in  schnet.yaml ?
A: In  schnet.yaml , the  num_test  parameter is located under  training.num_test . Current default:  100 .

Q: What is the default value for  training.num_test  in schnet.yaml?
A: The default value is  100 .

Q: How do I set the  max_epochs  in  schnet.yaml ?
A: In  schnet.yaml , the  max_epochs  parameter is located under  training.max_epochs . Current default:  3 .

Q: What is the default value for  training.max_epochs  in schnet.yaml?
A: The default value is  3 .

Q: How do I set the  num_workers  in  schnet.yaml ?
A: In  schnet.yaml , the  num_workers  parameter is located under  training.num_workers . Current default:  24 .

Q: What is the default value for  training.num_workers  in schnet.yaml?
A: The default value is  24 .

Q: How do I set the  pin_memory  in  schnet.yaml ?
A: In  schnet.yaml , the  pin_memory  parameter is located under  training.pin_memory . Current default:  True .

Q: What is the default value for  training.pin_memory  in schnet.yaml?
A: The default value is  True .

Q: How do I set the  log_every_n_steps  in  schnet.yaml ?
A: In  schnet.yaml , the  log_every_n_steps  parameter is located under  training.log_every_n_steps . Current default:  1 .

Q: What is the default value for  training.log_every_n_steps  in schnet.yaml?
A: The default value is  1 .

Q: How do I set the  type  in  schnet.yaml ?
A: In  schnet.yaml , the  type  parameter is located under  training.optimizer.type . Current default:  AdamW .

Q: What is the default value for  training.optimizer.type  in schnet.yaml?
A: The default value is  AdamW .

Q: How do I set the  lr  in  schnet.yaml ?
A: In  schnet.yaml , the  lr  parameter is located under  training.optimizer.lr . Current default:  0.0001 .

Q: What is the default value for  training.optimizer.lr  in schnet.yaml?
A: The default value is  0.0001 .

Q: How do I set the  type  in  schnet.yaml ?
A: In  schnet.yaml , the  type  parameter is located under  training.scheduler.type . Current default:  ReduceLROnPlateau .

Q: What is the default value for  training.scheduler.type  in schnet.yaml?
A: The default value is  ReduceLROnPlateau .

Q: How do I set the  factor  in  schnet.yaml ?
A: In  schnet.yaml , the  factor  parameter is located under  training.scheduler.factor . Current default:  0.8 .

Q: What is the default value for  training.scheduler.factor  in schnet.yaml?
A: The default value is  0.8 .

Q: How do I set the  patience  in  schnet.yaml ?
A: In  schnet.yaml , the  patience  parameter is located under  training.scheduler.patience . Current default:  30 .

Q: What is the default value for  training.scheduler.patience  in schnet.yaml?
A: The default value is  30 .

Q: How do I set the  verbose  in  schnet.yaml ?
A: In  schnet.yaml , the  verbose  parameter is located under  training.scheduler.verbose . Current default:  True .

Q: What is the default value for  training.scheduler.verbose  in schnet.yaml?
A: The default value is  True .

Q: How do I set the  monitor  in  schnet.yaml ?
A: In  schnet.yaml , the  monitor  parameter is located under  training.early_stopping.monitor . Current default:  val_loss .

Q: What is the default value for  training.early_stopping.monitor  in schnet.yaml?
A: The default value is  val_loss .

Q: How do I set the  patience  in  schnet.yaml ?
A: In  schnet.yaml , the  patience  parameter is located under  training.early_stopping.patience . Current default:  20 .

Q: What is the default value for  training.early_stopping.patience  in schnet.yaml?
A: The default value is  20 .

Q: How do I set the  min_delta  in  schnet.yaml ?
A: In  schnet.yaml , the  min_delta  parameter is located under  training.early_stopping.min_delta . Current default:  0.001 .

Q: What is the default value for  training.early_stopping.min_delta  in schnet.yaml?
A: The default value is  0.001 .

Q: How do I set the  mode  in  schnet.yaml ?
A: In  schnet.yaml , the  mode  parameter is located under  training.early_stopping.mode . Current default:  min .

Q: What is the default value for  training.early_stopping.mode  in schnet.yaml?
A: The default value is  min .

Q: How do I set the  folder  in  schnet.yaml ?
A: In  schnet.yaml , the  folder  parameter is located under  logging.folder . Current default:  ./results .

Q: What is the default value for  logging.folder  in schnet.yaml?
A: The default value is  ./results .

Q: How do I set the  log_dir  in  schnet.yaml ?
A: In  schnet.yaml , the  log_dir  parameter is located under  logging.log_dir . Current default:  lightning_logs .

Q: What is the default value for  logging.log_dir  in schnet.yaml?
A: The default value is  lightning_logs .

Q: How do I set the  checkpoint_dir  in  schnet.yaml ?
A: In  schnet.yaml , the  checkpoint_dir  parameter is located under  logging.checkpoint_dir . Current default:  best_inference_model .

Q: What is the default value for  logging.checkpoint_dir  in schnet.yaml?
A: The default value is  best_inference_model .

Q: How do I set the  monitor  in  schnet.yaml ?
A: In  schnet.yaml , the  monitor  parameter is located under  logging.monitor . Current default:  val_loss .

Q: What is the default value for  logging.monitor  in schnet.yaml?
A: The default value is  val_loss .

Q: How do I set the  trained_model_path  in  schnet.yaml ?
A: In  schnet.yaml , the  trained_model_path  parameter is located under  testing.trained_model_path . Current default:  ./results .

Q: What is the default value for  testing.trained_model_path  in schnet.yaml?
A: The default value is  ./results .

Q: How do I set the  csv_file_name  in  schnet.yaml ?
A: In  schnet.yaml , the  csv_file_name  parameter is located under  testing.csv_file_name . Current default:  actual_vs_predicted_enrgforc.csv .

Q: What is the default value for  testing.csv_file_name  in schnet.yaml?
A: The default value is  actual_vs_predicted_enrgforc.csv .

Q: How do I set the  resume_checkpoint_dir  in  schnet.yaml ?
A: In  schnet.yaml , the  resume_checkpoint_dir  parameter is located under  resume_training.resume_checkpoint_dir . Current default:  None .

Q: What is the default value for  resume_training.resume_checkpoint_dir  in schnet.yaml?
A: The default value is  None .

Q: How do I set the  pretrained_checkpoint  in  schnet.yaml ?
A: In  schnet.yaml , the  pretrained_checkpoint  parameter is located under  fine_tuning.pretrained_checkpoint . Current default:  None .

Q: What is the default value for  fine_tuning.pretrained_checkpoint  in schnet.yaml?
A: The default value is  None .

Q: How do I set the  freeze_embedding  in  schnet.yaml ?
A: In  schnet.yaml , the  freeze_embedding  parameter is located under  fine_tuning.freeze_embedding . Current default:  True .

Q: What is the default value for  fine_tuning.freeze_embedding  in schnet.yaml?
A: The default value is  True .

Q: How do I set the  freeze_interactions_up_to  in  schnet.yaml ?
A: In  schnet.yaml , the  freeze_interactions_up_to  parameter is located under  fine_tuning.freeze_interactions_up_to . Current default:  2 .

Q: What is the default value for  fine_tuning.freeze_interactions_up_to  in schnet.yaml?
A: The default value is  2 .

Q: How do I set the  freeze_all_representation  in  schnet.yaml ?
A: In  schnet.yaml , the  freeze_all_representation  parameter is located under  fine_tuning.freeze_all_representation . Current default:  True .

Q: What is the default value for  fine_tuning.freeze_all_representation  in schnet.yaml?
A: The default value is  True .

Q: How do I set the  lr  in  schnet.yaml ?
A: In  schnet.yaml , the  lr  parameter is located under  fine_tuning.lr . Current default:  5e-05 .

Q: What is the default value for  fine_tuning.lr  in schnet.yaml?
A: The default value is  5e-05 .

Q: How do I set the  early_stopping_patience  in  schnet.yaml ?
A: In  schnet.yaml , the  early_stopping_patience  parameter is located under  fine_tuning.early_stopping_patience . Current default:  10 .

Q: What is the default value for  fine_tuning.early_stopping_patience  in schnet.yaml?
A: The default value is  10 .

Q: How do I set the  best_model_dir  in  schnet.yaml ?
A: In  schnet.yaml , the  best_model_dir  parameter is located under  fine_tuning.best_model_dir . Current default:  fine_tuned_best_model .

Q: What is the default value for  fine_tuning.best_model_dir  in schnet.yaml?
A: The default value is  fine_tuned_best_model .

Q: How do I set the  checkpoint_dir  in  schnet.yaml ?
A: In  schnet.yaml , the  checkpoint_dir  parameter is located under  fine_tuning.checkpoint_dir . Current default:  fine_tuned_checkpoints .

Q: What is the default value for  fine_tuning.checkpoint_dir  in schnet.yaml?
A: The default value is  fine_tuned_checkpoints .

Q: How do I set the  log_name  in  schnet.yaml ?
A: In  schnet.yaml , the  log_name  parameter is located under  fine_tuning.log_name . Current default:  fine_tune_logs .

Q: What is the default value for  fine_tuning.log_name  in schnet.yaml?
A: The default value is  fine_tune_logs .

Q: Show me the full content of allegro.yaml.
A: Here is the content of  allegro.yaml :
   yaml
run: [train, val, test]


cutoff_radius: 12.0
chemical_symbols: [Cd, Cl, Se] 
model_type_names: ${chemical_symbols}

data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 456             
  split_dataset:
    file_path:  #./consolidated_dataset_1000_CdSe_new.xyz
    train: 0.8
    val: 0.1
    test: 0.1
  transforms:
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}
  
  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
  test_dataloader: ${data.val_dataloader}
  stats_manager:
    _target_: nequip.data.CommonDataStatisticsManager
    type_names: ${model_type_names}

trainer:
  _target_: lightning.Trainer
  accelerator: auto
  devices: 1    # NO of GPUs
  max_epochs: 3
  check_val_every_n_epoch: 1
  log_every_n_steps: 5
  
  callbacks:
      
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ./results 
      filename: best
      save_last: true
    
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      min_delta: 1e-3                         # how much to be considered a "change"
      patience: 20                            # how many instances of "no change" before stopping
      
  logger:
    - _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ./logs
      name: tutorial_log
      version: null
      default_hp_metric: false


# NOTE:
# interpolation parameters for Allegro model
num_scalar_features: 64


training_module:
  _target_: nequip.train.EMALightningModule
  
  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 0.05
      forces: 0.95
      
  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      per_atom_energy_mae: 0.05
      forces_mae: 0.95
      
  train_metrics: ${training_module.val_metrics}
  test_metrics: ${training_module.val_metrics}
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
  # ^ IMPORTANT: Allegro models do better with learning rates around 1e-3

  # to use the Allegro model in the NequIP framework, the following  model  block has to be changed to be that of Allegro's
  model:
    _target_: allegro.model.AllegroModel

    # === basic model params ===
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    # === two-body scalar embedding ===
    radial_chemical_embed:
      # the defaults for the Bessel embedding module are usually appropriate
      _target_: allegro.nn.TwoBodyBesselScalarEmbed
      num_bessels: 8
      bessel_trainable: false
      polynomial_cutoff_p: 6

    # output dimension of the radial-chemical embedding
    radial_chemical_embed_dim: ${num_scalar_features}

    # scalar embedding MLP
    scalar_embed_mlp_hidden_layers_depth: 1
    scalar_embed_mlp_hidden_layers_width: ${num_scalar_features}
    scalar_embed_mlp_nonlinearity: silu

    # === core hyperparameters ===
    # The following hyperparameters are the main ones that one should focus on tuning.

    # maximum order l to use in spherical harmonics embedding, 1 is baseline (fast), 2 is more accurate, but slower, 3 highly accurate but slow
    l_max: 1

    # number of tensor product layers, 1-3 usually best, more is more accurate but slower
    num_layers: 2

    # number of scalar features, more is more accurate but slower
    # 16, 32, 64, 128, 256 are good options to try depending on the dataset
    num_scalar_features: ${num_scalar_features}

    # number of tensor features, more is more accurate but slower
    # 8, 16, 32, 64 are good options to try depending on the dataset
    num_tensor_features: 32

    # == allegro MLPs ==
    # neural network parameters in the Allegro layers
    allegro_mlp_hidden_layers_depth: 1
    allegro_mlp_hidden_layers_width: ${num_scalar_features}
    allegro_mlp_nonlinearity: silu
    # ^ setting  nonlinearity  to  null  means that the Allegro MLPs are effectively linear layers

    # === advanced hyperparameters ===
    # The following hyperparameters should remain in their default states until the above core hyperparameters have been set.

    # whether to include features with odd mirror parity
    # often turning parity off gives equally good results but faster networks, so do consider this
    parity: true

    # whether the tensor product weights couple the paths and channels or not (otherwise the weights are only applied per-path)
    # default is  true , which is expected to be more expressive than  false 
    tp_path_channel_coupling: true

    # == readout MLP ==
    # neural network parameters in the readout layer
    readout_mlp_hidden_layers_depth: 1
    readout_mlp_hidden_layers_width: ${num_scalar_features}
    readout_mlp_nonlinearity: silu
    # ^ setting  nonlinearity  to  null  means that output MLP is effectively a linear layer

    # === misc hyperparameters ===
    # average number of neighbors for edge sum normalization
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}

    # per-type per-atom scales and shifts
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    # ^ this should typically be the isolated atom energies for your dataset
    #   provided as a dict, e.g.
    # per_type_energy_shifts: 
    #   C: 1.234
    #   H: 2.345
    #   O: 3.456
    per_type_energy_scales: ${training_data_stats:forces_rms}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    # ZBL pair potential (optional, can be removed or included depending on aplication)
    # see NequIP docs for details:
    # https://nequip.readthedocs.io/en/latest/api/nn.html#nequip.nn.pair_potential.ZBL
    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: real     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types


global_options:
  allow_tf32: false
   

Q: How do I set the  run  in  allegro.yaml ?
A: In  allegro.yaml , the  run  parameter is located under  run . Current default:  ['train', 'val', 'test'] .

Q: What is the default value for  run  in allegro.yaml?
A: The default value is  ['train', 'val', 'test'] .

Q: How do I set the  cutoff_radius  in  allegro.yaml ?
A: In  allegro.yaml , the  cutoff_radius  parameter is located under  cutoff_radius . Current default:  12.0 .

Q: What is the default value for  cutoff_radius  in allegro.yaml?
A: The default value is  12.0 .

Q: How do I set the  chemical_symbols  in  allegro.yaml ?
A: In  allegro.yaml , the  chemical_symbols  parameter is located under  chemical_symbols . Current default:  ['Cd', 'Cl', 'Se'] .

Q: What is the default value for  chemical_symbols  in allegro.yaml?
A: The default value is  ['Cd', 'Cl', 'Se'] .

Q: How do I set the  model_type_names  in  allegro.yaml ?
A: In  allegro.yaml , the  model_type_names  parameter is located under  model_type_names . Current default:  ${chemical_symbols} .

Q: What is the default value for  model_type_names  in allegro.yaml?
A: The default value is  ${chemical_symbols} .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  data._target_ . Current default:  nequip.data.datamodule.ASEDataModule .

Q: What is the default value for  data._target_  in allegro.yaml?
A: The default value is  nequip.data.datamodule.ASEDataModule .

Q: How do I set the  seed  in  allegro.yaml ?
A: In  allegro.yaml , the  seed  parameter is located under  data.seed . Current default:  456 .

Q: What is the default value for  data.seed  in allegro.yaml?
A: The default value is  456 .

Q: How do I set the  file_path  in  allegro.yaml ?
A: In  allegro.yaml , the  file_path  parameter is located under  data.split_dataset.file_path . Current default:  None .

Q: What is the default value for  data.split_dataset.file_path  in allegro.yaml?
A: The default value is  None .

Q: How do I set the  train  in  allegro.yaml ?
A: In  allegro.yaml , the  train  parameter is located under  data.split_dataset.train . Current default:  0.8 .

Q: What is the default value for  data.split_dataset.train  in allegro.yaml?
A: The default value is  0.8 .

Q: How do I set the  val  in  allegro.yaml ?
A: In  allegro.yaml , the  val  parameter is located under  data.split_dataset.val . Current default:  0.1 .

Q: What is the default value for  data.split_dataset.val  in allegro.yaml?
A: The default value is  0.1 .

Q: How do I set the  test  in  allegro.yaml ?
A: In  allegro.yaml , the  test  parameter is located under  data.split_dataset.test . Current default:  0.1 .

Q: What is the default value for  data.split_dataset.test  in allegro.yaml?
A: The default value is  0.1 .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  data.train_dataloader._target_ . Current default:  torch.utils.data.DataLoader .

Q: What is the default value for  data.train_dataloader._target_  in allegro.yaml?
A: The default value is  torch.utils.data.DataLoader .

Q: How do I set the  batch_size  in  allegro.yaml ?
A: In  allegro.yaml , the  batch_size  parameter is located under  data.train_dataloader.batch_size . Current default:  16 .

Q: What is the default value for  data.train_dataloader.batch_size  in allegro.yaml?
A: The default value is  16 .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  data.val_dataloader._target_ . Current default:  torch.utils.data.DataLoader .

Q: What is the default value for  data.val_dataloader._target_  in allegro.yaml?
A: The default value is  torch.utils.data.DataLoader .

Q: How do I set the  batch_size  in  allegro.yaml ?
A: In  allegro.yaml , the  batch_size  parameter is located under  data.val_dataloader.batch_size . Current default:  16 .

Q: What is the default value for  data.val_dataloader.batch_size  in allegro.yaml?
A: The default value is  16 .

Q: How do I set the  test_dataloader  in  allegro.yaml ?
A: In  allegro.yaml , the  test_dataloader  parameter is located under  data.test_dataloader . Current default:  ${data.val_dataloader} .

Q: What is the default value for  data.test_dataloader  in allegro.yaml?
A: The default value is  ${data.val_dataloader} .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  data.stats_manager._target_ . Current default:  nequip.data.CommonDataStatisticsManager .

Q: What is the default value for  data.stats_manager._target_  in allegro.yaml?
A: The default value is  nequip.data.CommonDataStatisticsManager .

Q: How do I set the  type_names  in  allegro.yaml ?
A: In  allegro.yaml , the  type_names  parameter is located under  data.stats_manager.type_names . Current default:  ${model_type_names} .

Q: What is the default value for  data.stats_manager.type_names  in allegro.yaml?
A: The default value is  ${model_type_names} .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  trainer._target_ . Current default:  lightning.Trainer .

Q: What is the default value for  trainer._target_  in allegro.yaml?
A: The default value is  lightning.Trainer .

Q: How do I set the  accelerator  in  allegro.yaml ?
A: In  allegro.yaml , the  accelerator  parameter is located under  trainer.accelerator . Current default:  auto .

Q: What is the default value for  trainer.accelerator  in allegro.yaml?
A: The default value is  auto .

Q: How do I set the  devices  in  allegro.yaml ?
A: In  allegro.yaml , the  devices  parameter is located under  trainer.devices . Current default:  1 .

Q: What is the default value for  trainer.devices  in allegro.yaml?
A: The default value is  1 .

Q: How do I set the  max_epochs  in  allegro.yaml ?
A: In  allegro.yaml , the  max_epochs  parameter is located under  trainer.max_epochs . Current default:  3 .

Q: What is the default value for  trainer.max_epochs  in allegro.yaml?
A: The default value is  3 .

Q: How do I set the  check_val_every_n_epoch  in  allegro.yaml ?
A: In  allegro.yaml , the  check_val_every_n_epoch  parameter is located under  trainer.check_val_every_n_epoch . Current default:  1 .

Q: What is the default value for  trainer.check_val_every_n_epoch  in allegro.yaml?
A: The default value is  1 .

Q: How do I set the  log_every_n_steps  in  allegro.yaml ?
A: In  allegro.yaml , the  log_every_n_steps  parameter is located under  trainer.log_every_n_steps . Current default:  5 .

Q: What is the default value for  trainer.log_every_n_steps  in allegro.yaml?
A: The default value is  5 .

Q: How do I set the  num_scalar_features  in  allegro.yaml ?
A: In  allegro.yaml , the  num_scalar_features  parameter is located under  num_scalar_features . Current default:  64 .

Q: What is the default value for  num_scalar_features  in allegro.yaml?
A: The default value is  64 .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module._target_ . Current default:  nequip.train.EMALightningModule .

Q: What is the default value for  training_module._target_  in allegro.yaml?
A: The default value is  nequip.train.EMALightningModule .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module.loss._target_ . Current default:  nequip.train.EnergyForceLoss .

Q: What is the default value for  training_module.loss._target_  in allegro.yaml?
A: The default value is  nequip.train.EnergyForceLoss .

Q: How do I set the  per_atom_energy  in  allegro.yaml ?
A: In  allegro.yaml , the  per_atom_energy  parameter is located under  training_module.loss.per_atom_energy . Current default:  True .

Q: What is the default value for  training_module.loss.per_atom_energy  in allegro.yaml?
A: The default value is  True .

Q: How do I set the  total_energy  in  allegro.yaml ?
A: In  allegro.yaml , the  total_energy  parameter is located under  training_module.loss.coeffs.total_energy . Current default:  0.05 .

Q: What is the default value for  training_module.loss.coeffs.total_energy  in allegro.yaml?
A: The default value is  0.05 .

Q: How do I set the  forces  in  allegro.yaml ?
A: In  allegro.yaml , the  forces  parameter is located under  training_module.loss.coeffs.forces . Current default:  0.95 .

Q: What is the default value for  training_module.loss.coeffs.forces  in allegro.yaml?
A: The default value is  0.95 .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module.val_metrics._target_ . Current default:  nequip.train.EnergyForceMetrics .

Q: What is the default value for  training_module.val_metrics._target_  in allegro.yaml?
A: The default value is  nequip.train.EnergyForceMetrics .

Q: How do I set the  per_atom_energy_mae  in  allegro.yaml ?
A: In  allegro.yaml , the  per_atom_energy_mae  parameter is located under  training_module.val_metrics.coeffs.per_atom_energy_mae . Current default:  0.05 .

Q: What is the default value for  training_module.val_metrics.coeffs.per_atom_energy_mae  in allegro.yaml?
A: The default value is  0.05 .

Q: How do I set the  forces_mae  in  allegro.yaml ?
A: In  allegro.yaml , the  forces_mae  parameter is located under  training_module.val_metrics.coeffs.forces_mae . Current default:  0.95 .

Q: What is the default value for  training_module.val_metrics.coeffs.forces_mae  in allegro.yaml?
A: The default value is  0.95 .

Q: How do I set the  train_metrics  in  allegro.yaml ?
A: In  allegro.yaml , the  train_metrics  parameter is located under  training_module.train_metrics . Current default:  ${training_module.val_metrics} .

Q: What is the default value for  training_module.train_metrics  in allegro.yaml?
A: The default value is  ${training_module.val_metrics} .

Q: How do I set the  test_metrics  in  allegro.yaml ?
A: In  allegro.yaml , the  test_metrics  parameter is located under  training_module.test_metrics . Current default:  ${training_module.val_metrics} .

Q: What is the default value for  training_module.test_metrics  in allegro.yaml?
A: The default value is  ${training_module.val_metrics} .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module.optimizer._target_ . Current default:  torch.optim.Adam .

Q: What is the default value for  training_module.optimizer._target_  in allegro.yaml?
A: The default value is  torch.optim.Adam .

Q: How do I set the  lr  in  allegro.yaml ?
A: In  allegro.yaml , the  lr  parameter is located under  training_module.optimizer.lr . Current default:  0.001 .

Q: What is the default value for  training_module.optimizer.lr  in allegro.yaml?
A: The default value is  0.001 .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module.model._target_ . Current default:  allegro.model.AllegroModel .

Q: What is the default value for  training_module.model._target_  in allegro.yaml?
A: The default value is  allegro.model.AllegroModel .

Q: How do I set the  seed  in  allegro.yaml ?
A: In  allegro.yaml , the  seed  parameter is located under  training_module.model.seed . Current default:  456 .

Q: What is the default value for  training_module.model.seed  in allegro.yaml?
A: The default value is  456 .

Q: How do I set the  model_dtype  in  allegro.yaml ?
A: In  allegro.yaml , the  model_dtype  parameter is located under  training_module.model.model_dtype . Current default:  float32 .

Q: What is the default value for  training_module.model.model_dtype  in allegro.yaml?
A: The default value is  float32 .

Q: How do I set the  type_names  in  allegro.yaml ?
A: In  allegro.yaml , the  type_names  parameter is located under  training_module.model.type_names . Current default:  ${model_type_names} .

Q: What is the default value for  training_module.model.type_names  in allegro.yaml?
A: The default value is  ${model_type_names} .

Q: How do I set the  r_max  in  allegro.yaml ?
A: In  allegro.yaml , the  r_max  parameter is located under  training_module.model.r_max . Current default:  ${cutoff_radius} .

Q: What is the default value for  training_module.model.r_max  in allegro.yaml?
A: The default value is  ${cutoff_radius} .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module.model.radial_chemical_embed._target_ . Current default:  allegro.nn.TwoBodyBesselScalarEmbed .

Q: What is the default value for  training_module.model.radial_chemical_embed._target_  in allegro.yaml?
A: The default value is  allegro.nn.TwoBodyBesselScalarEmbed .

Q: How do I set the  num_bessels  in  allegro.yaml ?
A: In  allegro.yaml , the  num_bessels  parameter is located under  training_module.model.radial_chemical_embed.num_bessels . Current default:  8 .

Q: What is the default value for  training_module.model.radial_chemical_embed.num_bessels  in allegro.yaml?
A: The default value is  8 .

Q: How do I set the  bessel_trainable  in  allegro.yaml ?
A: In  allegro.yaml , the  bessel_trainable  parameter is located under  training_module.model.radial_chemical_embed.bessel_trainable . Current default:  False .

Q: What is the default value for  training_module.model.radial_chemical_embed.bessel_trainable  in allegro.yaml?
A: The default value is  False .

Q: How do I set the  polynomial_cutoff_p  in  allegro.yaml ?
A: In  allegro.yaml , the  polynomial_cutoff_p  parameter is located under  training_module.model.radial_chemical_embed.polynomial_cutoff_p . Current default:  6 .

Q: What is the default value for  training_module.model.radial_chemical_embed.polynomial_cutoff_p  in allegro.yaml?
A: The default value is  6 .

Q: How do I set the  radial_chemical_embed_dim  in  allegro.yaml ?
A: In  allegro.yaml , the  radial_chemical_embed_dim  parameter is located under  training_module.model.radial_chemical_embed_dim . Current default:  ${num_scalar_features} .

Q: What is the default value for  training_module.model.radial_chemical_embed_dim  in allegro.yaml?
A: The default value is  ${num_scalar_features} .

Q: How do I set the  scalar_embed_mlp_hidden_layers_depth  in  allegro.yaml ?
A: In  allegro.yaml , the  scalar_embed_mlp_hidden_layers_depth  parameter is located under  training_module.model.scalar_embed_mlp_hidden_layers_depth . Current default:  1 .

Q: What is the default value for  training_module.model.scalar_embed_mlp_hidden_layers_depth  in allegro.yaml?
A: The default value is  1 .

Q: How do I set the  scalar_embed_mlp_hidden_layers_width  in  allegro.yaml ?
A: In  allegro.yaml , the  scalar_embed_mlp_hidden_layers_width  parameter is located under  training_module.model.scalar_embed_mlp_hidden_layers_width . Current default:  ${num_scalar_features} .

Q: What is the default value for  training_module.model.scalar_embed_mlp_hidden_layers_width  in allegro.yaml?
A: The default value is  ${num_scalar_features} .

Q: How do I set the  scalar_embed_mlp_nonlinearity  in  allegro.yaml ?
A: In  allegro.yaml , the  scalar_embed_mlp_nonlinearity  parameter is located under  training_module.model.scalar_embed_mlp_nonlinearity . Current default:  silu .

Q: What is the default value for  training_module.model.scalar_embed_mlp_nonlinearity  in allegro.yaml?
A: The default value is  silu .

Q: How do I set the  l_max  in  allegro.yaml ?
A: In  allegro.yaml , the  l_max  parameter is located under  training_module.model.l_max . Current default:  1 .

Q: What is the default value for  training_module.model.l_max  in allegro.yaml?
A: The default value is  1 .

Q: How do I set the  num_layers  in  allegro.yaml ?
A: In  allegro.yaml , the  num_layers  parameter is located under  training_module.model.num_layers . Current default:  2 .

Q: What is the default value for  training_module.model.num_layers  in allegro.yaml?
A: The default value is  2 .

Q: How do I set the  num_scalar_features  in  allegro.yaml ?
A: In  allegro.yaml , the  num_scalar_features  parameter is located under  training_module.model.num_scalar_features . Current default:  ${num_scalar_features} .

Q: What is the default value for  training_module.model.num_scalar_features  in allegro.yaml?
A: The default value is  ${num_scalar_features} .

Q: How do I set the  num_tensor_features  in  allegro.yaml ?
A: In  allegro.yaml , the  num_tensor_features  parameter is located under  training_module.model.num_tensor_features . Current default:  32 .

Q: What is the default value for  training_module.model.num_tensor_features  in allegro.yaml?
A: The default value is  32 .

Q: How do I set the  allegro_mlp_hidden_layers_depth  in  allegro.yaml ?
A: In  allegro.yaml , the  allegro_mlp_hidden_layers_depth  parameter is located under  training_module.model.allegro_mlp_hidden_layers_depth . Current default:  1 .

Q: What is the default value for  training_module.model.allegro_mlp_hidden_layers_depth  in allegro.yaml?
A: The default value is  1 .

Q: How do I set the  allegro_mlp_hidden_layers_width  in  allegro.yaml ?
A: In  allegro.yaml , the  allegro_mlp_hidden_layers_width  parameter is located under  training_module.model.allegro_mlp_hidden_layers_width . Current default:  ${num_scalar_features} .

Q: What is the default value for  training_module.model.allegro_mlp_hidden_layers_width  in allegro.yaml?
A: The default value is  ${num_scalar_features} .

Q: How do I set the  allegro_mlp_nonlinearity  in  allegro.yaml ?
A: In  allegro.yaml , the  allegro_mlp_nonlinearity  parameter is located under  training_module.model.allegro_mlp_nonlinearity . Current default:  silu .

Q: What is the default value for  training_module.model.allegro_mlp_nonlinearity  in allegro.yaml?
A: The default value is  silu .

Q: How do I set the  parity  in  allegro.yaml ?
A: In  allegro.yaml , the  parity  parameter is located under  training_module.model.parity . Current default:  True .

Q: What is the default value for  training_module.model.parity  in allegro.yaml?
A: The default value is  True .

Q: How do I set the  tp_path_channel_coupling  in  allegro.yaml ?
A: In  allegro.yaml , the  tp_path_channel_coupling  parameter is located under  training_module.model.tp_path_channel_coupling . Current default:  True .

Q: What is the default value for  training_module.model.tp_path_channel_coupling  in allegro.yaml?
A: The default value is  True .

Q: How do I set the  readout_mlp_hidden_layers_depth  in  allegro.yaml ?
A: In  allegro.yaml , the  readout_mlp_hidden_layers_depth  parameter is located under  training_module.model.readout_mlp_hidden_layers_depth . Current default:  1 .

Q: What is the default value for  training_module.model.readout_mlp_hidden_layers_depth  in allegro.yaml?
A: The default value is  1 .

Q: How do I set the  readout_mlp_hidden_layers_width  in  allegro.yaml ?
A: In  allegro.yaml , the  readout_mlp_hidden_layers_width  parameter is located under  training_module.model.readout_mlp_hidden_layers_width . Current default:  ${num_scalar_features} .

Q: What is the default value for  training_module.model.readout_mlp_hidden_layers_width  in allegro.yaml?
A: The default value is  ${num_scalar_features} .

Q: How do I set the  readout_mlp_nonlinearity  in  allegro.yaml ?
A: In  allegro.yaml , the  readout_mlp_nonlinearity  parameter is located under  training_module.model.readout_mlp_nonlinearity . Current default:  silu .

Q: What is the default value for  training_module.model.readout_mlp_nonlinearity  in allegro.yaml?
A: The default value is  silu .

Q: How do I set the  avg_num_neighbors  in  allegro.yaml ?
A: In  allegro.yaml , the  avg_num_neighbors  parameter is located under  training_module.model.avg_num_neighbors . Current default:  ${training_data_stats:num_neighbors_mean} .

Q: What is the default value for  training_module.model.avg_num_neighbors  in allegro.yaml?
A: The default value is  ${training_data_stats:num_neighbors_mean} .

Q: How do I set the  per_type_energy_shifts  in  allegro.yaml ?
A: In  allegro.yaml , the  per_type_energy_shifts  parameter is located under  training_module.model.per_type_energy_shifts . Current default:  ${training_data_stats:per_atom_energy_mean} .

Q: What is the default value for  training_module.model.per_type_energy_shifts  in allegro.yaml?
A: The default value is  ${training_data_stats:per_atom_energy_mean} .

Q: How do I set the  per_type_energy_scales  in  allegro.yaml ?
A: In  allegro.yaml , the  per_type_energy_scales  parameter is located under  training_module.model.per_type_energy_scales . Current default:  ${training_data_stats:forces_rms} .

Q: What is the default value for  training_module.model.per_type_energy_scales  in allegro.yaml?
A: The default value is  ${training_data_stats:forces_rms} .

Q: How do I set the  per_type_energy_scales_trainable  in  allegro.yaml ?
A: In  allegro.yaml , the  per_type_energy_scales_trainable  parameter is located under  training_module.model.per_type_energy_scales_trainable . Current default:  False .

Q: What is the default value for  training_module.model.per_type_energy_scales_trainable  in allegro.yaml?
A: The default value is  False .

Q: How do I set the  per_type_energy_shifts_trainable  in  allegro.yaml ?
A: In  allegro.yaml , the  per_type_energy_shifts_trainable  parameter is located under  training_module.model.per_type_energy_shifts_trainable . Current default:  False .

Q: What is the default value for  training_module.model.per_type_energy_shifts_trainable  in allegro.yaml?
A: The default value is  False .

Q: How do I set the  _target_  in  allegro.yaml ?
A: In  allegro.yaml , the  _target_  parameter is located under  training_module.model.pair_potential._target_ . Current default:  nequip.nn.pair_potential.ZBL .

Q: What is the default value for  training_module.model.pair_potential._target_  in allegro.yaml?
A: The default value is  nequip.nn.pair_potential.ZBL .

Q: How do I set the  units  in  allegro.yaml ?
A: In  allegro.yaml , the  units  parameter is located under  training_module.model.pair_potential.units . Current default:  real .

Q: What is the default value for  training_module.model.pair_potential.units  in allegro.yaml?
A: The default value is  real .

Q: How do I set the  chemical_species  in  allegro.yaml ?
A: In  allegro.yaml , the  chemical_species  parameter is located under  training_module.model.pair_potential.chemical_species . Current default:  ${chemical_symbols} .

Q: What is the default value for  training_module.model.pair_potential.chemical_species  in allegro.yaml?
A: The default value is  ${chemical_symbols} .

Q: How do I set the  allow_tf32  in  allegro.yaml ?
A: In  allegro.yaml , the  allow_tf32  parameter is located under  global_options.allow_tf32 . Current default:  False .

Q: What is the default value for  global_options.allow_tf32  in allegro.yaml?
A: The default value is  False .

Q: Show me the full content of fusion.yaml.
A: Here is the content of  fusion.yaml :
   yaml
# General settings
general:
    seed: 42  # Random seed for reproducibility
    database_name: 'CdSe.db'  # Name of the database file

# Data handling settings
data:
    dataset_path: 
    use_last_n: 100  # current testing
# Model architecture settings
model:
    model_type: nequip_mace_interaction_fusion   # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.
    n_rbf: 40
    n_atom_basis: 192
    n_interactions: 2
    
    dropout_rate: null # Default is 0.1 but it will not work when you select only 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null # [256, 128, 64, 32] (if null then n_neurons == n_atom_basis)
    distance_unit: 'Ang'
    property_unit_dict:
      energy: 'eV'
      forces: 'eV/Ang'

# Output settings
outputs:
    energy:
      loss_weight: 0.05
      metrics: "MAE"
    forces:
      loss_weight: 0.95
      metrics: "MAE"

# Training settings
training:
    accelerator: 'gpu'
    devices: 1
    precision: 32
    batch_size: 16
    num_train: 800
    num_val: 200
    max_epochs: 3
    num_workers: 24
    pin_memory: true
    optimizer:
      type: 'AdamW'
      lr: 0.0001 #changed 0.0001 to 0.001
    scheduler:
      type: 'ReduceLROnPlateau'
      factor: 0.8
      patience: 30
      verbose: true

# Logging and checkpoint settings
logging:
    folder: './results'
    log_dir: "lightning_logs"
    checkpoint_dir: "best_inference_model"
    monitor: "val_loss"

# Testing settings
testing:
    trained_model_path: './results'  # Path to load the trained model
    csv_file_name: 'actual_vs_predicted_enrgforc.csv'  # Path to save the predictions CSV

# Resume training settings
resume_training:
    resume_checkpoint_dir: null # Path to model checkpoint file if required 

# Fine Tuning settings
fine_tuning:
    pretrained_checkpoint: # checkpoint file path
    freeze_embedding: true
    freeze_interactions_up_to: 2 # add no of layers
    freeze_all_representation: true
    lr: 0.00005
    early_stopping_patience: 10
    best_model_dir: fine_tuned_best_model  # Subdirectory only
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory only
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
   

Q: How do I set the  seed  in  fusion.yaml ?
A: In  fusion.yaml , the  seed  parameter is located under  general.seed . Current default:  42 .

Q: What is the default value for  general.seed  in fusion.yaml?
A: The default value is  42 .

Q: How do I set the  database_name  in  fusion.yaml ?
A: In  fusion.yaml , the  database_name  parameter is located under  general.database_name . Current default:  CdSe.db .

Q: What is the default value for  general.database_name  in fusion.yaml?
A: The default value is  CdSe.db .

Q: How do I set the  dataset_path  in  fusion.yaml ?
A: In  fusion.yaml , the  dataset_path  parameter is located under  data.dataset_path . Current default:  None .

Q: What is the default value for  data.dataset_path  in fusion.yaml?
A: The default value is  None .

Q: How do I set the  use_last_n  in  fusion.yaml ?
A: In  fusion.yaml , the  use_last_n  parameter is located under  data.use_last_n . Current default:  100 .

Q: What is the default value for  data.use_last_n  in fusion.yaml?
A: The default value is  100 .

Q: How do I set the  model_type  in  fusion.yaml ?
A: In  fusion.yaml , the  model_type  parameter is located under  model.model_type . Current default:  nequip_mace_interaction_fusion .

Q: What is the default value for  model.model_type  in fusion.yaml?
A: The default value is  nequip_mace_interaction_fusion .

Q: How do I set the  cutoff  in  fusion.yaml ?
A: In  fusion.yaml , the  cutoff  parameter is located under  model.cutoff . Current default:  12.0 .

Q: What is the default value for  model.cutoff  in fusion.yaml?
A: The default value is  12.0 .

Q: How do I set the  n_rbf  in  fusion.yaml ?
A: In  fusion.yaml , the  n_rbf  parameter is located under  model.n_rbf . Current default:  40 .

Q: What is the default value for  model.n_rbf  in fusion.yaml?
A: The default value is  40 .

Q: How do I set the  n_atom_basis  in  fusion.yaml ?
A: In  fusion.yaml , the  n_atom_basis  parameter is located under  model.n_atom_basis . Current default:  192 .

Q: What is the default value for  model.n_atom_basis  in fusion.yaml?
A: The default value is  192 .

Q: How do I set the  n_interactions  in  fusion.yaml ?
A: In  fusion.yaml , the  n_interactions  parameter is located under  model.n_interactions . Current default:  2 .

Q: What is the default value for  model.n_interactions  in fusion.yaml?
A: The default value is  2 .

Q: How do I set the  dropout_rate  in  fusion.yaml ?
A: In  fusion.yaml , the  dropout_rate  parameter is located under  model.dropout_rate . Current default:  None .

Q: What is the default value for  model.dropout_rate  in fusion.yaml?
A: The default value is  None .

Q: How do I set the  n_layers  in  fusion.yaml ?
A: In  fusion.yaml , the  n_layers  parameter is located under  model.n_layers . Current default:  1 .

Q: What is the default value for  model.n_layers  in fusion.yaml?
A: The default value is  1 .

Q: How do I set the  n_neurons  in  fusion.yaml ?
A: In  fusion.yaml , the  n_neurons  parameter is located under  model.n_neurons . Current default:  None .

Q: What is the default value for  model.n_neurons  in fusion.yaml?
A: The default value is  None .

Q: How do I set the  distance_unit  in  fusion.yaml ?
A: In  fusion.yaml , the  distance_unit  parameter is located under  model.distance_unit . Current default:  Ang .

Q: What is the default value for  model.distance_unit  in fusion.yaml?
A: The default value is  Ang .

Q: How do I set the  energy  in  fusion.yaml ?
A: In  fusion.yaml , the  energy  parameter is located under  model.property_unit_dict.energy . Current default:  eV .

Q: What is the default value for  model.property_unit_dict.energy  in fusion.yaml?
A: The default value is  eV .

Q: How do I set the  forces  in  fusion.yaml ?
A: In  fusion.yaml , the  forces  parameter is located under  model.property_unit_dict.forces . Current default:  eV/Ang .

Q: What is the default value for  model.property_unit_dict.forces  in fusion.yaml?
A: The default value is  eV/Ang .

Q: How do I set the  loss_weight  in  fusion.yaml ?
A: In  fusion.yaml , the  loss_weight  parameter is located under  outputs.energy.loss_weight . Current default:  0.05 .

Q: What is the default value for  outputs.energy.loss_weight  in fusion.yaml?
A: The default value is  0.05 .

Q: How do I set the  metrics  in  fusion.yaml ?
A: In  fusion.yaml , the  metrics  parameter is located under  outputs.energy.metrics . Current default:  MAE .

Q: What is the default value for  outputs.energy.metrics  in fusion.yaml?
A: The default value is  MAE .

Q: How do I set the  loss_weight  in  fusion.yaml ?
A: In  fusion.yaml , the  loss_weight  parameter is located under  outputs.forces.loss_weight . Current default:  0.95 .

Q: What is the default value for  outputs.forces.loss_weight  in fusion.yaml?
A: The default value is  0.95 .

Q: How do I set the  metrics  in  fusion.yaml ?
A: In  fusion.yaml , the  metrics  parameter is located under  outputs.forces.metrics . Current default:  MAE .

Q: What is the default value for  outputs.forces.metrics  in fusion.yaml?
A: The default value is  MAE .

Q: How do I set the  accelerator  in  fusion.yaml ?
A: In  fusion.yaml , the  accelerator  parameter is located under  training.accelerator . Current default:  gpu .

Q: What is the default value for  training.accelerator  in fusion.yaml?
A: The default value is  gpu .

Q: How do I set the  devices  in  fusion.yaml ?
A: In  fusion.yaml , the  devices  parameter is located under  training.devices . Current default:  1 .

Q: What is the default value for  training.devices  in fusion.yaml?
A: The default value is  1 .

Q: How do I set the  precision  in  fusion.yaml ?
A: In  fusion.yaml , the  precision  parameter is located under  training.precision . Current default:  32 .

Q: What is the default value for  training.precision  in fusion.yaml?
A: The default value is  32 .

Q: How do I set the  batch_size  in  fusion.yaml ?
A: In  fusion.yaml , the  batch_size  parameter is located under  training.batch_size . Current default:  16 .

Q: What is the default value for  training.batch_size  in fusion.yaml?
A: The default value is  16 .

Q: How do I set the  num_train  in  fusion.yaml ?
A: In  fusion.yaml , the  num_train  parameter is located under  training.num_train . Current default:  800 .

Q: What is the default value for  training.num_train  in fusion.yaml?
A: The default value is  800 .

Q: How do I set the  num_val  in  fusion.yaml ?
A: In  fusion.yaml , the  num_val  parameter is located under  training.num_val . Current default:  200 .

Q: What is the default value for  training.num_val  in fusion.yaml?
A: The default value is  200 .

Q: How do I set the  max_epochs  in  fusion.yaml ?
A: In  fusion.yaml , the  max_epochs  parameter is located under  training.max_epochs . Current default:  3 .

Q: What is the default value for  training.max_epochs  in fusion.yaml?
A: The default value is  3 .

Q: How do I set the  num_workers  in  fusion.yaml ?
A: In  fusion.yaml , the  num_workers  parameter is located under  training.num_workers . Current default:  24 .

Q: What is the default value for  training.num_workers  in fusion.yaml?
A: The default value is  24 .

Q: How do I set the  pin_memory  in  fusion.yaml ?
A: In  fusion.yaml , the  pin_memory  parameter is located under  training.pin_memory . Current default:  True .

Q: What is the default value for  training.pin_memory  in fusion.yaml?
A: The default value is  True .

Q: How do I set the  type  in  fusion.yaml ?
A: In  fusion.yaml , the  type  parameter is located under  training.optimizer.type . Current default:  AdamW .

Q: What is the default value for  training.optimizer.type  in fusion.yaml?
A: The default value is  AdamW .

Q: How do I set the  lr  in  fusion.yaml ?
A: In  fusion.yaml , the  lr  parameter is located under  training.optimizer.lr . Current default:  0.0001 .

Q: What is the default value for  training.optimizer.lr  in fusion.yaml?
A: The default value is  0.0001 .

Q: How do I set the  type  in  fusion.yaml ?
A: In  fusion.yaml , the  type  parameter is located under  training.scheduler.type . Current default:  ReduceLROnPlateau .

Q: What is the default value for  training.scheduler.type  in fusion.yaml?
A: The default value is  ReduceLROnPlateau .

Q: How do I set the  factor  in  fusion.yaml ?
A: In  fusion.yaml , the  factor  parameter is located under  training.scheduler.factor . Current default:  0.8 .

Q: What is the default value for  training.scheduler.factor  in fusion.yaml?
A: The default value is  0.8 .

Q: How do I set the  patience  in  fusion.yaml ?
A: In  fusion.yaml , the  patience  parameter is located under  training.scheduler.patience . Current default:  30 .

Q: What is the default value for  training.scheduler.patience  in fusion.yaml?
A: The default value is  30 .

Q: How do I set the  verbose  in  fusion.yaml ?
A: In  fusion.yaml , the  verbose  parameter is located under  training.scheduler.verbose . Current default:  True .

Q: What is the default value for  training.scheduler.verbose  in fusion.yaml?
A: The default value is  True .

Q: How do I set the  folder  in  fusion.yaml ?
A: In  fusion.yaml , the  folder  parameter is located under  logging.folder . Current default:  ./results .

Q: What is the default value for  logging.folder  in fusion.yaml?
A: The default value is  ./results .

Q: How do I set the  log_dir  in  fusion.yaml ?
A: In  fusion.yaml , the  log_dir  parameter is located under  logging.log_dir . Current default:  lightning_logs .

Q: What is the default value for  logging.log_dir  in fusion.yaml?
A: The default value is  lightning_logs .

Q: How do I set the  checkpoint_dir  in  fusion.yaml ?
A: In  fusion.yaml , the  checkpoint_dir  parameter is located under  logging.checkpoint_dir . Current default:  best_inference_model .

Q: What is the default value for  logging.checkpoint_dir  in fusion.yaml?
A: The default value is  best_inference_model .

Q: How do I set the  monitor  in  fusion.yaml ?
A: In  fusion.yaml , the  monitor  parameter is located under  logging.monitor . Current default:  val_loss .

Q: What is the default value for  logging.monitor  in fusion.yaml?
A: The default value is  val_loss .

Q: How do I set the  trained_model_path  in  fusion.yaml ?
A: In  fusion.yaml , the  trained_model_path  parameter is located under  testing.trained_model_path . Current default:  ./results .

Q: What is the default value for  testing.trained_model_path  in fusion.yaml?
A: The default value is  ./results .

Q: How do I set the  csv_file_name  in  fusion.yaml ?
A: In  fusion.yaml , the  csv_file_name  parameter is located under  testing.csv_file_name . Current default:  actual_vs_predicted_enrgforc.csv .

Q: What is the default value for  testing.csv_file_name  in fusion.yaml?
A: The default value is  actual_vs_predicted_enrgforc.csv .

Q: How do I set the  resume_checkpoint_dir  in  fusion.yaml ?
A: In  fusion.yaml , the  resume_checkpoint_dir  parameter is located under  resume_training.resume_checkpoint_dir . Current default:  None .

Q: What is the default value for  resume_training.resume_checkpoint_dir  in fusion.yaml?
A: The default value is  None .

Q: How do I set the  pretrained_checkpoint  in  fusion.yaml ?
A: In  fusion.yaml , the  pretrained_checkpoint  parameter is located under  fine_tuning.pretrained_checkpoint . Current default:  None .

Q: What is the default value for  fine_tuning.pretrained_checkpoint  in fusion.yaml?
A: The default value is  None .

Q: How do I set the  freeze_embedding  in  fusion.yaml ?
A: In  fusion.yaml , the  freeze_embedding  parameter is located under  fine_tuning.freeze_embedding . Current default:  True .

Q: What is the default value for  fine_tuning.freeze_embedding  in fusion.yaml?
A: The default value is  True .

Q: How do I set the  freeze_interactions_up_to  in  fusion.yaml ?
A: In  fusion.yaml , the  freeze_interactions_up_to  parameter is located under  fine_tuning.freeze_interactions_up_to . Current default:  2 .

Q: What is the default value for  fine_tuning.freeze_interactions_up_to  in fusion.yaml?
A: The default value is  2 .

Q: How do I set the  freeze_all_representation  in  fusion.yaml ?
A: In  fusion.yaml , the  freeze_all_representation  parameter is located under  fine_tuning.freeze_all_representation . Current default:  True .

Q: What is the default value for  fine_tuning.freeze_all_representation  in fusion.yaml?
A: The default value is  True .

Q: How do I set the  lr  in  fusion.yaml ?
A: In  fusion.yaml , the  lr  parameter is located under  fine_tuning.lr . Current default:  5e-05 .

Q: What is the default value for  fine_tuning.lr  in fusion.yaml?
A: The default value is  5e-05 .

Q: How do I set the  early_stopping_patience  in  fusion.yaml ?
A: In  fusion.yaml , the  early_stopping_patience  parameter is located under  fine_tuning.early_stopping_patience . Current default:  10 .

Q: What is the default value for  fine_tuning.early_stopping_patience  in fusion.yaml?
A: The default value is  10 .

Q: How do I set the  best_model_dir  in  fusion.yaml ?
A: In  fusion.yaml , the  best_model_dir  parameter is located under  fine_tuning.best_model_dir . Current default:  fine_tuned_best_model .

Q: What is the default value for  fine_tuning.best_model_dir  in fusion.yaml?
A: The default value is  fine_tuned_best_model .

Q: How do I set the  checkpoint_dir  in  fusion.yaml ?
A: In  fusion.yaml , the  checkpoint_dir  parameter is located under  fine_tuning.checkpoint_dir . Current default:  fine_tuned_checkpoints .

Q: What is the default value for  fine_tuning.checkpoint_dir  in fusion.yaml?
A: The default value is  fine_tuned_checkpoints .

Q: How do I set the  log_name  in  fusion.yaml ?
A: In  fusion.yaml , the  log_name  parameter is located under  fine_tuning.log_name . Current default:  fine_tune_logs .

Q: What is the default value for  fine_tuning.log_name  in fusion.yaml?
A: The default value is  fine_tune_logs .

Q: Show me the full content of mace.yaml.
A: Here is the content of  mace.yaml :
   yaml
# ===========================
# Experiment & Paths
# ===========================
name: mace_cdsecl_model
seed: 42
log_level: INFO
error_table: PerAtomMAE   # Report validation metrics using MAE

# ===========================
# Hardware & Precision
# ===========================
device: cuda              # Options: cpu, cuda, mps, xpu
default_dtype: float32
distributed: false    # True for multiple GPU  keys: true, false
# ===========================
# Dataset & Keys
# ===========================
train_file:   #consolidate-cdse35_1000.xyz consolidated_dataset_1000_CdSe_new.xyz
valid_file: null
test_file: null

energy_key: energy  #REF_energy
forces_key: forces  #REF_forces
stress_key: null

valid_fraction: 0.2
batch_size: 8
num_workers: 24
pin_memory: true        # Enables faster CPU → GPU transfer

# ===========================
# Model Configuration
# ===========================
model: MACE
r_max: 12                  #cutoff
num_radial_basis: 20           #n-rbf
num_cutoff_basis: 6
max_ell: 3
num_channels: 64             #n_atom_basis
max_L: 2
num_interactions: 3
correlation: 3
avg_num_neighbors: 100.80

# ===========================
# Training Parameters
# ===========================
max_num_epochs: 3
ema: true
ema_decay: 0.99

# ===========================
# Validation & Early Stopping
# ===========================
valid_batch_size: 16         # Match GPU capacity
eval_interval: 1           # Check validation every 1 epochs
patience: 30                # Early stop if no val improvement in 30 checks

# ===========================
# Stochastic Weight Averaging
# ===========================
swa: true
start_swa: 400
swa_energy_weight: 1.0
swa_forces_weight: 100.0

# ===========================
# Loss Weights
# ===========================
forces_weight: 0.95
energy_weight: 0.05

# ===========================
# Optimizer & Scheduler
# ===========================
optimizer: adam
lr: 0.001
weight_decay: 1e-5

scheduler: ReduceLROnPlateau
lr_factor: 0.8
scheduler_patience: 5
lr_scheduler_gamma: 0.9993

# ===========================
# Energy Baseline & Scaling
# ===========================
E0s: "average"                         # Use average per-atom energy (like --E0s=average)
scaling: rms_forces_scaling
compute_avg_num_neighbors: true

   

Q: How do I set the  name  in  mace.yaml ?
A: In  mace.yaml , the  name  parameter is located under  name . Current default:  mace_cdsecl_model .

Q: What is the default value for  name  in mace.yaml?
A: The default value is  mace_cdsecl_model .

Q: How do I set the  seed  in  mace.yaml ?
A: In  mace.yaml , the  seed  parameter is located under  seed . Current default:  42 .

Q: What is the default value for  seed  in mace.yaml?
A: The default value is  42 .

Q: How do I set the  log_level  in  mace.yaml ?
A: In  mace.yaml , the  log_level  parameter is located under  log_level . Current default:  INFO .

Q: What is the default value for  log_level  in mace.yaml?
A: The default value is  INFO .

Q: How do I set the  error_table  in  mace.yaml ?
A: In  mace.yaml , the  error_table  parameter is located under  error_table . Current default:  PerAtomMAE .

Q: What is the default value for  error_table  in mace.yaml?
A: The default value is  PerAtomMAE .

Q: How do I set the  device  in  mace.yaml ?
A: In  mace.yaml , the  device  parameter is located under  device . Current default:  cuda .

Q: What is the default value for  device  in mace.yaml?
A: The default value is  cuda .

Q: How do I set the  default_dtype  in  mace.yaml ?
A: In  mace.yaml , the  default_dtype  parameter is located under  default_dtype . Current default:  float32 .

Q: What is the default value for  default_dtype  in mace.yaml?
A: The default value is  float32 .

Q: How do I set the  distributed  in  mace.yaml ?
A: In  mace.yaml , the  distributed  parameter is located under  distributed . Current default:  False .

Q: What is the default value for  distributed  in mace.yaml?
A: The default value is  False .

Q: How do I set the  train_file  in  mace.yaml ?
A: In  mace.yaml , the  train_file  parameter is located under  train_file . Current default:  None .

Q: What is the default value for  train_file  in mace.yaml?
A: The default value is  None .

Q: How do I set the  valid_file  in  mace.yaml ?
A: In  mace.yaml , the  valid_file  parameter is located under  valid_file . Current default:  None .

Q: What is the default value for  valid_file  in mace.yaml?
A: The default value is  None .

Q: How do I set the  test_file  in  mace.yaml ?
A: In  mace.yaml , the  test_file  parameter is located under  test_file . Current default:  None .

Q: What is the default value for  test_file  in mace.yaml?
A: The default value is  None .

Q: How do I set the  energy_key  in  mace.yaml ?
A: In  mace.yaml , the  energy_key  parameter is located under  energy_key . Current default:  energy .

Q: What is the default value for  energy_key  in mace.yaml?
A: The default value is  energy .

Q: How do I set the  forces_key  in  mace.yaml ?
A: In  mace.yaml , the  forces_key  parameter is located under  forces_key . Current default:  forces .

Q: What is the default value for  forces_key  in mace.yaml?
A: The default value is  forces .

Q: How do I set the  stress_key  in  mace.yaml ?
A: In  mace.yaml , the  stress_key  parameter is located under  stress_key . Current default:  None .

Q: What is the default value for  stress_key  in mace.yaml?
A: The default value is  None .

Q: How do I set the  valid_fraction  in  mace.yaml ?
A: In  mace.yaml , the  valid_fraction  parameter is located under  valid_fraction . Current default:  0.2 .

Q: What is the default value for  valid_fraction  in mace.yaml?
A: The default value is  0.2 .

Q: How do I set the  batch_size  in  mace.yaml ?
A: In  mace.yaml , the  batch_size  parameter is located under  batch_size . Current default:  8 .

Q: What is the default value for  batch_size  in mace.yaml?
A: The default value is  8 .

Q: How do I set the  num_workers  in  mace.yaml ?
A: In  mace.yaml , the  num_workers  parameter is located under  num_workers . Current default:  24 .

Q: What is the default value for  num_workers  in mace.yaml?
A: The default value is  24 .

Q: How do I set the  pin_memory  in  mace.yaml ?
A: In  mace.yaml , the  pin_memory  parameter is located under  pin_memory . Current default:  True .

Q: What is the default value for  pin_memory  in mace.yaml?
A: The default value is  True .

Q: How do I set the  model  in  mace.yaml ?
A: In  mace.yaml , the  model  parameter is located under  model . Current default:  MACE .

Q: What is the default value for  model  in mace.yaml?
A: The default value is  MACE .

Q: How do I set the  r_max  in  mace.yaml ?
A: In  mace.yaml , the  r_max  parameter is located under  r_max . Current default:  12 .

Q: What is the default value for  r_max  in mace.yaml?
A: The default value is  12 .

Q: How do I set the  num_radial_basis  in  mace.yaml ?
A: In  mace.yaml , the  num_radial_basis  parameter is located under  num_radial_basis . Current default:  20 .

Q: What is the default value for  num_radial_basis  in mace.yaml?
A: The default value is  20 .

Q: How do I set the  num_cutoff_basis  in  mace.yaml ?
A: In  mace.yaml , the  num_cutoff_basis  parameter is located under  num_cutoff_basis . Current default:  6 .

Q: What is the default value for  num_cutoff_basis  in mace.yaml?
A: The default value is  6 .

Q: How do I set the  max_ell  in  mace.yaml ?
A: In  mace.yaml , the  max_ell  parameter is located under  max_ell . Current default:  3 .

Q: What is the default value for  max_ell  in mace.yaml?
A: The default value is  3 .

Q: How do I set the  num_channels  in  mace.yaml ?
A: In  mace.yaml , the  num_channels  parameter is located under  num_channels . Current default:  64 .

Q: What is the default value for  num_channels  in mace.yaml?
A: The default value is  64 .

Q: How do I set the  max_L  in  mace.yaml ?
A: In  mace.yaml , the  max_L  parameter is located under  max_L . Current default:  2 .

Q: What is the default value for  max_L  in mace.yaml?
A: The default value is  2 .

Q: How do I set the  num_interactions  in  mace.yaml ?
A: In  mace.yaml , the  num_interactions  parameter is located under  num_interactions . Current default:  3 .

Q: What is the default value for  num_interactions  in mace.yaml?
A: The default value is  3 .

Q: How do I set the  correlation  in  mace.yaml ?
A: In  mace.yaml , the  correlation  parameter is located under  correlation . Current default:  3 .

Q: What is the default value for  correlation  in mace.yaml?
A: The default value is  3 .

Q: How do I set the  avg_num_neighbors  in  mace.yaml ?
A: In  mace.yaml , the  avg_num_neighbors  parameter is located under  avg_num_neighbors . Current default:  100.8 .

Q: What is the default value for  avg_num_neighbors  in mace.yaml?
A: The default value is  100.8 .

Q: How do I set the  max_num_epochs  in  mace.yaml ?
A: In  mace.yaml , the  max_num_epochs  parameter is located under  max_num_epochs . Current default:  3 .

Q: What is the default value for  max_num_epochs  in mace.yaml?
A: The default value is  3 .

Q: How do I set the  ema  in  mace.yaml ?
A: In  mace.yaml , the  ema  parameter is located under  ema . Current default:  True .

Q: What is the default value for  ema  in mace.yaml?
A: The default value is  True .

Q: How do I set the  ema_decay  in  mace.yaml ?
A: In  mace.yaml , the  ema_decay  parameter is located under  ema_decay . Current default:  0.99 .

Q: What is the default value for  ema_decay  in mace.yaml?
A: The default value is  0.99 .

Q: How do I set the  valid_batch_size  in  mace.yaml ?
A: In  mace.yaml , the  valid_batch_size  parameter is located under  valid_batch_size . Current default:  16 .

Q: What is the default value for  valid_batch_size  in mace.yaml?
A: The default value is  16 .

Q: How do I set the  eval_interval  in  mace.yaml ?
A: In  mace.yaml , the  eval_interval  parameter is located under  eval_interval . Current default:  1 .

Q: What is the default value for  eval_interval  in mace.yaml?
A: The default value is  1 .

Q: How do I set the  patience  in  mace.yaml ?
A: In  mace.yaml , the  patience  parameter is located under  patience . Current default:  30 .

Q: What is the default value for  patience  in mace.yaml?
A: The default value is  30 .

Q: How do I set the  swa  in  mace.yaml ?
A: In  mace.yaml , the  swa  parameter is located under  swa . Current default:  True .

Q: What is the default value for  swa  in mace.yaml?
A: The default value is  True .

Q: How do I set the  start_swa  in  mace.yaml ?
A: In  mace.yaml , the  start_swa  parameter is located under  start_swa . Current default:  400 .

Q: What is the default value for  start_swa  in mace.yaml?
A: The default value is  400 .

Q: How do I set the  swa_energy_weight  in  mace.yaml ?
A: In  mace.yaml , the  swa_energy_weight  parameter is located under  swa_energy_weight . Current default:  1.0 .

Q: What is the default value for  swa_energy_weight  in mace.yaml?
A: The default value is  1.0 .

Q: How do I set the  swa_forces_weight  in  mace.yaml ?
A: In  mace.yaml , the  swa_forces_weight  parameter is located under  swa_forces_weight . Current default:  100.0 .

Q: What is the default value for  swa_forces_weight  in mace.yaml?
A: The default value is  100.0 .

Q: How do I set the  forces_weight  in  mace.yaml ?
A: In  mace.yaml , the  forces_weight  parameter is located under  forces_weight . Current default:  0.95 .

Q: What is the default value for  forces_weight  in mace.yaml?
A: The default value is  0.95 .

Q: How do I set the  energy_weight  in  mace.yaml ?
A: In  mace.yaml , the  energy_weight  parameter is located under  energy_weight . Current default:  0.05 .

Q: What is the default value for  energy_weight  in mace.yaml?
A: The default value is  0.05 .

Q: How do I set the  optimizer  in  mace.yaml ?
A: In  mace.yaml , the  optimizer  parameter is located under  optimizer . Current default:  adam .

Q: What is the default value for  optimizer  in mace.yaml?
A: The default value is  adam .

Q: How do I set the  lr  in  mace.yaml ?
A: In  mace.yaml , the  lr  parameter is located under  lr . Current default:  0.001 .

Q: What is the default value for  lr  in mace.yaml?
A: The default value is  0.001 .

Q: How do I set the  weight_decay  in  mace.yaml ?
A: In  mace.yaml , the  weight_decay  parameter is located under  weight_decay . Current default:  1e-5 .

Q: What is the default value for  weight_decay  in mace.yaml?
A: The default value is  1e-5 .

Q: How do I set the  scheduler  in  mace.yaml ?
A: In  mace.yaml , the  scheduler  parameter is located under  scheduler . Current default:  ReduceLROnPlateau .

Q: What is the default value for  scheduler  in mace.yaml?
A: The default value is  ReduceLROnPlateau .

Q: How do I set the  lr_factor  in  mace.yaml ?
A: In  mace.yaml , the  lr_factor  parameter is located under  lr_factor . Current default:  0.8 .

Q: What is the default value for  lr_factor  in mace.yaml?
A: The default value is  0.8 .

Q: How do I set the  scheduler_patience  in  mace.yaml ?
A: In  mace.yaml , the  scheduler_patience  parameter is located under  scheduler_patience . Current default:  5 .

Q: What is the default value for  scheduler_patience  in mace.yaml?
A: The default value is  5 .

Q: How do I set the  lr_scheduler_gamma  in  mace.yaml ?
A: In  mace.yaml , the  lr_scheduler_gamma  parameter is located under  lr_scheduler_gamma . Current default:  0.9993 .

Q: What is the default value for  lr_scheduler_gamma  in mace.yaml?
A: The default value is  0.9993 .

Q: How do I set the  E0s  in  mace.yaml ?
A: In  mace.yaml , the  E0s  parameter is located under  E0s . Current default:  average .

Q: What is the default value for  E0s  in mace.yaml?
A: The default value is  average .

Q: How do I set the  scaling  in  mace.yaml ?
A: In  mace.yaml , the  scaling  parameter is located under  scaling . Current default:  rms_forces_scaling .

Q: What is the default value for  scaling  in mace.yaml?
A: The default value is  rms_forces_scaling .

Q: How do I set the  compute_avg_num_neighbors  in  mace.yaml ?
A: In  mace.yaml , the  compute_avg_num_neighbors  parameter is located under  compute_avg_num_neighbors . Current default:  True .

Q: What is the default value for  compute_avg_num_neighbors  in mace.yaml?
A: The default value is  True .

Q: Show me the full content of nequip.yaml.
A: Here is the content of  nequip.yaml :
   yaml
run: [train, val, test]

cutoff_radius: 12.0

chemical_symbols: [Cd, Cl, Se] 
model_type_names: ${chemical_symbols}

data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 456             # dataset seed for reproducibility
  
  split_dataset:
    file_path:   #./basic_consolidated_dataset_1000CdSe.xyz
    train: 0.8
    val: 0.1
    test: 0.1

  transforms:
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}

    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}

  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
    num_workers: 5
    shuffle: true
  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
    num_workers: ${data.train_dataloader.num_workers}  # we want to use the same num_workers -- variable interpolation helps
  test_dataloader: ${data.val_dataloader}  # variable interpolation comes in handy again

  stats_manager:
    _target_: nequip.data.CommonDataStatisticsManager

    dataloader_kwargs:
      batch_size: 16

    type_names: ${model_type_names}

trainer:
  _target_: lightning.Trainer
  accelerator: auto
  devices: 1    # NO of GPUs
  enable_checkpointing: true
  max_epochs: 3
  max_time: 03:00:00:00
  check_val_every_n_epoch: 1  # how often to validate
  log_every_n_steps: 1       # how often to log

  logger:
    - _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ./logs
      name: tutorial_log
      version: null
      default_hp_metric: false

  callbacks:
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      min_delta: 1e-3                         # how much to be considered a "change"
      patience: 20                            # how many instances of "no change" before stopping

    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      dirpath: ./results    
      filename: best                          # best.ckpt is the checkpoint name
      save_last: true                         # last.ckpt will be saved
      
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: epoch

training_module:
  _target_: nequip.train.EMALightningModule

  ema_decay: 0.999

  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 0.05
      forces: 0.95

  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      total_energy_mae: 1.0
      forces_mae: 1.0
      # keys  total_energy_rmse  and  forces_rmse ,  per_atom_energy_rmse  and  per_atom_energy_mae  are also available

  train_metrics: ${training_module.val_metrics}  # use variable interpolation
  test_metrics: ${training_module.val_metrics}  # use variable interpolation

  optimizer:
    _target_: torch.optim.Adam
    lr: 0.03

  lr_scheduler:
    scheduler:
      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
      factor: 0.6
      patience: 5
      threshold: 0.2
      min_lr: 1e-6
    monitor: val0_epoch/weighted_sum
    interval: epoch
    frequency: 1

  model:
    _target_: nequip.model.NequIPGNNModel

    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    num_bessels: 8                # number of basis functions used in the radial Bessel basis, the default of 8 usually works well
    bessel_trainable: false       # set true to train the bessel weights (default false)
    polynomial_cutoff_p: 6        # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance

    num_layers: 3       # number of interaction blocks, we find 3-5 to work best
    l_max: 1            # the maximum irrep order (rotation order) for the network's features, l=1 is a good default, l=2 is more accurate but slower
    parity: true        # whether to include features with odd mirror parity; often turning parity off gives equally good results but faster networks, so do consider this
    num_features: 32    # the multiplicity of the features, 32 is a good default for accurate network, if you want to be more accurate, go larger, if you want to be faster, go lower

    radial_mlp_depth: 2         # number of radial layers, usually 1-3 works best, smaller is faster
    radial_mlp_width: 64        # number of hidden neurons in radial function, smaller is faster

    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}

    per_type_energy_scales: ${training_data_stats:per_type_forces_rms}
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: metal     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types

global_options:
  allow_tf32: false

   

Q: How do I set the  run  in  nequip.yaml ?
A: In  nequip.yaml , the  run  parameter is located under  run . Current default:  ['train', 'val', 'test'] .

Q: What is the default value for  run  in nequip.yaml?
A: The default value is  ['train', 'val', 'test'] .

Q: How do I set the  cutoff_radius  in  nequip.yaml ?
A: In  nequip.yaml , the  cutoff_radius  parameter is located under  cutoff_radius . Current default:  12.0 .

Q: What is the default value for  cutoff_radius  in nequip.yaml?
A: The default value is  12.0 .

Q: How do I set the  chemical_symbols  in  nequip.yaml ?
A: In  nequip.yaml , the  chemical_symbols  parameter is located under  chemical_symbols . Current default:  ['Cd', 'Cl', 'Se'] .

Q: What is the default value for  chemical_symbols  in nequip.yaml?
A: The default value is  ['Cd', 'Cl', 'Se'] .

Q: How do I set the  model_type_names  in  nequip.yaml ?
A: In  nequip.yaml , the  model_type_names  parameter is located under  model_type_names . Current default:  ${chemical_symbols} .

Q: What is the default value for  model_type_names  in nequip.yaml?
A: The default value is  ${chemical_symbols} .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  data._target_ . Current default:  nequip.data.datamodule.ASEDataModule .

Q: What is the default value for  data._target_  in nequip.yaml?
A: The default value is  nequip.data.datamodule.ASEDataModule .

Q: How do I set the  seed  in  nequip.yaml ?
A: In  nequip.yaml , the  seed  parameter is located under  data.seed . Current default:  456 .

Q: What is the default value for  data.seed  in nequip.yaml?
A: The default value is  456 .

Q: How do I set the  file_path  in  nequip.yaml ?
A: In  nequip.yaml , the  file_path  parameter is located under  data.split_dataset.file_path . Current default:  None .

Q: What is the default value for  data.split_dataset.file_path  in nequip.yaml?
A: The default value is  None .

Q: How do I set the  train  in  nequip.yaml ?
A: In  nequip.yaml , the  train  parameter is located under  data.split_dataset.train . Current default:  0.8 .

Q: What is the default value for  data.split_dataset.train  in nequip.yaml?
A: The default value is  0.8 .

Q: How do I set the  val  in  nequip.yaml ?
A: In  nequip.yaml , the  val  parameter is located under  data.split_dataset.val . Current default:  0.1 .

Q: What is the default value for  data.split_dataset.val  in nequip.yaml?
A: The default value is  0.1 .

Q: How do I set the  test  in  nequip.yaml ?
A: In  nequip.yaml , the  test  parameter is located under  data.split_dataset.test . Current default:  0.1 .

Q: What is the default value for  data.split_dataset.test  in nequip.yaml?
A: The default value is  0.1 .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  data.train_dataloader._target_ . Current default:  torch.utils.data.DataLoader .

Q: What is the default value for  data.train_dataloader._target_  in nequip.yaml?
A: The default value is  torch.utils.data.DataLoader .

Q: How do I set the  batch_size  in  nequip.yaml ?
A: In  nequip.yaml , the  batch_size  parameter is located under  data.train_dataloader.batch_size . Current default:  16 .

Q: What is the default value for  data.train_dataloader.batch_size  in nequip.yaml?
A: The default value is  16 .

Q: How do I set the  num_workers  in  nequip.yaml ?
A: In  nequip.yaml , the  num_workers  parameter is located under  data.train_dataloader.num_workers . Current default:  5 .

Q: What is the default value for  data.train_dataloader.num_workers  in nequip.yaml?
A: The default value is  5 .

Q: How do I set the  shuffle  in  nequip.yaml ?
A: In  nequip.yaml , the  shuffle  parameter is located under  data.train_dataloader.shuffle . Current default:  True .

Q: What is the default value for  data.train_dataloader.shuffle  in nequip.yaml?
A: The default value is  True .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  data.val_dataloader._target_ . Current default:  torch.utils.data.DataLoader .

Q: What is the default value for  data.val_dataloader._target_  in nequip.yaml?
A: The default value is  torch.utils.data.DataLoader .

Q: How do I set the  batch_size  in  nequip.yaml ?
A: In  nequip.yaml , the  batch_size  parameter is located under  data.val_dataloader.batch_size . Current default:  16 .

Q: What is the default value for  data.val_dataloader.batch_size  in nequip.yaml?
A: The default value is  16 .

Q: How do I set the  num_workers  in  nequip.yaml ?
A: In  nequip.yaml , the  num_workers  parameter is located under  data.val_dataloader.num_workers . Current default:  ${data.train_dataloader.num_workers} .

Q: What is the default value for  data.val_dataloader.num_workers  in nequip.yaml?
A: The default value is  ${data.train_dataloader.num_workers} .

Q: How do I set the  test_dataloader  in  nequip.yaml ?
A: In  nequip.yaml , the  test_dataloader  parameter is located under  data.test_dataloader . Current default:  ${data.val_dataloader} .

Q: What is the default value for  data.test_dataloader  in nequip.yaml?
A: The default value is  ${data.val_dataloader} .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  data.stats_manager._target_ . Current default:  nequip.data.CommonDataStatisticsManager .

Q: What is the default value for  data.stats_manager._target_  in nequip.yaml?
A: The default value is  nequip.data.CommonDataStatisticsManager .

Q: How do I set the  batch_size  in  nequip.yaml ?
A: In  nequip.yaml , the  batch_size  parameter is located under  data.stats_manager.dataloader_kwargs.batch_size . Current default:  16 .

Q: What is the default value for  data.stats_manager.dataloader_kwargs.batch_size  in nequip.yaml?
A: The default value is  16 .

Q: How do I set the  type_names  in  nequip.yaml ?
A: In  nequip.yaml , the  type_names  parameter is located under  data.stats_manager.type_names . Current default:  ${model_type_names} .

Q: What is the default value for  data.stats_manager.type_names  in nequip.yaml?
A: The default value is  ${model_type_names} .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  trainer._target_ . Current default:  lightning.Trainer .

Q: What is the default value for  trainer._target_  in nequip.yaml?
A: The default value is  lightning.Trainer .

Q: How do I set the  accelerator  in  nequip.yaml ?
A: In  nequip.yaml , the  accelerator  parameter is located under  trainer.accelerator . Current default:  auto .

Q: What is the default value for  trainer.accelerator  in nequip.yaml?
A: The default value is  auto .

Q: How do I set the  devices  in  nequip.yaml ?
A: In  nequip.yaml , the  devices  parameter is located under  trainer.devices . Current default:  1 .

Q: What is the default value for  trainer.devices  in nequip.yaml?
A: The default value is  1 .

Q: How do I set the  enable_checkpointing  in  nequip.yaml ?
A: In  nequip.yaml , the  enable_checkpointing  parameter is located under  trainer.enable_checkpointing . Current default:  True .

Q: What is the default value for  trainer.enable_checkpointing  in nequip.yaml?
A: The default value is  True .

Q: How do I set the  max_epochs  in  nequip.yaml ?
A: In  nequip.yaml , the  max_epochs  parameter is located under  trainer.max_epochs . Current default:  3 .

Q: What is the default value for  trainer.max_epochs  in nequip.yaml?
A: The default value is  3 .

Q: How do I set the  max_time  in  nequip.yaml ?
A: In  nequip.yaml , the  max_time  parameter is located under  trainer.max_time . Current default:  03:00:00:00 .

Q: What is the default value for  trainer.max_time  in nequip.yaml?
A: The default value is  03:00:00:00 .

Q: How do I set the  check_val_every_n_epoch  in  nequip.yaml ?
A: In  nequip.yaml , the  check_val_every_n_epoch  parameter is located under  trainer.check_val_every_n_epoch . Current default:  1 .

Q: What is the default value for  trainer.check_val_every_n_epoch  in nequip.yaml?
A: The default value is  1 .

Q: How do I set the  log_every_n_steps  in  nequip.yaml ?
A: In  nequip.yaml , the  log_every_n_steps  parameter is located under  trainer.log_every_n_steps . Current default:  1 .

Q: What is the default value for  trainer.log_every_n_steps  in nequip.yaml?
A: The default value is  1 .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module._target_ . Current default:  nequip.train.EMALightningModule .

Q: What is the default value for  training_module._target_  in nequip.yaml?
A: The default value is  nequip.train.EMALightningModule .

Q: How do I set the  ema_decay  in  nequip.yaml ?
A: In  nequip.yaml , the  ema_decay  parameter is located under  training_module.ema_decay . Current default:  0.999 .

Q: What is the default value for  training_module.ema_decay  in nequip.yaml?
A: The default value is  0.999 .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module.loss._target_ . Current default:  nequip.train.EnergyForceLoss .

Q: What is the default value for  training_module.loss._target_  in nequip.yaml?
A: The default value is  nequip.train.EnergyForceLoss .

Q: How do I set the  per_atom_energy  in  nequip.yaml ?
A: In  nequip.yaml , the  per_atom_energy  parameter is located under  training_module.loss.per_atom_energy . Current default:  True .

Q: What is the default value for  training_module.loss.per_atom_energy  in nequip.yaml?
A: The default value is  True .

Q: How do I set the  total_energy  in  nequip.yaml ?
A: In  nequip.yaml , the  total_energy  parameter is located under  training_module.loss.coeffs.total_energy . Current default:  0.05 .

Q: What is the default value for  training_module.loss.coeffs.total_energy  in nequip.yaml?
A: The default value is  0.05 .

Q: How do I set the  forces  in  nequip.yaml ?
A: In  nequip.yaml , the  forces  parameter is located under  training_module.loss.coeffs.forces . Current default:  0.95 .

Q: What is the default value for  training_module.loss.coeffs.forces  in nequip.yaml?
A: The default value is  0.95 .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module.val_metrics._target_ . Current default:  nequip.train.EnergyForceMetrics .

Q: What is the default value for  training_module.val_metrics._target_  in nequip.yaml?
A: The default value is  nequip.train.EnergyForceMetrics .

Q: How do I set the  total_energy_mae  in  nequip.yaml ?
A: In  nequip.yaml , the  total_energy_mae  parameter is located under  training_module.val_metrics.coeffs.total_energy_mae . Current default:  1.0 .

Q: What is the default value for  training_module.val_metrics.coeffs.total_energy_mae  in nequip.yaml?
A: The default value is  1.0 .

Q: How do I set the  forces_mae  in  nequip.yaml ?
A: In  nequip.yaml , the  forces_mae  parameter is located under  training_module.val_metrics.coeffs.forces_mae . Current default:  1.0 .

Q: What is the default value for  training_module.val_metrics.coeffs.forces_mae  in nequip.yaml?
A: The default value is  1.0 .

Q: How do I set the  train_metrics  in  nequip.yaml ?
A: In  nequip.yaml , the  train_metrics  parameter is located under  training_module.train_metrics . Current default:  ${training_module.val_metrics} .

Q: What is the default value for  training_module.train_metrics  in nequip.yaml?
A: The default value is  ${training_module.val_metrics} .

Q: How do I set the  test_metrics  in  nequip.yaml ?
A: In  nequip.yaml , the  test_metrics  parameter is located under  training_module.test_metrics . Current default:  ${training_module.val_metrics} .

Q: What is the default value for  training_module.test_metrics  in nequip.yaml?
A: The default value is  ${training_module.val_metrics} .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module.optimizer._target_ . Current default:  torch.optim.Adam .

Q: What is the default value for  training_module.optimizer._target_  in nequip.yaml?
A: The default value is  torch.optim.Adam .

Q: How do I set the  lr  in  nequip.yaml ?
A: In  nequip.yaml , the  lr  parameter is located under  training_module.optimizer.lr . Current default:  0.03 .

Q: What is the default value for  training_module.optimizer.lr  in nequip.yaml?
A: The default value is  0.03 .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module.lr_scheduler.scheduler._target_ . Current default:  torch.optim.lr_scheduler.ReduceLROnPlateau .

Q: What is the default value for  training_module.lr_scheduler.scheduler._target_  in nequip.yaml?
A: The default value is  torch.optim.lr_scheduler.ReduceLROnPlateau .

Q: How do I set the  factor  in  nequip.yaml ?
A: In  nequip.yaml , the  factor  parameter is located under  training_module.lr_scheduler.scheduler.factor . Current default:  0.6 .

Q: What is the default value for  training_module.lr_scheduler.scheduler.factor  in nequip.yaml?
A: The default value is  0.6 .

Q: How do I set the  patience  in  nequip.yaml ?
A: In  nequip.yaml , the  patience  parameter is located under  training_module.lr_scheduler.scheduler.patience . Current default:  5 .

Q: What is the default value for  training_module.lr_scheduler.scheduler.patience  in nequip.yaml?
A: The default value is  5 .

Q: How do I set the  threshold  in  nequip.yaml ?
A: In  nequip.yaml , the  threshold  parameter is located under  training_module.lr_scheduler.scheduler.threshold . Current default:  0.2 .

Q: What is the default value for  training_module.lr_scheduler.scheduler.threshold  in nequip.yaml?
A: The default value is  0.2 .

Q: How do I set the  min_lr  in  nequip.yaml ?
A: In  nequip.yaml , the  min_lr  parameter is located under  training_module.lr_scheduler.scheduler.min_lr . Current default:  1e-6 .

Q: What is the default value for  training_module.lr_scheduler.scheduler.min_lr  in nequip.yaml?
A: The default value is  1e-6 .

Q: How do I set the  monitor  in  nequip.yaml ?
A: In  nequip.yaml , the  monitor  parameter is located under  training_module.lr_scheduler.monitor . Current default:  val0_epoch/weighted_sum .

Q: What is the default value for  training_module.lr_scheduler.monitor  in nequip.yaml?
A: The default value is  val0_epoch/weighted_sum .

Q: How do I set the  interval  in  nequip.yaml ?
A: In  nequip.yaml , the  interval  parameter is located under  training_module.lr_scheduler.interval . Current default:  epoch .

Q: What is the default value for  training_module.lr_scheduler.interval  in nequip.yaml?
A: The default value is  epoch .

Q: How do I set the  frequency  in  nequip.yaml ?
A: In  nequip.yaml , the  frequency  parameter is located under  training_module.lr_scheduler.frequency . Current default:  1 .

Q: What is the default value for  training_module.lr_scheduler.frequency  in nequip.yaml?
A: The default value is  1 .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module.model._target_ . Current default:  nequip.model.NequIPGNNModel .

Q: What is the default value for  training_module.model._target_  in nequip.yaml?
A: The default value is  nequip.model.NequIPGNNModel .

Q: How do I set the  seed  in  nequip.yaml ?
A: In  nequip.yaml , the  seed  parameter is located under  training_module.model.seed . Current default:  456 .

Q: What is the default value for  training_module.model.seed  in nequip.yaml?
A: The default value is  456 .

Q: How do I set the  model_dtype  in  nequip.yaml ?
A: In  nequip.yaml , the  model_dtype  parameter is located under  training_module.model.model_dtype . Current default:  float32 .

Q: What is the default value for  training_module.model.model_dtype  in nequip.yaml?
A: The default value is  float32 .

Q: How do I set the  type_names  in  nequip.yaml ?
A: In  nequip.yaml , the  type_names  parameter is located under  training_module.model.type_names . Current default:  ${model_type_names} .

Q: What is the default value for  training_module.model.type_names  in nequip.yaml?
A: The default value is  ${model_type_names} .

Q: How do I set the  r_max  in  nequip.yaml ?
A: In  nequip.yaml , the  r_max  parameter is located under  training_module.model.r_max . Current default:  ${cutoff_radius} .

Q: What is the default value for  training_module.model.r_max  in nequip.yaml?
A: The default value is  ${cutoff_radius} .

Q: How do I set the  num_bessels  in  nequip.yaml ?
A: In  nequip.yaml , the  num_bessels  parameter is located under  training_module.model.num_bessels . Current default:  8 .

Q: What is the default value for  training_module.model.num_bessels  in nequip.yaml?
A: The default value is  8 .

Q: How do I set the  bessel_trainable  in  nequip.yaml ?
A: In  nequip.yaml , the  bessel_trainable  parameter is located under  training_module.model.bessel_trainable . Current default:  False .

Q: What is the default value for  training_module.model.bessel_trainable  in nequip.yaml?
A: The default value is  False .

Q: How do I set the  polynomial_cutoff_p  in  nequip.yaml ?
A: In  nequip.yaml , the  polynomial_cutoff_p  parameter is located under  training_module.model.polynomial_cutoff_p . Current default:  6 .

Q: What is the default value for  training_module.model.polynomial_cutoff_p  in nequip.yaml?
A: The default value is  6 .

Q: How do I set the  num_layers  in  nequip.yaml ?
A: In  nequip.yaml , the  num_layers  parameter is located under  training_module.model.num_layers . Current default:  3 .

Q: What is the default value for  training_module.model.num_layers  in nequip.yaml?
A: The default value is  3 .

Q: How do I set the  l_max  in  nequip.yaml ?
A: In  nequip.yaml , the  l_max  parameter is located under  training_module.model.l_max . Current default:  1 .

Q: What is the default value for  training_module.model.l_max  in nequip.yaml?
A: The default value is  1 .

Q: How do I set the  parity  in  nequip.yaml ?
A: In  nequip.yaml , the  parity  parameter is located under  training_module.model.parity . Current default:  True .

Q: What is the default value for  training_module.model.parity  in nequip.yaml?
A: The default value is  True .

Q: How do I set the  num_features  in  nequip.yaml ?
A: In  nequip.yaml , the  num_features  parameter is located under  training_module.model.num_features . Current default:  32 .

Q: What is the default value for  training_module.model.num_features  in nequip.yaml?
A: The default value is  32 .

Q: How do I set the  radial_mlp_depth  in  nequip.yaml ?
A: In  nequip.yaml , the  radial_mlp_depth  parameter is located under  training_module.model.radial_mlp_depth . Current default:  2 .

Q: What is the default value for  training_module.model.radial_mlp_depth  in nequip.yaml?
A: The default value is  2 .

Q: How do I set the  radial_mlp_width  in  nequip.yaml ?
A: In  nequip.yaml , the  radial_mlp_width  parameter is located under  training_module.model.radial_mlp_width . Current default:  64 .

Q: What is the default value for  training_module.model.radial_mlp_width  in nequip.yaml?
A: The default value is  64 .

Q: How do I set the  avg_num_neighbors  in  nequip.yaml ?
A: In  nequip.yaml , the  avg_num_neighbors  parameter is located under  training_module.model.avg_num_neighbors . Current default:  ${training_data_stats:num_neighbors_mean} .

Q: What is the default value for  training_module.model.avg_num_neighbors  in nequip.yaml?
A: The default value is  ${training_data_stats:num_neighbors_mean} .

Q: How do I set the  per_type_energy_scales  in  nequip.yaml ?
A: In  nequip.yaml , the  per_type_energy_scales  parameter is located under  training_module.model.per_type_energy_scales . Current default:  ${training_data_stats:per_type_forces_rms} .

Q: What is the default value for  training_module.model.per_type_energy_scales  in nequip.yaml?
A: The default value is  ${training_data_stats:per_type_forces_rms} .

Q: How do I set the  per_type_energy_shifts  in  nequip.yaml ?
A: In  nequip.yaml , the  per_type_energy_shifts  parameter is located under  training_module.model.per_type_energy_shifts . Current default:  ${training_data_stats:per_atom_energy_mean} .

Q: What is the default value for  training_module.model.per_type_energy_shifts  in nequip.yaml?
A: The default value is  ${training_data_stats:per_atom_energy_mean} .

Q: How do I set the  per_type_energy_scales_trainable  in  nequip.yaml ?
A: In  nequip.yaml , the  per_type_energy_scales_trainable  parameter is located under  training_module.model.per_type_energy_scales_trainable . Current default:  False .

Q: What is the default value for  training_module.model.per_type_energy_scales_trainable  in nequip.yaml?
A: The default value is  False .

Q: How do I set the  per_type_energy_shifts_trainable  in  nequip.yaml ?
A: In  nequip.yaml , the  per_type_energy_shifts_trainable  parameter is located under  training_module.model.per_type_energy_shifts_trainable . Current default:  False .

Q: What is the default value for  training_module.model.per_type_energy_shifts_trainable  in nequip.yaml?
A: The default value is  False .

Q: How do I set the  _target_  in  nequip.yaml ?
A: In  nequip.yaml , the  _target_  parameter is located under  training_module.model.pair_potential._target_ . Current default:  nequip.nn.pair_potential.ZBL .

Q: What is the default value for  training_module.model.pair_potential._target_  in nequip.yaml?
A: The default value is  nequip.nn.pair_potential.ZBL .

Q: How do I set the  units  in  nequip.yaml ?
A: In  nequip.yaml , the  units  parameter is located under  training_module.model.pair_potential.units . Current default:  metal .

Q: What is the default value for  training_module.model.pair_potential.units  in nequip.yaml?
A: The default value is  metal .

Q: How do I set the  chemical_species  in  nequip.yaml ?
A: In  nequip.yaml , the  chemical_species  parameter is located under  training_module.model.pair_potential.chemical_species . Current default:  ${chemical_symbols} .

Q: What is the default value for  training_module.model.pair_potential.chemical_species  in nequip.yaml?
A: The default value is  ${chemical_symbols} .

Q: How do I set the  allow_tf32  in  nequip.yaml ?
A: In  nequip.yaml , the  allow_tf32  parameter is located under  global_options.allow_tf32 . Current default:  False .

Q: What is the default value for  global_options.allow_tf32  in nequip.yaml?
A: The default value is  False .

Q: Show me the full content of painn.yaml.
A: Here is the content of  painn.yaml :
   yaml
# General settings
general:
    seed: 42  # Random seed for reproducibility
    database_name: 'Database.db'  # Name of the database file

# Data handling settings
data:
    dataset_path: 
    use_last_n: 100  # current testing
# Model architecture settings
model:
    model_type: painn   # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.
    n_rbf: 40
    n_atom_basis: 192
    n_interactions: 2
    
    dropout_rate: null # Default is 0.1 but it will not work when you select only 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null # [256, 128, 64, 32] (if null then n_neurons == n_atom_basis)
    distance_unit: 'Ang'
    property_unit_dict:
      energy: 'eV'
      forces: 'eV/Ang'

# Output settings
outputs:
    energy:
      loss_weight: 0.05
      metrics: "MAE"
    forces:
      loss_weight: 0.95
      metrics: "MAE"

# Training settings
training:
    accelerator: 'gpu'
    devices: 1
    precision: 32
    batch_size: 16
    num_train: 800
    num_val: 200
    max_epochs: 3
    num_workers: 24
    pin_memory: true
    optimizer:
      type: 'AdamW'
      lr: 0.0001 #changed 0.0001 to 0.001
    scheduler:
      type: 'ReduceLROnPlateau'
      factor: 0.8
      patience: 30
      verbose: true

# Logging and checkpoint settings
logging:
    folder: './results'
    log_dir: "lightning_logs"
    checkpoint_dir: "best_inference_model"
    monitor: "val_loss"

# Testing settings
testing:
    trained_model_path: './results'  # Path to load the trained model
    csv_file_name: 'actual_vs_predicted_enrgforc.csv'  # Path to save the predictions CSV

# Resume training settings
resume_training:
    resume_checkpoint_dir: null # Path to model checkpoint file if required 

# Fine Tuning settings
fine_tuning:
    pretrained_checkpoint: # checkpoint file path
    freeze_embedding: true
    freeze_interactions_up_to: 2 # add no of layers
    freeze_all_representation: true
    lr: 0.00005
    early_stopping_patience: 10
    best_model_dir: fine_tuned_best_model  # Subdirectory only
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory only
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
   

Q: How do I set the  seed  in  painn.yaml ?
A: In  painn.yaml , the  seed  parameter is located under  general.seed . Current default:  42 .

Q: What is the default value for  general.seed  in painn.yaml?
A: The default value is  42 .

Q: How do I set the  database_name  in  painn.yaml ?
A: In  painn.yaml , the  database_name  parameter is located under  general.database_name . Current default:  Database.db .

Q: What is the default value for  general.database_name  in painn.yaml?
A: The default value is  Database.db .

Q: How do I set the  dataset_path  in  painn.yaml ?
A: In  painn.yaml , the  dataset_path  parameter is located under  data.dataset_path . Current default:  None .

Q: What is the default value for  data.dataset_path  in painn.yaml?
A: The default value is  None .

Q: How do I set the  use_last_n  in  painn.yaml ?
A: In  painn.yaml , the  use_last_n  parameter is located under  data.use_last_n . Current default:  100 .

Q: What is the default value for  data.use_last_n  in painn.yaml?
A: The default value is  100 .

Q: How do I set the  model_type  in  painn.yaml ?
A: In  painn.yaml , the  model_type  parameter is located under  model.model_type . Current default:  painn .

Q: What is the default value for  model.model_type  in painn.yaml?
A: The default value is  painn .

Q: How do I set the  cutoff  in  painn.yaml ?
A: In  painn.yaml , the  cutoff  parameter is located under  model.cutoff . Current default:  12.0 .

Q: What is the default value for  model.cutoff  in painn.yaml?
A: The default value is  12.0 .

Q: How do I set the  n_rbf  in  painn.yaml ?
A: In  painn.yaml , the  n_rbf  parameter is located under  model.n_rbf . Current default:  40 .

Q: What is the default value for  model.n_rbf  in painn.yaml?
A: The default value is  40 .

Q: How do I set the  n_atom_basis  in  painn.yaml ?
A: In  painn.yaml , the  n_atom_basis  parameter is located under  model.n_atom_basis . Current default:  192 .

Q: What is the default value for  model.n_atom_basis  in painn.yaml?
A: The default value is  192 .

Q: How do I set the  n_interactions  in  painn.yaml ?
A: In  painn.yaml , the  n_interactions  parameter is located under  model.n_interactions . Current default:  2 .

Q: What is the default value for  model.n_interactions  in painn.yaml?
A: The default value is  2 .

Q: How do I set the  dropout_rate  in  painn.yaml ?
A: In  painn.yaml , the  dropout_rate  parameter is located under  model.dropout_rate . Current default:  None .

Q: What is the default value for  model.dropout_rate  in painn.yaml?
A: The default value is  None .

Q: How do I set the  n_layers  in  painn.yaml ?
A: In  painn.yaml , the  n_layers  parameter is located under  model.n_layers . Current default:  1 .

Q: What is the default value for  model.n_layers  in painn.yaml?
A: The default value is  1 .

Q: How do I set the  n_neurons  in  painn.yaml ?
A: In  painn.yaml , the  n_neurons  parameter is located under  model.n_neurons . Current default:  None .

Q: What is the default value for  model.n_neurons  in painn.yaml?
A: The default value is  None .

Q: How do I set the  distance_unit  in  painn.yaml ?
A: In  painn.yaml , the  distance_unit  parameter is located under  model.distance_unit . Current default:  Ang .

Q: What is the default value for  model.distance_unit  in painn.yaml?
A: The default value is  Ang .

Q: How do I set the  energy  in  painn.yaml ?
A: In  painn.yaml , the  energy  parameter is located under  model.property_unit_dict.energy . Current default:  eV .

Q: What is the default value for  model.property_unit_dict.energy  in painn.yaml?
A: The default value is  eV .

Q: How do I set the  forces  in  painn.yaml ?
A: In  painn.yaml , the  forces  parameter is located under  model.property_unit_dict.forces . Current default:  eV/Ang .

Q: What is the default value for  model.property_unit_dict.forces  in painn.yaml?
A: The default value is  eV/Ang .

Q: How do I set the  loss_weight  in  painn.yaml ?
A: In  painn.yaml , the  loss_weight  parameter is located under  outputs.energy.loss_weight . Current default:  0.05 .

Q: What is the default value for  outputs.energy.loss_weight  in painn.yaml?
A: The default value is  0.05 .

Q: How do I set the  metrics  in  painn.yaml ?
A: In  painn.yaml , the  metrics  parameter is located under  outputs.energy.metrics . Current default:  MAE .

Q: What is the default value for  outputs.energy.metrics  in painn.yaml?
A: The default value is  MAE .

Q: How do I set the  loss_weight  in  painn.yaml ?
A: In  painn.yaml , the  loss_weight  parameter is located under  outputs.forces.loss_weight . Current default:  0.95 .

Q: What is the default value for  outputs.forces.loss_weight  in painn.yaml?
A: The default value is  0.95 .

Q: How do I set the  metrics  in  painn.yaml ?
A: In  painn.yaml , the  metrics  parameter is located under  outputs.forces.metrics . Current default:  MAE .

Q: What is the default value for  outputs.forces.metrics  in painn.yaml?
A: The default value is  MAE .

Q: How do I set the  accelerator  in  painn.yaml ?
A: In  painn.yaml , the  accelerator  parameter is located under  training.accelerator . Current default:  gpu .

Q: What is the default value for  training.accelerator  in painn.yaml?
A: The default value is  gpu .

Q: How do I set the  devices  in  painn.yaml ?
A: In  painn.yaml , the  devices  parameter is located under  training.devices . Current default:  1 .

Q: What is the default value for  training.devices  in painn.yaml?
A: The default value is  1 .

Q: How do I set the  precision  in  painn.yaml ?
A: In  painn.yaml , the  precision  parameter is located under  training.precision . Current default:  32 .

Q: What is the default value for  training.precision  in painn.yaml?
A: The default value is  32 .

Q: How do I set the  batch_size  in  painn.yaml ?
A: In  painn.yaml , the  batch_size  parameter is located under  training.batch_size . Current default:  16 .

Q: What is the default value for  training.batch_size  in painn.yaml?
A: The default value is  16 .

Q: How do I set the  num_train  in  painn.yaml ?
A: In  painn.yaml , the  num_train  parameter is located under  training.num_train . Current default:  800 .

Q: What is the default value for  training.num_train  in painn.yaml?
A: The default value is  800 .

Q: How do I set the  num_val  in  painn.yaml ?
A: In  painn.yaml , the  num_val  parameter is located under  training.num_val . Current default:  200 .

Q: What is the default value for  training.num_val  in painn.yaml?
A: The default value is  200 .

Q: How do I set the  max_epochs  in  painn.yaml ?
A: In  painn.yaml , the  max_epochs  parameter is located under  training.max_epochs . Current default:  3 .

Q: What is the default value for  training.max_epochs  in painn.yaml?
A: The default value is  3 .

Q: How do I set the  num_workers  in  painn.yaml ?
A: In  painn.yaml , the  num_workers  parameter is located under  training.num_workers . Current default:  24 .

Q: What is the default value for  training.num_workers  in painn.yaml?
A: The default value is  24 .

Q: How do I set the  pin_memory  in  painn.yaml ?
A: In  painn.yaml , the  pin_memory  parameter is located under  training.pin_memory . Current default:  True .

Q: What is the default value for  training.pin_memory  in painn.yaml?
A: The default value is  True .

Q: How do I set the  type  in  painn.yaml ?
A: In  painn.yaml , the  type  parameter is located under  training.optimizer.type . Current default:  AdamW .

Q: What is the default value for  training.optimizer.type  in painn.yaml?
A: The default value is  AdamW .

Q: How do I set the  lr  in  painn.yaml ?
A: In  painn.yaml , the  lr  parameter is located under  training.optimizer.lr . Current default:  0.0001 .

Q: What is the default value for  training.optimizer.lr  in painn.yaml?
A: The default value is  0.0001 .

Q: How do I set the  type  in  painn.yaml ?
A: In  painn.yaml , the  type  parameter is located under  training.scheduler.type . Current default:  ReduceLROnPlateau .

Q: What is the default value for  training.scheduler.type  in painn.yaml?
A: The default value is  ReduceLROnPlateau .

Q: How do I set the  factor  in  painn.yaml ?
A: In  painn.yaml , the  factor  parameter is located under  training.scheduler.factor . Current default:  0.8 .

Q: What is the default value for  training.scheduler.factor  in painn.yaml?
A: The default value is  0.8 .

Q: How do I set the  patience  in  painn.yaml ?
A: In  painn.yaml , the  patience  parameter is located under  training.scheduler.patience . Current default:  30 .

Q: What is the default value for  training.scheduler.patience  in painn.yaml?
A: The default value is  30 .

Q: How do I set the  verbose  in  painn.yaml ?
A: In  painn.yaml , the  verbose  parameter is located under  training.scheduler.verbose . Current default:  True .

Q: What is the default value for  training.scheduler.verbose  in painn.yaml?
A: The default value is  True .

Q: How do I set the  folder  in  painn.yaml ?
A: In  painn.yaml , the  folder  parameter is located under  logging.folder . Current default:  ./results .

Q: What is the default value for  logging.folder  in painn.yaml?
A: The default value is  ./results .

Q: How do I set the  log_dir  in  painn.yaml ?
A: In  painn.yaml , the  log_dir  parameter is located under  logging.log_dir . Current default:  lightning_logs .

Q: What is the default value for  logging.log_dir  in painn.yaml?
A: The default value is  lightning_logs .

Q: How do I set the  checkpoint_dir  in  painn.yaml ?
A: In  painn.yaml , the  checkpoint_dir  parameter is located under  logging.checkpoint_dir . Current default:  best_inference_model .

Q: What is the default value for  logging.checkpoint_dir  in painn.yaml?
A: The default value is  best_inference_model .

Q: How do I set the  monitor  in  painn.yaml ?
A: In  painn.yaml , the  monitor  parameter is located under  logging.monitor . Current default:  val_loss .

Q: What is the default value for  logging.monitor  in painn.yaml?
A: The default value is  val_loss .

Q: How do I set the  trained_model_path  in  painn.yaml ?
A: In  painn.yaml , the  trained_model_path  parameter is located under  testing.trained_model_path . Current default:  ./results .

Q: What is the default value for  testing.trained_model_path  in painn.yaml?
A: The default value is  ./results .

Q: How do I set the  csv_file_name  in  painn.yaml ?
A: In  painn.yaml , the  csv_file_name  parameter is located under  testing.csv_file_name . Current default:  actual_vs_predicted_enrgforc.csv .

Q: What is the default value for  testing.csv_file_name  in painn.yaml?
A: The default value is  actual_vs_predicted_enrgforc.csv .

Q: How do I set the  resume_checkpoint_dir  in  painn.yaml ?
A: In  painn.yaml , the  resume_checkpoint_dir  parameter is located under  resume_training.resume_checkpoint_dir . Current default:  None .

Q: What is the default value for  resume_training.resume_checkpoint_dir  in painn.yaml?
A: The default value is  None .

Q: How do I set the  pretrained_checkpoint  in  painn.yaml ?
A: In  painn.yaml , the  pretrained_checkpoint  parameter is located under  fine_tuning.pretrained_checkpoint . Current default:  None .

Q: What is the default value for  fine_tuning.pretrained_checkpoint  in painn.yaml?
A: The default value is  None .

Q: How do I set the  freeze_embedding  in  painn.yaml ?
A: In  painn.yaml , the  freeze_embedding  parameter is located under  fine_tuning.freeze_embedding . Current default:  True .

Q: What is the default value for  fine_tuning.freeze_embedding  in painn.yaml?
A: The default value is  True .

Q: How do I set the  freeze_interactions_up_to  in  painn.yaml ?
A: In  painn.yaml , the  freeze_interactions_up_to  parameter is located under  fine_tuning.freeze_interactions_up_to . Current default:  2 .

Q: What is the default value for  fine_tuning.freeze_interactions_up_to  in painn.yaml?
A: The default value is  2 .

Q: How do I set the  freeze_all_representation  in  painn.yaml ?
A: In  painn.yaml , the  freeze_all_representation  parameter is located under  fine_tuning.freeze_all_representation . Current default:  True .

Q: What is the default value for  fine_tuning.freeze_all_representation  in painn.yaml?
A: The default value is  True .

Q: How do I set the  lr  in  painn.yaml ?
A: In  painn.yaml , the  lr  parameter is located under  fine_tuning.lr . Current default:  5e-05 .

Q: What is the default value for  fine_tuning.lr  in painn.yaml?
A: The default value is  5e-05 .

Q: How do I set the  early_stopping_patience  in  painn.yaml ?
A: In  painn.yaml , the  early_stopping_patience  parameter is located under  fine_tuning.early_stopping_patience . Current default:  10 .

Q: What is the default value for  fine_tuning.early_stopping_patience  in painn.yaml?
A: The default value is  10 .

Q: How do I set the  best_model_dir  in  painn.yaml ?
A: In  painn.yaml , the  best_model_dir  parameter is located under  fine_tuning.best_model_dir . Current default:  fine_tuned_best_model .

Q: What is the default value for  fine_tuning.best_model_dir  in painn.yaml?
A: The default value is  fine_tuned_best_model .

Q: How do I set the  checkpoint_dir  in  painn.yaml ?
A: In  painn.yaml , the  checkpoint_dir  parameter is located under  fine_tuning.checkpoint_dir . Current default:  fine_tuned_checkpoints .

Q: What is the default value for  fine_tuning.checkpoint_dir  in painn.yaml?
A: The default value is  fine_tuned_checkpoints .

Q: How do I set the  log_name  in  painn.yaml ?
A: In  painn.yaml , the  log_name  parameter is located under  fine_tuning.log_name . Current default:  fine_tune_logs .

Q: What is the default value for  fine_tuning.log_name  in painn.yaml?
A: The default value is  fine_tune_logs .

Q: Show me the full content of schnet.yaml.
A: Here is the content of  schnet.yaml :
   yaml
# General settings
general:
    seed: 42  # Random seed for reproducibility
    database_name: 'Database.db'  # Name of the database file

# Data handling settings
data:
    dataset_path: 
    use_last_n: 100  # current testing
# Model architecture settings
model:
    model_type: schnet   # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.
    n_rbf: 40
    n_atom_basis: 192
    n_interactions: 2
    
    dropout_rate: null # Default is 0.1 but it will not work when you select only 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null # [256, 128, 64, 32] (if null then n_neurons == n_atom_basis)
    distance_unit: 'Ang'
    property_unit_dict:
      energy: 'eV'
      forces: 'eV/Ang'

# Output settings
outputs:
    energy:
      loss_weight: 0.05
      metrics: "MAE"
    forces:
      loss_weight: 0.95
      metrics: "MAE"

# Training settings
training:
    accelerator: 'gpu'
    devices: 1
    precision: 32
    batch_size: 16
    num_train: 800
    num_val: 100
    num_test: 100
    max_epochs: 3
    num_workers: 24
    pin_memory: true
    log_every_n_steps: 1
    optimizer:
      type: 'AdamW'
      lr: 0.0001 #changed 0.0001 to 0.001
    scheduler:
      type: 'ReduceLROnPlateau'
      factor: 0.8
      patience: 30
      verbose: true
    early_stopping:      
      monitor: val_loss
      patience: 20
      min_delta: 0.001
      mode: min

# Logging and checkpoint settings
logging:
    folder: './results'
    log_dir: "lightning_logs"
    checkpoint_dir: "best_inference_model"
    monitor: "val_loss"

# Testing settings
testing:
    trained_model_path: './results'  # Path to load the trained model
    csv_file_name: 'actual_vs_predicted_enrgforc.csv'  # Path to save the predictions CSV

# Resume training settings
resume_training:
    resume_checkpoint_dir: null # Path to model checkpoint file if required 

# Fine Tuning settings
fine_tuning:
    pretrained_checkpoint: # checkpoint file path
    freeze_embedding: true
    freeze_interactions_up_to: 2 # add no of layers
    freeze_all_representation: true
    lr: 0.00005
    early_stopping_patience: 10
    best_model_dir: fine_tuned_best_model  # Subdirectory only
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory only
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
   

Q: How do I set the  seed  in  schnet.yaml ?
A: In  schnet.yaml , the  seed  parameter is located under  general.seed . Current default:  42 .

Q: What is the default value for  general.seed  in schnet.yaml?
A: The default value is  42 .

Q: How do I set the  database_name  in  schnet.yaml ?
A: In  schnet.yaml , the  database_name  parameter is located under  general.database_name . Current default:  Database.db .

Q: What is the default value for  general.database_name  in schnet.yaml?
A: The default value is  Database.db .

Q: How do I set the  dataset_path  in  schnet.yaml ?
A: In  schnet.yaml , the  dataset_path  parameter is located under  data.dataset_path . Current default:  None .

Q: What is the default value for  data.dataset_path  in schnet.yaml?
A: The default value is  None .

Q: How do I set the  use_last_n  in  schnet.yaml ?
A: In  schnet.yaml , the  use_last_n  parameter is located under  data.use_last_n . Current default:  100 .

Q: What is the default value for  data.use_last_n  in schnet.yaml?
A: The default value is  100 .

Q: How do I set the  model_type  in  schnet.yaml ?
A: In  schnet.yaml , the  model_type  parameter is located under  model.model_type . Current default:  schnet .

Q: What is the default value for  model.model_type  in schnet.yaml?
A: The default value is  schnet .

Q: How do I set the  cutoff  in  schnet.yaml ?
A: In  schnet.yaml , the  cutoff  parameter is located under  model.cutoff . Current default:  12.0 .

Q: What is the default value for  model.cutoff  in schnet.yaml?
A: The default value is  12.0 .

Q: How do I set the  n_rbf  in  schnet.yaml ?
A: In  schnet.yaml , the  n_rbf  parameter is located under  model.n_rbf . Current default:  40 .

Q: What is the default value for  model.n_rbf  in schnet.yaml?
A: The default value is  40 .

Q: How do I set the  n_atom_basis  in  schnet.yaml ?
A: In  schnet.yaml , the  n_atom_basis  parameter is located under  model.n_atom_basis . Current default:  192 .

Q: What is the default value for  model.n_atom_basis  in schnet.yaml?
A: The default value is  192 .

Q: How do I set the  n_interactions  in  schnet.yaml ?
A: In  schnet.yaml , the  n_interactions  parameter is located under  model.n_interactions . Current default:  2 .

Q: What is the default value for  model.n_interactions  in schnet.yaml?
A: The default value is  2 .

Q: How do I set the  dropout_rate  in  schnet.yaml ?
A: In  schnet.yaml , the  dropout_rate  parameter is located under  model.dropout_rate . Current default:  None .

Q: What is the default value for  model.dropout_rate  in schnet.yaml?
A: The default value is  None .

Q: How do I set the  n_layers  in  schnet.yaml ?
A: In  schnet.yaml , the  n_layers  parameter is located under  model.n_layers . Current default:  1 .

Q: What is the default value for  model.n_layers  in schnet.yaml?
A: The default value is  1 .

Q: How do I set the  n_neurons  in  schnet.yaml ?
A: In  schnet.yaml , the  n_neurons  parameter is located under  model.n_neurons . Current default:  None .

Q: What is the default value for  model.n_neurons  in schnet.yaml?
A: The default value is  None .

Q: How do I set the  distance_unit  in  schnet.yaml ?
A: In  schnet.yaml , the  distance_unit  parameter is located under  model.distance_unit . Current default:  Ang .

Q: What is the default value for  model.distance_unit  in schnet.yaml?
A: The default value is  Ang .

Q: How do I set the  energy  in  schnet.yaml ?
A: In  schnet.yaml , the  energy  parameter is located under  model.property_unit_dict.energy . Current default:  eV .

Q: What is the default value for  model.property_unit_dict.energy  in schnet.yaml?
A: The default value is  eV .

Q: How do I set the  forces  in  schnet.yaml ?
A: In  schnet.yaml , the  forces  parameter is located under  model.property_unit_dict.forces . Current default:  eV/Ang .

Q: What is the default value for  model.property_unit_dict.forces  in schnet.yaml?
A: The default value is  eV/Ang .

Q: How do I set the  loss_weight  in  schnet.yaml ?
A: In  schnet.yaml , the  loss_weight  parameter is located under  outputs.energy.loss_weight . Current default:  0.05 .

Q: What is the default value for  outputs.energy.loss_weight  in schnet.yaml?
A: The default value is  0.05 .

Q: How do I set the  metrics  in  schnet.yaml ?
A: In  schnet.yaml , the  metrics  parameter is located under  outputs.energy.metrics . Current default:  MAE .

Q: What is the default value for  outputs.energy.metrics  in schnet.yaml?
A: The default value is  MAE .

Q: How do I set the  loss_weight  in  schnet.yaml ?
A: In  schnet.yaml , the  loss_weight  parameter is located under  outputs.forces.loss_weight . Current default:  0.95 .

Q: What is the default value for  outputs.forces.loss_weight  in schnet.yaml?
A: The default value is  0.95 .

Q: How do I set the  metrics  in  schnet.yaml ?
A: In  schnet.yaml , the  metrics  parameter is located under  outputs.forces.metrics . Current default:  MAE .

Q: What is the default value for  outputs.forces.metrics  in schnet.yaml?
A: The default value is  MAE .

Q: How do I set the  accelerator  in  schnet.yaml ?
A: In  schnet.yaml , the  accelerator  parameter is located under  training.accelerator . Current default:  gpu .

Q: What is the default value for  training.accelerator  in schnet.yaml?
A: The default value is  gpu .

Q: How do I set the  devices  in  schnet.yaml ?
A: In  schnet.yaml , the  devices  parameter is located under  training.devices . Current default:  1 .

Q: What is the default value for  training.devices  in schnet.yaml?
A: The default value is  1 .

Q: How do I set the  precision  in  schnet.yaml ?
A: In  schnet.yaml , the  precision  parameter is located under  training.precision . Current default:  32 .

Q: What is the default value for  training.precision  in schnet.yaml?
A: The default value is  32 .

Q: How do I set the  batch_size  in  schnet.yaml ?
A: In  schnet.yaml , the  batch_size  parameter is located under  training.batch_size . Current default:  16 .

Q: What is the default value for  training.batch_size  in schnet.yaml?
A: The default value is  16 .

Q: How do I set the  num_train  in  schnet.yaml ?
A: In  schnet.yaml , the  num_train  parameter is located under  training.num_train . Current default:  800 .

Q: What is the default value for  training.num_train  in schnet.yaml?
A: The default value is  800 .

Q: How do I set the  num_val  in  schnet.yaml ?
A: In  schnet.yaml , the  num_val  parameter is located under  training.num_val . Current default:  100 .

Q: What is the default value for  training.num_val  in schnet.yaml?
A: The default value is  100 .

Q: How do I set the  num_test  in  schnet.yaml ?
A: In  schnet.yaml , the  num_test  parameter is located under  training.num_test . Current default:  100 .

Q: What is the default value for  training.num_test  in schnet.yaml?
A: The default value is  100 .

Q: How do I set the  max_epochs  in  schnet.yaml ?
A: In  schnet.yaml , the  max_epochs  parameter is located under  training.max_epochs . Current default:  3 .

Q: What is the default value for  training.max_epochs  in schnet.yaml?
A: The default value is  3 .

Q: How do I set the  num_workers  in  schnet.yaml ?
A: In  schnet.yaml , the  num_workers  parameter is located under  training.num_workers . Current default:  24 .

Q: What is the default value for  training.num_workers  in schnet.yaml?
A: The default value is  24 .

Q: How do I set the  pin_memory  in  schnet.yaml ?
A: In  schnet.yaml , the  pin_memory  parameter is located under  training.pin_memory . Current default:  True .

Q: What is the default value for  training.pin_memory  in schnet.yaml?
A: The default value is  True .

Q: How do I set the  log_every_n_steps  in  schnet.yaml ?
A: In  schnet.yaml , the  log_every_n_steps  parameter is located under  training.log_every_n_steps . Current default:  1 .

Q: What is the default value for  training.log_every_n_steps  in schnet.yaml?
A: The default value is  1 .

Q: How do I set the  type  in  schnet.yaml ?
A: In  schnet.yaml , the  type  parameter is located under  training.optimizer.type . Current default:  AdamW .

Q: What is the default value for  training.optimizer.type  in schnet.yaml?
A: The default value is  AdamW .

Q: How do I set the  lr  in  schnet.yaml ?
A: In  schnet.yaml , the  lr  parameter is located under  training.optimizer.lr . Current default:  0.0001 .

Q: What is the default value for  training.optimizer.lr  in schnet.yaml?
A: The default value is  0.0001 .

Q: How do I set the  type  in  schnet.yaml ?
A: In  schnet.yaml , the  type  parameter is located under  training.scheduler.type . Current default:  ReduceLROnPlateau .

Q: What is the default value for  training.scheduler.type  in schnet.yaml?
A: The default value is  ReduceLROnPlateau .

Q: How do I set the  factor  in  schnet.yaml ?
A: In  schnet.yaml , the  factor  parameter is located under  training.scheduler.factor . Current default:  0.8 .

Q: What is the default value for  training.scheduler.factor  in schnet.yaml?
A: The default value is  0.8 .

Q: How do I set the  patience  in  schnet.yaml ?
A: In  schnet.yaml , the  patience  parameter is located under  training.scheduler.patience . Current default:  30 .

Q: What is the default value for  training.scheduler.patience  in schnet.yaml?
A: The default value is  30 .

Q: How do I set the  verbose  in  schnet.yaml ?
A: In  schnet.yaml , the  verbose  parameter is located under  training.scheduler.verbose . Current default:  True .

Q: What is the default value for  training.scheduler.verbose  in schnet.yaml?
A: The default value is  True .

Q: How do I set the  monitor  in  schnet.yaml ?
A: In  schnet.yaml , the  monitor  parameter is located under  training.early_stopping.monitor . Current default:  val_loss .

Q: What is the default value for  training.early_stopping.monitor  in schnet.yaml?
A: The default value is  val_loss .

Q: How do I set the  patience  in  schnet.yaml ?
A: In  schnet.yaml , the  patience  parameter is located under  training.early_stopping.patience . Current default:  20 .

Q: What is the default value for  training.early_stopping.patience  in schnet.yaml?
A: The default value is  20 .

Q: How do I set the  min_delta  in  schnet.yaml ?
A: In  schnet.yaml , the  min_delta  parameter is located under  training.early_stopping.min_delta . Current default:  0.001 .

Q: What is the default value for  training.early_stopping.min_delta  in schnet.yaml?
A: The default value is  0.001 .

Q: How do I set the  mode  in  schnet.yaml ?
A: In  schnet.yaml , the  mode  parameter is located under  training.early_stopping.mode . Current default:  min .

Q: What is the default value for  training.early_stopping.mode  in schnet.yaml?
A: The default value is  min .

Q: How do I set the  folder  in  schnet.yaml ?
A: In  schnet.yaml , the  folder  parameter is located under  logging.folder . Current default:  ./results .

Q: What is the default value for  logging.folder  in schnet.yaml?
A: The default value is  ./results .

Q: How do I set the  log_dir  in  schnet.yaml ?
A: In  schnet.yaml , the  log_dir  parameter is located under  logging.log_dir . Current default:  lightning_logs .

Q: What is the default value for  logging.log_dir  in schnet.yaml?
A: The default value is  lightning_logs .

Q: How do I set the  checkpoint_dir  in  schnet.yaml ?
A: In  schnet.yaml , the  checkpoint_dir  parameter is located under  logging.checkpoint_dir . Current default:  best_inference_model .

Q: What is the default value for  logging.checkpoint_dir  in schnet.yaml?
A: The default value is  best_inference_model .

Q: How do I set the  monitor  in  schnet.yaml ?
A: In  schnet.yaml , the  monitor  parameter is located under  logging.monitor . Current default:  val_loss .

Q: What is the default value for  logging.monitor  in schnet.yaml?
A: The default value is  val_loss .

Q: How do I set the  trained_model_path  in  schnet.yaml ?
A: In  schnet.yaml , the  trained_model_path  parameter is located under  testing.trained_model_path . Current default:  ./results .

Q: What is the default value for  testing.trained_model_path  in schnet.yaml?
A: The default value is  ./results .

Q: How do I set the  csv_file_name  in  schnet.yaml ?
A: In  schnet.yaml , the  csv_file_name  parameter is located under  testing.csv_file_name . Current default:  actual_vs_predicted_enrgforc.csv .

Q: What is the default value for  testing.csv_file_name  in schnet.yaml?
A: The default value is  actual_vs_predicted_enrgforc.csv .

Q: How do I set the  resume_checkpoint_dir  in  schnet.yaml ?
A: In  schnet.yaml , the  resume_checkpoint_dir  parameter is located under  resume_training.resume_checkpoint_dir . Current default:  None .

Q: What is the default value for  resume_training.resume_checkpoint_dir  in schnet.yaml?
A: The default value is  None .

Q: How do I set the  pretrained_checkpoint  in  schnet.yaml ?
A: In  schnet.yaml , the  pretrained_checkpoint  parameter is located under  fine_tuning.pretrained_checkpoint . Current default:  None .

Q: What is the default value for  fine_tuning.pretrained_checkpoint  in schnet.yaml?
A: The default value is  None .

Q: How do I set the  freeze_embedding  in  schnet.yaml ?
A: In  schnet.yaml , the  freeze_embedding  parameter is located under  fine_tuning.freeze_embedding . Current default:  True .

Q: What is the default value for  fine_tuning.freeze_embedding  in schnet.yaml?
A: The default value is  True .

Q: How do I set the  freeze_interactions_up_to  in  schnet.yaml ?
A: In  schnet.yaml , the  freeze_interactions_up_to  parameter is located under  fine_tuning.freeze_interactions_up_to . Current default:  2 .

Q: What is the default value for  fine_tuning.freeze_interactions_up_to  in schnet.yaml?
A: The default value is  2 .

Q: How do I set the  freeze_all_representation  in  schnet.yaml ?
A: In  schnet.yaml , the  freeze_all_representation  parameter is located under  fine_tuning.freeze_all_representation . Current default:  True .

Q: What is the default value for  fine_tuning.freeze_all_representation  in schnet.yaml?
A: The default value is  True .

Q: How do I set the  lr  in  schnet.yaml ?
A: In  schnet.yaml , the  lr  parameter is located under  fine_tuning.lr . Current default:  5e-05 .

Q: What is the default value for  fine_tuning.lr  in schnet.yaml?
A: The default value is  5e-05 .

Q: How do I set the  early_stopping_patience  in  schnet.yaml ?
A: In  schnet.yaml , the  early_stopping_patience  parameter is located under  fine_tuning.early_stopping_patience . Current default:  10 .

Q: What is the default value for  fine_tuning.early_stopping_patience  in schnet.yaml?
A: The default value is  10 .

Q: How do I set the  best_model_dir  in  schnet.yaml ?
A: In  schnet.yaml , the  best_model_dir  parameter is located under  fine_tuning.best_model_dir . Current default:  fine_tuned_best_model .

Q: What is the default value for  fine_tuning.best_model_dir  in schnet.yaml?
A: The default value is  fine_tuned_best_model .

Q: How do I set the  checkpoint_dir  in  schnet.yaml ?
A: In  schnet.yaml , the  checkpoint_dir  parameter is located under  fine_tuning.checkpoint_dir . Current default:  fine_tuned_checkpoints .

Q: What is the default value for  fine_tuning.checkpoint_dir  in schnet.yaml?
A: The default value is  fine_tuned_checkpoints .

Q: How do I set the  log_name  in  schnet.yaml ?
A: In  schnet.yaml , the  log_name  parameter is located under  fine_tuning.log_name . Current default:  fine_tune_logs .

Q: What is the default value for  fine_tuning.log_name  in schnet.yaml?
A: The default value is  fine_tune_logs .

Q: Show me the content of environment.yaml
A: Here is the configuration for  environment.yaml :

   yaml
name: mlff_chat
channels:
  - defaults
  - conda-forge
  - pytorch
dependencies:
  - python=3.12
  - pip
  - pandas
  - scikit-learn
  - wandb
  - pip:
      - schnetpack
      - dscribe
      - nequip==0.10.0
      - git+https://github.com/mir-group/allegro.git@v0.6.1
   

Q: Where is the environment.yaml file located?
A: The file  environment.yaml  is located at  MLFF_QD/environment.yaml .

Q: Show me the content of preprocess_config.yaml
A: Here is the configuration for  preprocess_config.yaml :

   yaml
# ───────────────────────────────────────────────
#                 Dataset Input
#  Choose ONE of the two options below for your data.
# ───────────────────────────────────────────────
dataset:
  # --- Option A: Use a single, pre-combined file (Recommended) ---
  # Uncomment 'input_file' if you have one file with positions, forces, and energy.
  # input_file: "dataset_pos_frc_ev.xyz"

  # --- Option B: Auto combine separate position and force files ---
  # Use these lines if your positions and forces are in two different files.
   pos_file: "HgTe-pos-1.xyz"
   frc_file: "HgTe-frc-1.xyz"
  
   output_prefix: "consolidated_dataset" 
   
   sizes: [200,500]
  # Subset counts (number of structures from each method)
   subset_counts:
     MD: 4036  # Structures obtained from Molecular Dynamics (MD) simulation 
   contamination: 0.05 # Fraction of outliers removed by Isolation Forest 
   
# ───────────────────────────────────────────────
# Optional SOAP descriptor settings
# (You can safely remove this section to let MLFF-QD
#  auto-detect species and use default SOAP parameters)
# ───────────────────────────────────────────────
SOAP: 
   species: ["Hg", "Cl", "Te"] 
   r_cut: 12.0 
   n_max: 7 
   l_max: 3 
   sigma: 0.1 
   periodic: False
   sparse: False
   

Q: Where is the preprocess_config.yaml file located?
A: The file  preprocess_config.yaml  is located at  MLFF_QD/config_files/preprocessing/preprocess_config.yaml .

Q: Show me the content of allegro.yaml
A: Here is the configuration for  allegro.yaml :

   yaml
run: [train, val, test]


cutoff_radius: 12.0
chemical_symbols: [Cd, Cl, Se] 
model_type_names: ${chemical_symbols}

data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 456             
  split_dataset:
    file_path:  #./consolidated_dataset_1000_CdSe_new.xyz
    train: 0.8
    val: 0.1
    test: 0.1
  transforms:
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}
  
  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
  test_dataloader: ${data.val_dataloader}
  stats_manager:
    _target_: nequip.data.CommonDataStatisticsManager
    type_names: ${model_type_names}

trainer:
  _target_: lightning.Trainer
  accelerator: auto
  devices: 1
  max_epochs: 3
  check_val_every_n_epoch: 1
  log_every_n_steps: 5
  
  callbacks:
      
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ./results 
      filename: best
      save_last: true
    
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      min_delta: 1e-3                         # how much to be considered a "change"
      patience: 20                            # how many instances of "no change" before stopping
      
  logger:
    - _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ./logs
      name: tutorial_log
      version: null
      default_hp_metric: false


# NOTE:
# interpolation parameters for Allegro model
num_scalar_features: 64


training_module:
  _target_: nequip.train.EMALightningModule
  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 0.05
      forces: 0.95
  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      per_atom_energy_mae: 0.05
      forces_mae: 0.95
  test_metrics: ${training_module.val_metrics}
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
  # ^ IMPORTANT: Allegro models do better with learning rates around 1e-3

  # to use the Allegro model in the NequIP framework, the following  model  block has to be changed to be that of Allegro's
  model:
    _target_: allegro.model.AllegroModel

    # === basic model params ===
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    # === two-body scalar embedding ===
    radial_chemical_embed:
      # the defaults for the Bessel embedding module are usually appropriate
      _target_: allegro.nn.TwoBodyBesselScalarEmbed
      num_bessels: 8
      bessel_trainable: false
      polynomial_cutoff_p: 6

    # output dimension of the radial-chemical embedding
    radial_chemical_embed_dim: ${num_scalar_features}

    # scalar embedding MLP
    scalar_embed_mlp_hidden_layers_depth: 1
    scalar_embed_mlp_hidden_layers_width: ${num_scalar_features}
    scalar_embed_mlp_nonlinearity: silu

    # === core hyperparameters ===
    # The following hyperparameters are the main ones that one should focus on tuning.

    # maximum order l to use in spherical harmonics embedding, 1 is baseline (fast), 2 is more accurate, but slower, 3 highly accurate but slow
    l_max: 1

    # number of tensor product layers, 1-3 usually best, more is more accurate but slower
    num_layers: 2

    # number of scalar features, more is more accurate but slower
    # 16, 32, 64, 128, 256 are good options to try depending on the dataset
    num_scalar_features: ${num_scalar_features}

    # number of tensor features, more is more accurate but slower
    # 8, 16, 32, 64 are good options to try depending on the dataset
    num_tensor_features: 32

    # == allegro MLPs ==
    # neural network parameters in the Allegro layers
    allegro_mlp_hidden_layers_depth: 1
    allegro_mlp_hidden_layers_width: ${num_scalar_features}
    allegro_mlp_nonlinearity: silu
    # ^ setting  nonlinearity  to  null  means that the Allegro MLPs are effectively linear layers

    # === advanced hyperparameters ===
    # The following hyperparameters should remain in their default states until the above core hyperparameters have been set.

    # whether to include features with odd mirror parity
    # often turning parity off gives equally good results but faster networks, so do consider this
    parity: true

    # whether the tensor product weights couple the paths and channels or not (otherwise the weights are only applied per-path)
    # default is  true , which is expected to be more expressive than  false 
    tp_path_channel_coupling: true

    # == readout MLP ==
    # neural network parameters in the readout layer
    readout_mlp_hidden_layers_depth: 1
    readout_mlp_hidden_layers_width: ${num_scalar_features}
    readout_mlp_nonlinearity: silu
    # ^ setting  nonlinearity  to  null  means that output MLP is effectively a linear layer

    # === misc hyperparameters ===
    # average number of neighbors for edge sum normalization
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}

    # per-type per-atom scales and shifts
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    # ^ this should typically be the isolated atom energies for your dataset
    #   provided as a dict, e.g.
    # per_type_energy_shifts: 
    #   C: 1.234
    #   H: 2.345
    #   O: 3.456
    per_type_energy_scales: ${training_data_stats:forces_rms}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    # ZBL pair potential (optional, can be removed or included depending on aplication)
    # see NequIP docs for details:
    # https://nequip.readthedocs.io/en/latest/api/nn.html#nequip.nn.pair_potential.ZBL
    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: real     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types


global_options:
  allow_tf32: false
   

Q: Where is the allegro.yaml file located?
A: The file  allegro.yaml  is located at  MLFF_QD/config_files/training/allegro.yaml .

Q: Show me the content of fusion.yaml
A: Here is the configuration for  fusion.yaml :

   yaml
# General settings
general:
    seed: 42  # Random seed for reproducibility
    database_name: 'CdSe.db'  # Name of the database file

# Data handling settings
data:
    dataset_path: 
    use_last_n: 100  # current testing
# Model architecture settings
model:
    model_type: nequip_mace_interaction_fusion   # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.
    n_rbf: 40
    n_atom_basis: 192
    n_interactions: 2
    
    dropout_rate: null # Default is 0.1 but it will not work when you select only 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null # [256, 128, 64, 32] (if null then n_neurons == n_atom_basis)
    distance_unit: 'Ang'
    property_unit_dict:
      energy: 'eV'
      forces: 'eV/Ang'

# Output settings
outputs:
    energy:
      loss_weight: 0.05
      metrics: "MAE"
    forces:
      loss_weight: 0.95
      metrics: "MAE"

# Training settings
training:
    accelerator: 'gpu'
    devices: 1
    precision: 32
    batch_size: 16
    num_train: 800
    num_val: 200
    max_epochs: 3
    num_workers: 24
    pin_memory: true
    optimizer:
      type: 'AdamW'
      lr: 0.0001 #changed 0.0001 to 0.001
    scheduler:
      type: 'ReduceLROnPlateau'
      factor: 0.8
      patience: 30
      verbose: true

# Logging and checkpoint settings
logging:
    folder: './results'
    log_dir: "lightning_logs"
    checkpoint_dir: "best_inference_model"
    monitor: "val_loss"

# Testing settings
testing:
    trained_model_path: './results'  # Path to load the trained model
    csv_file_name: 'actual_vs_predicted_enrgforc.csv'  # Path to save the predictions CSV

# Resume training settings
resume_training:
    resume_checkpoint_dir: null # Path to model checkpoint file if required 

# Fine Tuning settings
fine_tuning:
    pretrained_checkpoint: # checkpoint file path
    freeze_embedding: true
    freeze_interactions_up_to: 2 # add no of layers
    freeze_all_representation: true
    lr: 0.00005
    early_stopping_patience: 10
    best_model_dir: fine_tuned_best_model  # Subdirectory only
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory only
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
   

Q: Where is the fusion.yaml file located?
A: The file  fusion.yaml  is located at  MLFF_QD/config_files/training/fusion.yaml .

Q: Show me the content of input.yaml
A: Here is the configuration for  input.yaml :

   yaml
# ==============================================================================
# Notes for users:
#
# - Use **dot notation** for override keys (see below for example).
# - In case of conflict, keys from  common  always take priority over  overrides .
# - Keys not present in the engine template will be ignored (with a warning).
# - n_rbf: For schnet, painn, fusion → RBF basis functions. For nequip/allegro → mapped to bessel basis.
# ==============================================================================

platform: fusion  # [schnet, painn, fusion, nequip, allegro, mace]

common:
  data:
    input_xyz_file: ./basic_consolidated_dataset_1000CdSe.xyz   # Path to your XYZ file

  model:
    mp_layers: 3       # Number of message-passing layers in the neural network.
    features: 32       # Dimension of atomic feature vectors (per-atom embedding size).
    cutoff: 12.0
    n_rbf: 8           # For schnet, painn, fusion: number of RBF (radial basis functions)
                       # For nequip, allegro: this value will be mapped to number of bessel basis functions
    l_max: 1
    parity: true
    model_dtype: float32
    chemical_symbols: [Cd, Se, Cl]
    # pair_potential: Option to enable ZBL for NequIP/Allegro models only.
    #   - Set to "ZBL" (as a string) to ENABLE ZBL pair potential
    #   - Set to null to DISABLE the pair potential block (recommended for most cases)
    #   - Any other value will raise an error
    pair_potential: null   # Use "ZBL" (string), or null to disable

  training:
    seed: 42
    train_size: 0.8
    val_size: 0.1
    test_size: 0.1
    batch_size: 16      # Global batch size for training. 
                        # If using multiple GPUs, this value is split evenly across devices 
                        # (e.g., 16 total → 8 per GPU when devices=2). 
                        # For a single GPU, the full batch size is used.
    epochs: 3
    learning_rate: 0.001
    num_workers: 24
    accelerator: cuda
    devices: 2              # Number of GPUs to use for distributed or data-parallel training. Set 1 for single-GPU.
    log_every_n_steps: 5    # Frequency (in steps) for logging metrics and losses to the console or logger.
    optimizer: AdamW
    scheduler:
      type: ReduceLROnPlateau
      factor: 0.8
      patience: 5
    pin_memory: true      # If true, preloads data into page-locked memory for faster GPU transfer.
    
    early_stopping:
      enabled: true       # Enable or disable early stopping to avoid overfitting.
      patience: 30        # Stop training if validation loss doesn’t improve for this many epochs.
      min_delta: 0.003    # Minimum change in monitored metric to qualify as improvement.
      monitor: val_loss
      # monitor: val_loss         # for schnet
      # monitor: val0_epoch/weighted_sum   # for nequip
      # (If omitted, the code auto-inserts the correct default!)

  loss:
    energy_weight: 0.05
    forces_weight: 0.95

  output:
    output_dir: ./resultsNewNewX

# ------------------------------------------------------------------------------
# Overrides section:
# - Use dot notation for all keys (e.g., model.n_rbf, trainer.callbacks[0].patience)
# - Only specify keys you want to override for a specific engine.
# - If a key is in both  common  and  overrides ,  common  wins.
# ------------------------------------------------------------------------------

overrides:

  schnet:
    model.n_rbf: 30
    model.activation: relu
    model.n_layers: 1
    model.dropout: 0.2
    logging.folder: ./resultsExpert
    trainer:
      callbacks:
        - _target_: lightning.pytorch.callbacks.EarlyStopping
          patience: 100

  nequip:
    model.num_bessels: 50                     # dot notation for nested keys
    training_module.model.parity: false        # disables parity in the model
    model.n_layers: 5                         # ignored if mp_layers is set in common
    training_module.model.num_layers: 5        # ignored if mp_layers is set in common
    model.activation: relu                     # ignored if not in template
    model.dropout: 0.1                         # ignored if not in template
    logging.folder: ./resultsNequIP
    trainer.logger[0].save_dir: logsNequIPX
    trainer.callbacks[1].filename: bestNew
    trainer.callbacks[0].patience: 100

    # Example: Add new parameter not present in template to see warning in logs
    model.new_param: 12345
    training_module.loss.coeffs.total_energy: 0.02

  painn:
    model.n_atom_basis: 50
    training.num_val: 0.3
    outputs.forces.loss_weight: 0.91
    logging.folder: ./resultsPainn
    trainer.callbacks[1].monitor: val_loss
    trainer.logger[0].save_dir: ./logsPainnX
    fine_tuning.lr: 0.05

  fusion:
    model.n_interactions: 4
    training.num_train: 0.65
    outputs.energy.loss_weight: 0.09
    logging.folder: ./resultsFusion
    trainer.callbacks[0].min_delta: 0.01
    trainer.logger[0].save_dir: ./logsFusionX

  allegro:
    training_module.model.radial_chemical_embed.num_bessels: 17
    model.n_bessels: 50
    model.n_rbf: 30
    training_module.model.num_scalar_features: 48
    training_module.model.l_max: 2
    training_module.model.parity: false
    trainer.callbacks[0].patience: 10
    trainer.callbacks[2].logging_interval: epoch
    trainer.logger[0].save_dir: ./logsAllegroX

  mace:
    num_channels: 64
    model.n_rbf: 30
    max_L: 1
    lr: 0.007
    eval_interval: 10
    valid_file: ./converted_data/mace_val.xyz


   

Q: Where is the input.yaml file located?
A: The file  input.yaml  is located at  MLFF_QD/config_files/training/input.yaml .

Q: Show me the content of mace.yaml
A: Here is the configuration for  mace.yaml :

   yaml
# ===========================
# Experiment & Paths
# ===========================
name: mace_cdsecl_model
seed: 42
log_level: INFO
error_table: PerAtomMAE   # Report validation metrics using MAE

# ===========================
# Hardware & Precision
# ===========================
device: cuda              # Options: cpu, cuda, mps, xpu
default_dtype: float32

# ===========================
# Dataset & Keys
# ===========================
train_file:   #consolidate-cdse35_1000.xyz consolidated_dataset_1000_CdSe_new.xyz
valid_file: null
test_file: null

energy_key: energy  #REF_energy
forces_key: forces  #REF_forces
stress_key: null

valid_fraction: 0.2
batch_size: 8
num_workers: 24
pin_memory: true        # Enables faster CPU → GPU transfer

# ===========================
# Model Configuration
# ===========================
model: MACE
r_max: 12                  #cutoff
num_radial_basis: 20           #n-rbf
num_cutoff_basis: 6
max_ell: 3
num_channels: 64             #n_atom_basis
max_L: 2
num_interactions: 3
correlation: 3
avg_num_neighbors: 100.80

# ===========================
# Training Parameters
# ===========================
max_num_epochs: 3
ema: true
ema_decay: 0.99

# ===========================
# Validation & Early Stopping
# ===========================
valid_batch_size: 16         # Match GPU capacity
eval_interval: 1           # Check validation every 1 epochs
patience: 30                # Early stop if no val improvement in 30 checks

# ===========================
# Stochastic Weight Averaging
# ===========================
swa: true
start_swa: 400
swa_energy_weight: 1.0
swa_forces_weight: 100.0

# ===========================
# Loss Weights
# ===========================
forces_weight: 0.95
energy_weight: 0.05

# ===========================
# Optimizer & Scheduler
# ===========================
optimizer: adam
lr: 0.001
weight_decay: 1e-5

scheduler: ReduceLROnPlateau
lr_factor: 0.8
scheduler_patience: 5
lr_scheduler_gamma: 0.9993

# ===========================
# Energy Baseline & Scaling
# ===========================
E0s: "average"                         # Use average per-atom energy (like --E0s=average)
scaling: rms_forces_scaling
compute_avg_num_neighbors: true

   

Q: Where is the mace.yaml file located?
A: The file  mace.yaml  is located at  MLFF_QD/config_files/training/mace.yaml .

Q: Show me the content of nequip.yaml
A: Here is the configuration for  nequip.yaml :

   yaml
# The config file is divided into 4 sections --  data ,  train ,  model , and  global_options 
# The config system relies on omegaconf (https://omegaconf.readthedocs.io/en/2.3_branch/index.html)
# and hydra (https://hydra.cc/docs/intro/) functionalities, such as
# - omegaconf's variable interpolation (https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#variable-interpolation)
# - omegaconf's resolvers (https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#resolvers)
# - hydra's instantiate (https://hydra.cc/docs/advanced/instantiate_objects/overview/)
# With hydra's instantiation (notice the  _target_ s everywhere), the config file (almost) directly corresponds to instantiating objects as one would normally do in Python.
# Much of the infrastructure is based on PyTorch Lightning (https://lightning.ai/docs/pytorch/stable/), such as the use of Lightning's Trainer, DataModule, LightningModule, Callback objects.

# ===========
#     RUN
# ===========
# the run types will be completed in sequence
# one can do  train ,  val ,  test  run types
run: [train, val, test]


# the following parameters (cutoff_radius, chemical_symbols, model_type_names) are not used direcly by the code
# parameters that take thier values show up multiple times in the config, so this allows us to use
# variable interpolation to keep their multiple instances consistent

# data and model r_max can be different (model's r_max should be smaller), but we try to make them the same
cutoff_radius: 12.0

# There are two sets of atomic types to keep track of in most applications
# -- there is the conventional atomic species (e.g. C, H), and a separate  type_names  known to the model.
# The model only knows types based on a set of zero-based indices and user-given  type_names  argument.
# An example where this distinction is necessary include datasets with the same atomic species with different charge states:
# we could define  chemical_symbols: [C, C]  and model  type_names: [C3, C4]  for +3 and +4 charge states.
# There could also be instances such as coarse graining we only care about the model's  type_names  (no need to define chemical species).
# Because of this distinction, these variables show up as arguments across different categories, including, data, model, metrics and even callbacks.
# In this case, we fix both to be the same, so we define a single set of each here and use variable interpolation to retrieve them below.
# This ensures a single location where the values are set to reduce the chances of mis-configuring runs.
chemical_symbols: [Cd, Cl, Se] 
model_type_names: ${chemical_symbols}


# ============
#     DATA
# ============
#  data  is managed by  LightningDataModule s
# NequIP provides some standard datamodules that can be found in  nequip.data.datamodule 
# Users are free to define and use their own datamodules that subclass nequip.data.datamodule.NequIPDataModule
data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 456             # dataset seed for reproducibility
  
  # here we take an ASE-readable file (in extxyz format) and split it into train:val:test = 80:10:10
  split_dataset: 
    file_path:  #./basic_consolidated_dataset_1000CdSe.xyz
    train: 0.8
    val: 0.1
    test: 0.1

  #  transforms  convert data from the Dataset to a form that can be used by the ML model
  # the transforms are only performed right before data is given to the model
  # data is kept in its untransformed form
  
  transforms:
    # data doesn't usually come with a neighborlist -- this tranforms prepares the neighborlist
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}
    # the models only know atom types, which can be different from the chemical species (e.g. C, H)
    # for instance we can have data with different charge states of carbon, which means they are
    # all labeled by chemical species  C , but may have different atom type labels based on the charge states
    # in this case, the atom types are the same as the chemical species, but we still have to include this
    # transformation to ensure that the data has 0-indexed atom type lists used in the various model operations 
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}

  # the following are torch.utils.data.DataLoader configs excluding the arguments  dataset  and  collate_fn 
  # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader
  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
    num_workers: 5
    shuffle: true
  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
    num_workers: ${data.train_dataloader.num_workers}  # we want to use the same num_workers -- variable interpolation helps
  test_dataloader: ${data.val_dataloader}  # variable interpolation comes in handy again

  # dataset statistics can be calculated to be used for model initialization such as for shifting, scaling and standardizing.
  # it is advised to provide custom names -- you will have to retrieve them later under model to initialize certain parameters to the dataset statistics computed
  stats_manager:
    # dataset statistics is handled by the  DataStatisticsManager 
    # here, we use  CommonDataStatisticsManager  for a basic set of dataset statistics for general use cases
    # the dataset statistics include  num_neighbors_mean ,  per_atom_energy_mean ,  forces_rms ,  per_type_forces_rms 
    _target_: nequip.data.CommonDataStatisticsManager
    # dataloader kwargs for data statistics computation
    #  batch_size  should ideally be as large as possible without trigerring OOM
    dataloader_kwargs:
      batch_size: 16
    # we need to provide the same type names that correspond to the model's  type_names 
    # so we interpolate the "central source of truth" model type names from above
    type_names: ${model_type_names}

#  trainer  (mandatory) is a Lightning.Trainer object (https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-class-api)
trainer:
  _target_: lightning.Trainer
  accelerator: auto
  devices: 1
  enable_checkpointing: true
  max_epochs: 3
  max_time: 03:00:00:00
  check_val_every_n_epoch: 1  # how often to validate
  log_every_n_steps: 1       # how often to log

  # use any Lightning supported logger
  logger:
    - _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ./logs
      name: tutorial_log
      version: null
      default_hp_metric: false

  # use any Lightning callbacks https://lightning.ai/docs/pytorch/stable/api_references.html#callbacks
  # and any custom callbakcs that subclass Lightning's Callback parent class
  callbacks:
    # Common callbacks used in ML

    # stop training when some criterion is met
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      min_delta: 1e-3                         # how much to be considered a "change"
      patience: 20                            # how many instances of "no change" before stopping

    # checkpoint based on some criterion
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      dirpath: ./results    
      filename: best                          # best.ckpt is the checkpoint name
      save_last: true                         # last.ckpt will be saved
      
    # log learning rate, e.g. to monitor what the learning rate scheduler is doing
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: epoch

# training_module refers to a NequIPLightningModule
training_module:
  _target_: nequip.train.EMALightningModule

  # We are using an EMA model (i.e. we keep a separate model whose weights are an exponential moving average of the base model's weights)
  # The use of an EMA model is configured by setting  ema_decay  to be a float (e.g. 0.999) under  training_module  (it is a  NequIPLightningModule  argument). The default of  ema_decay  is None, which means an EMA model is not used, if  ema_decay  is not explicitly configured
  # EMA allows for smoother validation curves and thus more reliable metrics for monitoring
  # Loading from a checkpoint for use in the  nequip.ase.NequIPCalculator  or during  nequip-compile  and  nequip-package  will always load the EMA model if it's present
  ema_decay: 0.999

  # here, we use a simplified MetricsManager wrapper (see docs) to construct the energy-force loss function
  # the more general  nequip.train.MetricsManager  could also be used to configure a custom loss function
  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 0.05
      forces: 0.95

  # again, we use a simplified MetricsManager wrapper (see docs) to construct the energy-force metrics
  # the more general  nequip.train.MetricsManager  could also be used in this case
  # validation metrics are used for monitoring and influencing training, e.g. with LR schedulers or early stopping, etc
  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      total_energy_mae: 1.0
      forces_mae: 1.0
      # keys  total_energy_rmse  and  forces_rmse ,  per_atom_energy_rmse  and  per_atom_energy_mae  are also available

  # we could have train_metrics and test_metrics be different from val_metrics, but it makes sense to have them be the same
  train_metrics: ${training_module.val_metrics}  # use variable interpolation
  test_metrics: ${training_module.val_metrics}  # use variable interpolation

  # any torch compatible optimizer: https://pytorch.org/docs/stable/optim.html#algorithms
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.03

  # see options for lr_scheduler_config
  # https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule.configure_optimizers
  lr_scheduler:
    # any torch compatible lr sceduler
    scheduler:
      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
      factor: 0.6
      patience: 5
      threshold: 0.2
      min_lr: 1e-6
    monitor: val0_epoch/weighted_sum
    interval: epoch
    frequency: 1

  # model details
  model:
    _target_: nequip.model.NequIPGNNModel

    # == basic model params ==
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    # == bessel encoding ==
    num_bessels: 8                # number of basis functions used in the radial Bessel basis, the default of 8 usually works well
    bessel_trainable: false       # set true to train the bessel weights (default false)
    polynomial_cutoff_p: 6        # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance

    # == convnet layers ==
    num_layers: 3       # number of interaction blocks, we find 3-5 to work best
    l_max: 1            # the maximum irrep order (rotation order) for the network's features, l=1 is a good default, l=2 is more accurate but slower
    parity: true        # whether to include features with odd mirror parity; often turning parity off gives equally good results but faster networks, so do consider this
    num_features: 32    # the multiplicity of the features, 32 is a good default for accurate network, if you want to be more accurate, go larger, if you want to be faster, go lower

    # == radial network ==
    radial_mlp_depth: 2         # number of radial layers, usually 1-3 works best, smaller is faster
    radial_mlp_width: 64        # number of hidden neurons in radial function, smaller is faster

    # dataset statistics used to inform the model's initial parameters for normalization, shifting and rescaling
    # we use omegaconf's resolvers (https://omegaconf.readthedocs.io/en/2.3_branch/usage.html#resolvers)
    # to facilitate getting the dataset statistics from the DataStatisticsManager
    
    # average number of neighbors for edge sum normalization
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}
    
    # == per-type per-atom scales and shifts ==
    per_type_energy_scales: ${training_data_stats:per_type_forces_rms}
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    # == ZBL pair potential ==
    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: metal     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types

# global options
global_options:
  allow_tf32: false
   

Q: Where is the nequip.yaml file located?
A: The file  nequip.yaml  is located at  MLFF_QD/config_files/training/nequip.yaml .

Q: Show me the content of painn.yaml
A: Here is the configuration for  painn.yaml :

   yaml
# General settings
general:
    seed: 42  # Random seed for reproducibility
    database_name: 'Database.db'  # Name of the database file

# Data handling settings
data:
    dataset_path: 
    use_last_n: 100  # current testing
# Model architecture settings
model:
    model_type: painn   # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.
    n_rbf: 40
    n_atom_basis: 192
    n_interactions: 2
    
    dropout_rate: null # Default is 0.1 but it will not work when you select only 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null # [256, 128, 64, 32] (if null then n_neurons == n_atom_basis)
    distance_unit: 'Ang'
    property_unit_dict:
      energy: 'eV'
      forces: 'eV/Ang'

# Output settings
outputs:
    energy:
      loss_weight: 0.05
      metrics: "MAE"
    forces:
      loss_weight: 0.95
      metrics: "MAE"

# Training settings
training:
    accelerator: 'gpu'
    devices: 1
    precision: 32
    batch_size: 16
    num_train: 800
    num_val: 100
    num_test: 100
    max_epochs: 3
    num_workers: 24
    pin_memory: true
    log_every_n_steps: 1
    optimizer:
      type: 'AdamW'
      lr: 0.0001 #changed 0.0001 to 0.001
    scheduler:
      type: 'ReduceLROnPlateau'
      factor: 0.8
      patience: 30
      verbose: true
    early_stopping:      
      monitor: val_loss
      patience: 20
      min_delta: 0.001
      mode: min

# Logging and checkpoint settings
logging:
    folder: './results'
    log_dir: "lightning_logs"
    checkpoint_dir: "best_inference_model"
    monitor: "val_loss"

# Testing settings
testing:
    trained_model_path: './results'  # Path to load the trained model
    csv_file_name: 'actual_vs_predicted_enrgforc.csv'  # Path to save the predictions CSV

# Resume training settings
resume_training:
    resume_checkpoint_dir: null # Path to model checkpoint file if required 

# Fine Tuning settings
fine_tuning:
    pretrained_checkpoint: # checkpoint file path
    freeze_embedding: true
    freeze_interactions_up_to: 2 # add no of layers
    freeze_all_representation: true
    lr: 0.00005
    early_stopping_patience: 10
    best_model_dir: fine_tuned_best_model  # Subdirectory only
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory only
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
   

Q: Where is the painn.yaml file located?
A: The file  painn.yaml  is located at  MLFF_QD/config_files/training/painn.yaml .

Q: Show me the content of schnet.yaml
A: Here is the configuration for  schnet.yaml :

   yaml
# General settings
general:
    seed: 42  # Random seed for reproducibility
    database_name: 'Database.db'  # Name of the database file

# Data handling settings
data:
    dataset_path: 
    use_last_n: 100  # current testing
# Model architecture settings
model:
    model_type: schnet   # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.
    n_rbf: 40
    n_atom_basis: 192
    n_interactions: 2
    
    dropout_rate: null # Default is 0.1 but it will not work when you select only 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null # [256, 128, 64, 32] (if null then n_neurons == n_atom_basis)
    distance_unit: 'Ang'
    property_unit_dict:
      energy: 'eV'
      forces: 'eV/Ang'

# Output settings
outputs:
    energy:
      loss_weight: 0.05
      metrics: "MAE"
    forces:
      loss_weight: 0.95
      metrics: "MAE"

# Training settings
training:
    accelerator: 'gpu'
    devices: 1
    precision: 32
    batch_size: 16
    num_train: 800
    num_val: 100
    num_test: 100
    max_epochs: 3
    num_workers: 24
    pin_memory: true
    log_every_n_steps: 1
    optimizer:
      type: 'AdamW'
      lr: 0.0001 #changed 0.0001 to 0.001
    scheduler:
      type: 'ReduceLROnPlateau'
      factor: 0.8
      patience: 30
      verbose: true
    early_stopping:      
      monitor: val_loss
      patience: 20
      min_delta: 0.001
      mode: min

# Logging and checkpoint settings
logging:
    folder: './results'
    log_dir: "lightning_logs"
    checkpoint_dir: "best_inference_model"
    monitor: "val_loss"

# Testing settings
testing:
    trained_model_path: './results'  # Path to load the trained model
    csv_file_name: 'actual_vs_predicted_enrgforc.csv'  # Path to save the predictions CSV

# Resume training settings
resume_training:
    resume_checkpoint_dir: null # Path to model checkpoint file if required 

# Fine Tuning settings
fine_tuning:
    pretrained_checkpoint: # checkpoint file path
    freeze_embedding: true
    freeze_interactions_up_to: 2 # add no of layers
    freeze_all_representation: true
    lr: 0.00005
    early_stopping_patience: 10
    best_model_dir: fine_tuned_best_model  # Subdirectory only
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory only
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
   

Q: Where is the schnet.yaml file located?
A: The file  schnet.yaml  is located at  MLFF_QD/config_files/training/schnet.yaml .

Q: Show me the content of allegro.yaml
A: Here is the configuration for  allegro.yaml :

   yaml
run: [train, val, test]


cutoff_radius: 12.0
chemical_symbols: [Cd, Cl, Se] 
model_type_names: ${chemical_symbols}

data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 456             
  split_dataset:
    file_path:  #./consolidated_dataset_1000_CdSe_new.xyz
    train: 0.8
    val: 0.1
    test: 0.1
  transforms:
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}
    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}
  
  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
  test_dataloader: ${data.val_dataloader}
  stats_manager:
    _target_: nequip.data.CommonDataStatisticsManager
    type_names: ${model_type_names}

trainer:
  _target_: lightning.Trainer
  accelerator: auto
  devices: 1    # NO of GPUs
  max_epochs: 3
  check_val_every_n_epoch: 1
  log_every_n_steps: 5
  
  callbacks:
      
    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      dirpath: ./results 
      filename: best
      save_last: true
    
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      min_delta: 1e-3                         # how much to be considered a "change"
      patience: 20                            # how many instances of "no change" before stopping
      
  logger:
    - _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ./logs
      name: tutorial_log
      version: null
      default_hp_metric: false


# NOTE:
# interpolation parameters for Allegro model
num_scalar_features: 64


training_module:
  _target_: nequip.train.EMALightningModule
  
  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 0.05
      forces: 0.95
      
  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      per_atom_energy_mae: 0.05
      forces_mae: 0.95
      
  train_metrics: ${training_module.val_metrics}
  test_metrics: ${training_module.val_metrics}
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
  # ^ IMPORTANT: Allegro models do better with learning rates around 1e-3

  # to use the Allegro model in the NequIP framework, the following  model  block has to be changed to be that of Allegro's
  model:
    _target_: allegro.model.AllegroModel

    # === basic model params ===
    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    # === two-body scalar embedding ===
    radial_chemical_embed:
      # the defaults for the Bessel embedding module are usually appropriate
      _target_: allegro.nn.TwoBodyBesselScalarEmbed
      num_bessels: 8
      bessel_trainable: false
      polynomial_cutoff_p: 6

    # output dimension of the radial-chemical embedding
    radial_chemical_embed_dim: ${num_scalar_features}

    # scalar embedding MLP
    scalar_embed_mlp_hidden_layers_depth: 1
    scalar_embed_mlp_hidden_layers_width: ${num_scalar_features}
    scalar_embed_mlp_nonlinearity: silu

    # === core hyperparameters ===
    # The following hyperparameters are the main ones that one should focus on tuning.

    # maximum order l to use in spherical harmonics embedding, 1 is baseline (fast), 2 is more accurate, but slower, 3 highly accurate but slow
    l_max: 1

    # number of tensor product layers, 1-3 usually best, more is more accurate but slower
    num_layers: 2

    # number of scalar features, more is more accurate but slower
    # 16, 32, 64, 128, 256 are good options to try depending on the dataset
    num_scalar_features: ${num_scalar_features}

    # number of tensor features, more is more accurate but slower
    # 8, 16, 32, 64 are good options to try depending on the dataset
    num_tensor_features: 32

    # == allegro MLPs ==
    # neural network parameters in the Allegro layers
    allegro_mlp_hidden_layers_depth: 1
    allegro_mlp_hidden_layers_width: ${num_scalar_features}
    allegro_mlp_nonlinearity: silu
    # ^ setting  nonlinearity  to  null  means that the Allegro MLPs are effectively linear layers

    # === advanced hyperparameters ===
    # The following hyperparameters should remain in their default states until the above core hyperparameters have been set.

    # whether to include features with odd mirror parity
    # often turning parity off gives equally good results but faster networks, so do consider this
    parity: true

    # whether the tensor product weights couple the paths and channels or not (otherwise the weights are only applied per-path)
    # default is  true , which is expected to be more expressive than  false 
    tp_path_channel_coupling: true

    # == readout MLP ==
    # neural network parameters in the readout layer
    readout_mlp_hidden_layers_depth: 1
    readout_mlp_hidden_layers_width: ${num_scalar_features}
    readout_mlp_nonlinearity: silu
    # ^ setting  nonlinearity  to  null  means that output MLP is effectively a linear layer

    # === misc hyperparameters ===
    # average number of neighbors for edge sum normalization
    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}

    # per-type per-atom scales and shifts
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    # ^ this should typically be the isolated atom energies for your dataset
    #   provided as a dict, e.g.
    # per_type_energy_shifts: 
    #   C: 1.234
    #   H: 2.345
    #   O: 3.456
    per_type_energy_scales: ${training_data_stats:forces_rms}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    # ZBL pair potential (optional, can be removed or included depending on aplication)
    # see NequIP docs for details:
    # https://nequip.readthedocs.io/en/latest/api/nn.html#nequip.nn.pair_potential.ZBL
    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: real     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types


global_options:
  allow_tf32: false
   

Q: Where is the allegro.yaml file located?
A: The file  allegro.yaml  is located at  MLFF_QD/src/mlff_qd/templates/allegro.yaml .

Q: Show me the content of fusion.yaml
A: Here is the configuration for  fusion.yaml :

   yaml
# General settings
general:
    seed: 42  # Random seed for reproducibility
    database_name: 'CdSe.db'  # Name of the database file

# Data handling settings
data:
    dataset_path: 
    use_last_n: 100  # current testing
# Model architecture settings
model:
    model_type: nequip_mace_interaction_fusion   # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.
    n_rbf: 40
    n_atom_basis: 192
    n_interactions: 2
    
    dropout_rate: null # Default is 0.1 but it will not work when you select only 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null # [256, 128, 64, 32] (if null then n_neurons == n_atom_basis)
    distance_unit: 'Ang'
    property_unit_dict:
      energy: 'eV'
      forces: 'eV/Ang'

# Output settings
outputs:
    energy:
      loss_weight: 0.05
      metrics: "MAE"
    forces:
      loss_weight: 0.95
      metrics: "MAE"

# Training settings
training:
    accelerator: 'gpu'
    devices: 1
    precision: 32
    batch_size: 16
    num_train: 800
    num_val: 200
    max_epochs: 3
    num_workers: 24
    pin_memory: true
    optimizer:
      type: 'AdamW'
      lr: 0.0001 #changed 0.0001 to 0.001
    scheduler:
      type: 'ReduceLROnPlateau'
      factor: 0.8
      patience: 30
      verbose: true

# Logging and checkpoint settings
logging:
    folder: './results'
    log_dir: "lightning_logs"
    checkpoint_dir: "best_inference_model"
    monitor: "val_loss"

# Testing settings
testing:
    trained_model_path: './results'  # Path to load the trained model
    csv_file_name: 'actual_vs_predicted_enrgforc.csv'  # Path to save the predictions CSV

# Resume training settings
resume_training:
    resume_checkpoint_dir: null # Path to model checkpoint file if required 

# Fine Tuning settings
fine_tuning:
    pretrained_checkpoint: # checkpoint file path
    freeze_embedding: true
    freeze_interactions_up_to: 2 # add no of layers
    freeze_all_representation: true
    lr: 0.00005
    early_stopping_patience: 10
    best_model_dir: fine_tuned_best_model  # Subdirectory only
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory only
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
   

Q: Where is the fusion.yaml file located?
A: The file  fusion.yaml  is located at  MLFF_QD/src/mlff_qd/templates/fusion.yaml .

Q: Show me the content of mace.yaml
A: Here is the configuration for  mace.yaml :

   yaml
# ===========================
# Experiment & Paths
# ===========================
name: mace_cdsecl_model
seed: 42
log_level: INFO
error_table: PerAtomMAE   # Report validation metrics using MAE

# ===========================
# Hardware & Precision
# ===========================
device: cuda              # Options: cpu, cuda, mps, xpu
default_dtype: float32
distributed: false    # True for multiple GPU  keys: true, false
# ===========================
# Dataset & Keys
# ===========================
train_file:   #consolidate-cdse35_1000.xyz consolidated_dataset_1000_CdSe_new.xyz
valid_file: null
test_file: null

energy_key: energy  #REF_energy
forces_key: forces  #REF_forces
stress_key: null

valid_fraction: 0.2
batch_size: 8
num_workers: 24
pin_memory: true        # Enables faster CPU → GPU transfer

# ===========================
# Model Configuration
# ===========================
model: MACE
r_max: 12                  #cutoff
num_radial_basis: 20           #n-rbf
num_cutoff_basis: 6
max_ell: 3
num_channels: 64             #n_atom_basis
max_L: 2
num_interactions: 3
correlation: 3
avg_num_neighbors: 100.80

# ===========================
# Training Parameters
# ===========================
max_num_epochs: 3
ema: true
ema_decay: 0.99

# ===========================
# Validation & Early Stopping
# ===========================
valid_batch_size: 16         # Match GPU capacity
eval_interval: 1           # Check validation every 1 epochs
patience: 30                # Early stop if no val improvement in 30 checks

# ===========================
# Stochastic Weight Averaging
# ===========================
swa: true
start_swa: 400
swa_energy_weight: 1.0
swa_forces_weight: 100.0

# ===========================
# Loss Weights
# ===========================
forces_weight: 0.95
energy_weight: 0.05

# ===========================
# Optimizer & Scheduler
# ===========================
optimizer: adam
lr: 0.001
weight_decay: 1e-5

scheduler: ReduceLROnPlateau
lr_factor: 0.8
scheduler_patience: 5
lr_scheduler_gamma: 0.9993

# ===========================
# Energy Baseline & Scaling
# ===========================
E0s: "average"                         # Use average per-atom energy (like --E0s=average)
scaling: rms_forces_scaling
compute_avg_num_neighbors: true

   

Q: Where is the mace.yaml file located?
A: The file  mace.yaml  is located at  MLFF_QD/src/mlff_qd/templates/mace.yaml .

Q: Show me the content of nequip.yaml
A: Here is the configuration for  nequip.yaml :

   yaml
run: [train, val, test]

cutoff_radius: 12.0

chemical_symbols: [Cd, Cl, Se] 
model_type_names: ${chemical_symbols}

data:
  _target_: nequip.data.datamodule.ASEDataModule
  seed: 456             # dataset seed for reproducibility
  
  split_dataset:
    file_path:   #./basic_consolidated_dataset_1000CdSe.xyz
    train: 0.8
    val: 0.1
    test: 0.1

  transforms:
    - _target_: nequip.data.transforms.NeighborListTransform
      r_max: ${cutoff_radius}

    - _target_: nequip.data.transforms.ChemicalSpeciesToAtomTypeMapper
      chemical_symbols: ${chemical_symbols}

  train_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
    num_workers: 5
    shuffle: true
  val_dataloader:
    _target_: torch.utils.data.DataLoader
    batch_size: 16
    num_workers: ${data.train_dataloader.num_workers}  # we want to use the same num_workers -- variable interpolation helps
  test_dataloader: ${data.val_dataloader}  # variable interpolation comes in handy again

  stats_manager:
    _target_: nequip.data.CommonDataStatisticsManager

    dataloader_kwargs:
      batch_size: 16

    type_names: ${model_type_names}

trainer:
  _target_: lightning.Trainer
  accelerator: auto
  devices: 1    # NO of GPUs
  enable_checkpointing: true
  max_epochs: 3
  max_time: 03:00:00:00
  check_val_every_n_epoch: 1  # how often to validate
  log_every_n_steps: 1       # how often to log

  logger:
    - _target_: lightning.pytorch.loggers.TensorBoardLogger
      save_dir: ./logs
      name: tutorial_log
      version: null
      default_hp_metric: false

  callbacks:
    - _target_: lightning.pytorch.callbacks.EarlyStopping
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      min_delta: 1e-3                         # how much to be considered a "change"
      patience: 20                            # how many instances of "no change" before stopping

    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      monitor: val0_epoch/weighted_sum        # validation metric to monitor
      dirpath: ./results    
      filename: best                          # best.ckpt is the checkpoint name
      save_last: true                         # last.ckpt will be saved
      
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: epoch

training_module:
  _target_: nequip.train.EMALightningModule

  ema_decay: 0.999

  loss:
    _target_: nequip.train.EnergyForceLoss
    per_atom_energy: true
    coeffs:
      total_energy: 0.05
      forces: 0.95

  val_metrics:
    _target_: nequip.train.EnergyForceMetrics
    coeffs:
      total_energy_mae: 1.0
      forces_mae: 1.0
      # keys  total_energy_rmse  and  forces_rmse ,  per_atom_energy_rmse  and  per_atom_energy_mae  are also available

  train_metrics: ${training_module.val_metrics}  # use variable interpolation
  test_metrics: ${training_module.val_metrics}  # use variable interpolation

  optimizer:
    _target_: torch.optim.Adam
    lr: 0.03

  lr_scheduler:
    scheduler:
      _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
      factor: 0.6
      patience: 5
      threshold: 0.2
      min_lr: 1e-6
    monitor: val0_epoch/weighted_sum
    interval: epoch
    frequency: 1

  model:
    _target_: nequip.model.NequIPGNNModel

    seed: 456
    model_dtype: float32
    type_names: ${model_type_names}
    r_max: ${cutoff_radius}

    num_bessels: 8                # number of basis functions used in the radial Bessel basis, the default of 8 usually works well
    bessel_trainable: false       # set true to train the bessel weights (default false)
    polynomial_cutoff_p: 6        # p-exponent used in polynomial cutoff function, smaller p corresponds to stronger decay with distance

    num_layers: 3       # number of interaction blocks, we find 3-5 to work best
    l_max: 1            # the maximum irrep order (rotation order) for the network's features, l=1 is a good default, l=2 is more accurate but slower
    parity: true        # whether to include features with odd mirror parity; often turning parity off gives equally good results but faster networks, so do consider this
    num_features: 32    # the multiplicity of the features, 32 is a good default for accurate network, if you want to be more accurate, go larger, if you want to be faster, go lower

    radial_mlp_depth: 2         # number of radial layers, usually 1-3 works best, smaller is faster
    radial_mlp_width: 64        # number of hidden neurons in radial function, smaller is faster

    avg_num_neighbors: ${training_data_stats:num_neighbors_mean}

    per_type_energy_scales: ${training_data_stats:per_type_forces_rms}
    per_type_energy_shifts: ${training_data_stats:per_atom_energy_mean}
    per_type_energy_scales_trainable: false
    per_type_energy_shifts_trainable: false

    pair_potential:
      _target_: nequip.nn.pair_potential.ZBL
      units: metal     # Ang and kcal/mol, LAMMPS unit names;  allowed values "metal" and "real"
      chemical_species: ${chemical_symbols}   # must tell ZBL the chemical species of the various model atom types

global_options:
  allow_tf32: false

   

Q: Where is the nequip.yaml file located?
A: The file  nequip.yaml  is located at  MLFF_QD/src/mlff_qd/templates/nequip.yaml .

Q: Show me the content of painn.yaml
A: Here is the configuration for  painn.yaml :

   yaml
# General settings
general:
    seed: 42  # Random seed for reproducibility
    database_name: 'Database.db'  # Name of the database file

# Data handling settings
data:
    dataset_path: 
    use_last_n: 100  # current testing
# Model architecture settings
model:
    model_type: painn   # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.
    n_rbf: 40
    n_atom_basis: 192
    n_interactions: 2
    
    dropout_rate: null # Default is 0.1 but it will not work when you select only 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null # [256, 128, 64, 32] (if null then n_neurons == n_atom_basis)
    distance_unit: 'Ang'
    property_unit_dict:
      energy: 'eV'
      forces: 'eV/Ang'

# Output settings
outputs:
    energy:
      loss_weight: 0.05
      metrics: "MAE"
    forces:
      loss_weight: 0.95
      metrics: "MAE"

# Training settings
training:
    accelerator: 'gpu'
    devices: 1
    precision: 32
    batch_size: 16
    num_train: 800
    num_val: 200
    max_epochs: 3
    num_workers: 24
    pin_memory: true
    optimizer:
      type: 'AdamW'
      lr: 0.0001 #changed 0.0001 to 0.001
    scheduler:
      type: 'ReduceLROnPlateau'
      factor: 0.8
      patience: 30
      verbose: true

# Logging and checkpoint settings
logging:
    folder: './results'
    log_dir: "lightning_logs"
    checkpoint_dir: "best_inference_model"
    monitor: "val_loss"

# Testing settings
testing:
    trained_model_path: './results'  # Path to load the trained model
    csv_file_name: 'actual_vs_predicted_enrgforc.csv'  # Path to save the predictions CSV

# Resume training settings
resume_training:
    resume_checkpoint_dir: null # Path to model checkpoint file if required 

# Fine Tuning settings
fine_tuning:
    pretrained_checkpoint: # checkpoint file path
    freeze_embedding: true
    freeze_interactions_up_to: 2 # add no of layers
    freeze_all_representation: true
    lr: 0.00005
    early_stopping_patience: 10
    best_model_dir: fine_tuned_best_model  # Subdirectory only
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory only
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
   

Q: Where is the painn.yaml file located?
A: The file  painn.yaml  is located at  MLFF_QD/src/mlff_qd/templates/painn.yaml .

Q: Show me the content of schnet.yaml
A: Here is the configuration for  schnet.yaml :

   yaml
# General settings
general:
    seed: 42  # Random seed for reproducibility
    database_name: 'Database.db'  # Name of the database file

# Data handling settings
data:
    dataset_path: 
    use_last_n: 100  # current testing
# Model architecture settings
model:
    model_type: schnet   # Options: nequip_mace_so3tensor_fusion
    cutoff: 12.
    n_rbf: 40
    n_atom_basis: 192
    n_interactions: 2
    
    dropout_rate: null # Default is 0.1 but it will not work when you select only 1 layer
    n_layers: 1  # Default is 1
    n_neurons: null # [256, 128, 64, 32] (if null then n_neurons == n_atom_basis)
    distance_unit: 'Ang'
    property_unit_dict:
      energy: 'eV'
      forces: 'eV/Ang'

# Output settings
outputs:
    energy:
      loss_weight: 0.05
      metrics: "MAE"
    forces:
      loss_weight: 0.95
      metrics: "MAE"

# Training settings
training:
    accelerator: 'gpu'
    devices: 1
    precision: 32
    batch_size: 16
    num_train: 800
    num_val: 100
    num_test: 100
    max_epochs: 3
    num_workers: 24
    pin_memory: true
    log_every_n_steps: 1
    optimizer:
      type: 'AdamW'
      lr: 0.0001 #changed 0.0001 to 0.001
    scheduler:
      type: 'ReduceLROnPlateau'
      factor: 0.8
      patience: 30
      verbose: true
    early_stopping:      
      monitor: val_loss
      patience: 20
      min_delta: 0.001
      mode: min

# Logging and checkpoint settings
logging:
    folder: './results'
    log_dir: "lightning_logs"
    checkpoint_dir: "best_inference_model"
    monitor: "val_loss"

# Testing settings
testing:
    trained_model_path: './results'  # Path to load the trained model
    csv_file_name: 'actual_vs_predicted_enrgforc.csv'  # Path to save the predictions CSV

# Resume training settings
resume_training:
    resume_checkpoint_dir: null # Path to model checkpoint file if required 

# Fine Tuning settings
fine_tuning:
    pretrained_checkpoint: # checkpoint file path
    freeze_embedding: true
    freeze_interactions_up_to: 2 # add no of layers
    freeze_all_representation: true
    lr: 0.00005
    early_stopping_patience: 10
    best_model_dir: fine_tuned_best_model  # Subdirectory only
    checkpoint_dir: fine_tuned_checkpoints  # Subdirectory only
    log_name: fine_tune_logs  # Subdirectory for TensorBoard logs
   

Q: Where is the schnet.yaml file located?
A: The file  schnet.yaml  is located at  MLFF_QD/src/mlff_qd/templates/schnet.yaml .

